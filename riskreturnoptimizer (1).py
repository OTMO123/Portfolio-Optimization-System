# -*- coding: utf-8 -*-
"""RiskReturnOptimizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-58e_kASYGXPgNy_nWfjGfD88Abq9ZNO

# Description for the investment allocation modules
"""

!pip install yfinance
!pip install fredapi
!pip install requests
!pip install alpha_vantage
import numpy as np
import pandas as pd
import yfinance as yf
from scipy.optimize import minimize
from fredapi import Fred
import time
import requests

# === ALPHA VANTAGE API KEY ===
API_KEY = "ENTER YOUR KEY"  # Ваш API ключ от Alpha Vantage

# === MACROECONOMIC DATA ===
# Подключение к FRED API
fred_api_key = "ENTER YOUR KEY"
fred = Fred(api_key=fred_api_key)

# Макроэкономические индикаторы
macro_indicators = {
    "FEDFUNDS": "ФРС Ставка",  # Federal Funds Rate
    "CPIAUCSL": "Индекс инфляции",  # Consumer Price Index
    "UNRATE": "Уровень безработицы"  # Unemployment Rate
}

# Загружаем макроэкономику
macro_data = {name: fred.get_series(code) for code, name in macro_indicators.items()}
macro_df = pd.DataFrame(macro_data).resample("ME").last().dropna()

# === DATA LOADING ===
def load_stock_data_from_yahoo(tickers, start="2020-01-01", end="2025-01-01"):
    """Загружает данные о ценах акций с Yahoo Finance."""
    data = yf.download(tickers, start=start, end=end)

    # Проверка, что данные содержат 'Adj Close', если нет - использовать 'Close'
    adj_close_data = pd.DataFrame()
    for ticker in tickers:
        if ('Adj Close', ticker) in data.columns:
            adj_close_data[ticker] = data[('Adj Close', ticker)]
        else:
            adj_close_data[ticker] = data[('Close', ticker)]  # используем 'Close', если нет 'Adj Close'

    return adj_close_data


def load_stock_data(tickers, start="2020-01-01", end="2025-01-01"):
    """Загружает данные о ценах акций с Yahoo Finance, затем из Alpha Vantage при необходимости."""
    data = load_stock_data_from_yahoo(tickers, start, end)
    missing_tickers = [ticker for ticker in tickers if ticker not in data.columns]

    if missing_tickers:
        print(f"Отсутствующие тикеры в Yahoo: {', '.join(missing_tickers)}. Пытаемся загрузить данные из Alpha Vantage...")
        for ticker in missing_tickers:
            alpha_data = get_stock_data_from_alpha_vantage(ticker)
            if alpha_data:
                dates = pd.to_datetime(list(alpha_data.keys()))
                close_prices = [float(alpha_data[date]['4. close']) for date in alpha_data]
                alpha_df = pd.DataFrame(close_prices, index=dates, columns=[ticker])
                data = pd.concat([data, alpha_df], axis=1)
            time.sleep(12)  # Ждем 12 секунд, чтобы не превышать лимит запросов
    return data

# === CLUSTERING AND OPTIMIZATION ===

def portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):
    """Вычисляет производительность портфеля (коэффициент Шарпа)."""
    port_return = np.dot(weights, mean_returns)
    port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    sharpe = (port_return - risk_free_rate) / port_risk
    return -sharpe  # Минимизируем отрицательное значение Шарпа

import numpy as np
from scipy.optimize import minimize

# === CLUSTERING AND OPTIMIZATION ===


def optimize_cluster(cluster_tickers, returns, macro_data):
    """Оптимизация портфеля для одного кластера тикеров с учетом макроэкономических данных."""

    # Извлекаем доходности для тикеров в кластере
    cluster_returns = returns[cluster_tickers]

    # Расчет годовой доходности и ковариации
    mu = cluster_returns.mean() * 252  # Годовая доходность
    cov = cluster_returns.cov() * 252  # Годовая ковариация

    # Переименование столбцов макроэкономических данных для стандартизации
    macro_data.rename(columns={
        'ФРС Ставка': 'FEDFUNDS',
        'Индекс инфляции': 'CPIAUCSL',
        'Уровень безработицы': 'UNRATE'
    }, inplace=True)

    # Проверка наличия нужных макроэкономических данных
    if 'FEDFUNDS' not in macro_data.columns or 'CPIAUCSL' not in macro_data.columns or 'UNRATE' not in macro_data.columns:
        print("Ошибка: макроэкономические данные неполные. Необходимо иметь данные для FEDFUNDS, CPIAUCSL, UNRATE.")
        print(f"Доступные данные: {macro_data.columns.tolist()}")
        return None  # Прерываем оптимизацию, если данных не хватает

    # Получаем последние данные о макроэкономических индикаторах
    fed_funds_rate = macro_data['FEDFUNDS'][-1]  # Последнее значение ставки ФРС
    cpi = macro_data['CPIAUCSL'][-1]  # Последний индекс инфляции
    unemployment_rate = macro_data['UNRATE'][-1]  # Последний уровень безработицы

    # Адаптация доходности и ковариации в зависимости от макроэкономических факторов

    # Пример: уменьшение доходности при высокой ставке ФРС
    if fed_funds_rate > 2.5:
        mu *= 0.95  # Уменьшаем доходность, если ставка ФРС высокая

    # Пример: увеличение ковариации при высоком уровне инфляции
    if cpi > 3.0:
        cov *= 1.2  # Увеличиваем ковариацию, если инфляция высокая

    # Пример: увеличение ковариации при высоком уровне безработицы
    if unemployment_rate > 6.0:
        cov *= 1.1  # Увеличиваем ковариацию, если уровень безработицы высокий

    # Начальные веса: равномерное распределение
    init_w = np.ones(len(cluster_tickers)) / len(cluster_tickers)

    # Условия оптимизации: сумма весов = 1
    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})

    # Ограничения: веса от 0 до 1 для каждого актива
    bounds = tuple((0, 1) for _ in range(len(cluster_tickers)))

    # Оптимизация портфеля с использованием метода SLSQP
    opt = minimize(portfolio_performance, init_w, args=(mu, cov), method='SLSQP', bounds=bounds, constraints=constraints)

    return opt.x  # Возвращаем оптимальные веса для кластера


# === MAIN ===

def main():
    # Ввод тикеров через запятую
    tickers_input = input("Введите тикеры акций, разделенные запятой (например, AAPL, MSFT, TSLA): ")
    tickers = tickers_input.split(',')
    investment_amount = float(input("Введите сумму для инвестирования в USD: "))

    # Загружаем данные
    print("\nЗагрузка данных...")
    data = load_stock_data(tickers)
    returns = data.pct_change().dropna()

    # Загружаем макроэкономические данные
    print("\nЗагрузка макроэкономических данных...")
    if len(macro_df) == 0:
        print("Ошибка: не удалось загрузить макроэкономические данные.")
        return

    # Кластеризация и оптимизация
    print("\nОптимизация портфеля...")
    cluster_results = {}
    cluster_results[tuple(tickers)] = optimize_


def portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):
    """Вычисляет производительность портфеля (коэффициент Шарпа)."""
    port_return = np.dot(weights, mean_returns)
    port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
    sharpe = (port_return - risk_free_rate) / port_risk
    return -sharpe  # Минимизируем отрицательное значение Шарпа


# === CALCULATE FINAL ALLOCATION ===
'''
Эффективный фронт — это график, который показывает наилучшие возможные сочетания риска и доходности для портфеля активов. Оптимизация с учётом эффективного фронта позволяет выбрать такие веса активов, которые максимизируют доходность при заданном уровне риска (или минимизируют риск при заданной доходности).

Влияние на портфель:

Максимизация эффективности: Когда вы интегрируете веса с эффективного фронта, активы с высокими потенциалами (по доходности и низкому риску) будут занимать большее место в портфеле. Это помогает создать более эффективный портфель, который приносит больше доходности при тех же рисках или минимизирует риски при той же доходности.

Минимизация риска: Если целью является минимизация волатильности (риска), то активы с низкой корреляцией и низким риском будут иметь больший вес в портфеле, что снижает общий риск портфеля.
'''
def final_allocation(cluster_results, macro_df, returns, total_investment, mean_returns, cov_matrix):
    """Calculate the final investment allocation with respect to the effective frontier."""
    total_inverse_corr = 0
    cluster_allocations = {}

    # Calculate the budget share for each cluster
    for cluster, weights in cluster_results.items():
        num_portfolios = 10000
        portfolio_returns, portfolio_volatilities, sharpe_ratios = generate_random_portfolios(returns[weights], num_portfolios)

        # Optimizing based on efficient frontier
        initial_weights = np.ones(len(weights)) / len(weights)
        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
        bounds = tuple((0, 1) for _ in range(len(weights)))

        opt_results = minimize(portfolio_performance, initial_weights, args=(mean_returns[weights], cov_matrix[weights, :][:, weights]),
                               method='SLSQP', bounds=bounds, constraints=constraints)

        optimal_weights = opt_results.x

        # Calculate the inverse correlation (less correlated = more weight)
        avg_corr = np.mean([np.corrcoef(returns[weights].iloc[:, i], returns[weights].iloc[:, j])[0, 1] for i in range(len(weights)) for j in range(i+1, len(weights))])
        cluster_allocations[cluster] = 1 / (avg_corr + 1e-6)
        total_inverse_corr += cluster_allocations[cluster]

    # Normalize the budget share for each cluster
    for cluster in cluster_allocations:
        cluster_allocations[cluster] /= total_inverse_corr

    # Normalize the final weights for each asset within clusters
    final_weights = {}
    for cluster, weights in cluster_results.items():
        cluster_weight = cluster_allocations[cluster]
        for i, ticker in enumerate(weights):
            final_weights[ticker] = optimal_weights[i] * cluster_weight

    # Normalize the final weights
    total_weight = sum(final_weights.values())
    for ticker in final_weights:
        final_weights[ticker] /= total_weight

    return final_weights


# --- MAIN ---

def main():
    # Ввод тикеров
    tickers = input("Введите тикеры для анализа (через запятую): ").split(',')

    # Выбор начальной и конечной даты
    start_date = get_start_date()
    end_date = get_end_date()

    # Загрузка данных для каждого тикера
    data = load_stock_data(tickers, start_date, end_date)

    # Проверка на отсутствующие значения
    missing_tickers = [ticker for ticker in tickers if ticker not in data.columns]
    if data.isnull().values.any() or missing_tickers:
        print("Некоторые тикеры отсутствуют или содержат отсутствующие данные. Проверьте:")
        print(f"Список тикеров, не включенных в анализ из-за отсутствующих данных: {', '.join(missing_tickers)}")
        return

    # Период инвестирования
    period_months = int(input("Введите период инвестирования в месяцах: "))

    # Расчет доходности и риска на основе выбранного периода
    mean_returns, cov_matrix, std_deviation = calculate_returns(data, period_months)

    # Генерация случайных портфелей и расчет эффективного фронта
    num_portfolios = 10000
    portfolio_returns, portfolio_volatilities, sharpe_ratios = generate_random_portfolios(data, num_portfolios)

    # Построение эффективного фронта
    plot_efficient_frontier(portfolio_returns, portfolio_volatilities, sharpe_ratios)

    # Оценка диверсификации (корреляция между активами)
    correlation_matrix = data.pct_change().corr()
    print("\nКорреляционная матрица между активами:")
    print(correlation_matrix)

    # Построение матрицы корреляции
    plot_correlation_matrix(data.pct_change().dropna())

    # Оптимизация портфеля по коэффициенту Шарпа с учетом корреляции
    initial_weights = np.ones(len(data.columns)) / len(data.columns)  # Равномерное распределение
    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    bounds = tuple((0, 1) for _ in range(len(data.columns)))  # Веса от 0 до 100%

    # Оптимизация портфеля по коэффициенту Шарпа с учетом корреляции
    opt_results = minimize(portfolio_performance, initial_weights, args=(mean_returns, cov_matrix),
                           method='SLSQP', bounds=bounds, constraints=constraints)

    optimal_weights = opt_results.x
    optimal_returns = np.dot(optimal_weights, mean_returns)
    optimal_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
    optimal_sharpe = (optimal_returns - 0.02) / optimal_risk

    # Оптимизация портфеля для минимизации волатильности с учетом корреляции
    min_vol_results = minimize(min_volatility, initial_weights, args=(mean_returns, cov_matrix),
                               method='SLSQP', bounds=bounds, constraints=constraints)

    min_vol_weights = min_vol_results.x
    min_vol_risk = np.sqrt(np.dot(min_vol_weights.T, np.dot(cov_matrix, min_vol_weights)))
    min_vol_return = np.dot(min_vol_weights, mean_returns)

    # Вывод результатов
    print("\nСредняя доходность и риск активов:")  # Печать средней доходности и риска для каждого актива
    for ticker, ret, risk in zip(data.columns, mean_returns, std_deviation):
        print(f"{ticker}: Доходность = {ret:.2%}, Риск = {risk:.2%}")

    # Вывод результатов оптимизации
    print_results("Оптимальный портфель", optimal_returns, optimal_risk, optimal_sharpe)
    print_results("Портфель с минимальной волатильностью", min_vol_return, min_vol_risk)

    # Расчет инвестиционного распределения для обоих портфелей
    investment_amount = float(input("Введите сумму для инвестирования в USD: "))
    print("\nИнвестиционное распределение (Оптимальный портфель):")
    print_investment_allocation(optimal_weights, investment_amount, data.columns)

    print("\nИнвестиционное распределение (Портфель с минимальной волатильностью):")
    print_investment_allocation(min_vol_weights, investment_amount, data.columns)

    # Получаем итоговое распределение с учетом кластеров и макроэкономических данных
    print("\nПолучаем итоговое распределение...")
    final_weights = final_allocation(cluster_results, macro_df, mean_returns, cov_matrix, investment_amount)

    # Выводим итоговое распределение инвестиций
    print("\nИтоговое распределение:")
    for ticker, weight in final_weights.items():
        print(f"{ticker}: {weight * 100:.2f}%")

'''
есть несколько акций в портфеле. Каждая из этих акций имеет свою волатильность (это мера того, насколько сильно её цена может колебаться). Но когда эти акции комбинируются в одном портфеле, важно учитывать не только их индивидуальные колебания, но и то, как эти акции ведут себя друг относительно друга.

Корреляция — это мера того, насколько сильно акции «движутся» вместе. Например:

Если две акции имеют положительную корреляцию, это значит, что когда одна растет в цене, другая тоже, и наоборот.

Если две акции имеют отрицательную корреляцию, это значит, что когда одна растет в цене, другая падает.

Теперь, чтобы узнать волатильность портфеля, мы должны учесть, как ведут себя все акции вместе. Даже если каждая акция в портфеле может сильно колебаться, их комбинация может быть менее волатильной, если они слабо коррелируют (например, если одна акция растет, а другая падает, они могут сбалансировать друг друга).

Вот как работает функция portfolio_volatility:
Она берет все возможные корреляции между акциями (например, как одна акция изменяется по сравнению с другой).

Она использует ковариационную матрицу, чтобы вычислить, как все акции взаимодействуют друг с другом.

Затем она вычисляет, насколько весь портфель будет волатильным, принимая во внимание, как активы комбинируются, а не только их отдельную волатильность.

Таким образом, функция помогает нам понять, насколько рискованным будет весь портфель, если учесть не только отдельные акции, но и их взаимосвязь (корреляцию).

"""project/
│
├── main.py               # Главный скрипт, который использует все модули
├── module_data.py        # Модуль для загрузки данных
├── module_optimization.py# Модуль для оптимизации портфеля
├── module_visualization.py# Модуль для визуализации
└── module_date.py        # Модуль для обработки дат
- module_forecasting.py # в разработке (пока что не учитывай)

## display_best_portfolios.py
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile display_best_portfolios.py
# """
# Portfolio Performance Display Utility
# 
# This module provides functions to analyze and display the best performing portfolios
# based on different optimization criteria.
# """
# 
# import numpy as np
# import pandas as pd
# from typing import List, Optional, Dict, Union, Tuple
# 
# def display_best_portfolios(
#     portfolio_returns: np.ndarray,
#     portfolio_volatilities: np.ndarray,
#     sharpe_ratios: np.ndarray,
#     weights_record: Optional[np.ndarray] = None,
#     asset_names: Optional[List[str]] = None,
#     risk_free_rate: float = 0.02
# ) -> Dict[str, Dict[str, Union[float, np.ndarray]]]:
#     """
#     Analyze and display the best performing portfolios.
# 
#     Args:
#         portfolio_returns (np.ndarray): Array of portfolio returns
#         portfolio_volatilities (np.ndarray): Array of portfolio volatilities
#         sharpe_ratios (np.ndarray): Array of Sharpe ratios
#         weights_record (np.ndarray, optional): Array of portfolio weights
#         asset_names (List[str], optional): Names of assets in the portfolio
#         risk_free_rate (float, optional): Risk-free rate for Sharpe ratio calculation
# 
#     Returns:
#         Dict containing details of the best portfolios
#     """
#     # Validate input arrays
#     if not (len(portfolio_returns) == len(portfolio_volatilities) == len(sharpe_ratios)):
#         raise ValueError("Input arrays must have the same length")
# 
#     # Find indices of best portfolios
#     max_sharpe_idx = np.argmax(sharpe_ratios)
#     min_volatility_idx = np.argmin(portfolio_volatilities)
# 
#     # Prepare results dictionary
#     results = {
#         'max_sharpe': {
#             'return_rate': portfolio_returns[max_sharpe_idx],
#             'volatility': portfolio_volatilities[max_sharpe_idx],
#             'sharpe_ratio': sharpe_ratios[max_sharpe_idx]
#         },
#         'min_volatility': {
#             'return_rate': portfolio_returns[min_volatility_idx],
#             'volatility': portfolio_volatilities[min_volatility_idx],
#             'sharpe_ratio': sharpe_ratios[min_volatility_idx]
#         }
#     }
# 
#     # Print detailed information
#     print("\n=== Portfolio Performance Analysis ===")
# 
#     # Max Sharpe Ratio Portfolio
#     print("\nBest Portfolio (Maximum Sharpe Ratio):")
#     print(f"Annual Return: {results['max_sharpe']['return_rate']*100:.2f}%")
#     print(f"Annual Volatility: {results['max_sharpe']['volatility']*100:.2f}%")
#     print(f"Sharpe Ratio: {results['max_sharpe']['sharpe_ratio']:.4f}")
# 
#     # Minimum Volatility Portfolio
#     print("\nBest Portfolio (Minimum Volatility):")
#     print(f"Annual Return: {results['min_volatility']['return_rate']*100:.2f}%")
#     print(f"Annual Volatility: {results['min_volatility']['volatility']*100:.2f}%")
#     print(f"Sharpe Ratio: {results['min_volatility']['sharpe_ratio']:.4f}")
# 
#     # Display weights if available
#     if weights_record is not None:
#         max_sharpe_weights = weights_record[max_sharpe_idx]
#         min_vol_weights = weights_record[min_volatility_idx]
# 
#         results['max_sharpe']['weights'] = max_sharpe_weights
#         results['min_volatility']['weights'] = min_vol_weights
# 
#         # Print weights with asset names if provided
#         if asset_names is not None:
#             print("\nMax Sharpe Ratio Portfolio Weights:")
#             for name, weight in zip(asset_names, max_sharpe_weights):
#                 if weight > 0.01:  # Only show weights > 1%
#                     print(f"{name}: {weight*100:.2f}%")
# 
#             print("\nMinimum Volatility Portfolio Weights:")
#             for name, weight in zip(asset_names, min_vol_weights):
#                 if weight > 0.01:  # Only show weights > 1%
#                     print(f"{name}: {weight*100:.2f}%")
# 
#     return results
# 
# def compare_portfolio_performance(
#     portfolios: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],
#     portfolio_names: Optional[List[str]] = None,
#     asset_names: Optional[List[str]] = None
# ) -> pd.DataFrame:
#     """
#     Compare performance of multiple portfolio configurations.
# 
#     Args:
#         portfolios (List[Tuple]): List of portfolio performance tuples (returns, volatilities, sharpe_ratios)
#         portfolio_names (List[str], optional): Names for each portfolio configuration
#         asset_names (List[str], optional): Names of assets in the portfolio
# 
#     Returns:
#         pd.DataFrame: Comparative performance analysis
#     """
#     # Default portfolio names if not provided
#     if portfolio_names is None:
#         portfolio_names = [f"Portfolio {i+1}" for i in range(len(portfolios))]
# 
#     # Prepare performance data
#     performance_data = []
# 
#     for (returns, volatilities, sharpe_ratios), name in zip(portfolios, portfolio_names):
#         max_sharpe_idx = np.argmax(sharpe_ratios)
#         min_volatility_idx = np.argmin(volatilities)
# 
#         performance_data.append({
#             'Portfolio': name,
#             'Max Sharpe Return': returns[max_sharpe_idx] * 100,
#             'Max Sharpe Volatility': volatilities[max_sharpe_idx] * 100,
#             'Max Sharpe Ratio': sharpe_ratios[max_sharpe_idx],
#             'Min Volatility Return': returns[min_volatility_idx] * 100,
#             'Min Volatility Volatility': volatilities[min_volatility_idx] * 100,
#             'Min Volatility Sharpe Ratio': sharpe_ratios[min_volatility_idx]
#         })
# 
#     # Create DataFrame
#     performance_df = pd.DataFrame(performance_data)
#     performance_df.set_index('Portfolio', inplace=True)
# 
#     # Display comparative results
#     print("\n=== Portfolio Performance Comparison ===")
#     print(performance_df)
# 
#     return performance_df
# 
# def main():
#     """
#     Example usage of portfolio display functions
#     """
#     # Simulate some portfolio data
#     np.random.seed(42)
#     num_portfolios = 10000
#     num_assets = 5
# 
#     # Random returns and risk generation
#     returns = np.random.normal(0.1, 0.2, num_portfolios)
#     volatilities = np.random.normal(0.15, 0.1, num_portfolios)
#     sharpe_ratios = (returns - 0.02) / volatilities
# 
#     # Sample asset names
#     asset_names = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META']
# 
#     # Simulate weights
#     weights = np.random.dirichlet(np.ones(num_assets), num_portfolios)
# 
#     # Display best portfolios
#     display_best_portfolios(
#         returns,
#         volatilities,
#         sharpe_ratios,
#         weights,
#         asset_names
#     )
# 
# if __name__ == "__main__":
#     main()

"""## module_efficient_frontier_and_correlation.py

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_efficient_frontier_and_correlation.py
# """
# Module for efficient frontier calculation and advanced correlation analysis.
# 
# Этот модуль предоставляет функциональность для:
# 1. Вычисления эффективного фронта инвестиционного портфеля
# 2. Анализа корреляций между активами с учетом исторических данных и новостного фона
# 3. Оптимизации портфеля с использованием AI-анализа новостей через Anthropic API
# 4. Динамического определения оптимального веса новостного влияния на корреляции
# 5. Прогнозирования изменений волатильности на основе новостного анализа
# 6. Визуализации результатов анализа и оптимизации
# 
# Ключевая инновация модуля - интеграция с Anthropic API для анализа новостей и их влияния
# на взаимосвязи между компаниями, что позволяет создать более точную модель рыночных
# корреляций и оптимизировать инвестиционный портфель с учетом текущей рыночной ситуации.
# """
# 
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import anthropic
# import json
# import time
# import logging
# import os
# import re
# from datetime import datetime, timedelta
# from typing import List, Dict, Tuple, Optional, Union, Any
# from scipy.optimize import minimize
# 
# # Import other modules
# from module_investment_allocation import load_stock_data  # For loading stock data
# try:
#     from module_news_sentiment import get_company_news, analyze_company_sentiment  # For news analysis
# except ImportError:
#     pass  # Will handle this case later
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# # Add reference lines
#     ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
#     ax1.axvline(x=5, color='gray', linestyle='--', alpha=0.7)
# 
#     # Set labels and title
#     ax1.set_xlabel('Релевантность новости (0-10)')
#     ax1.set_ylabel('Сентимент (-5 to +5)')
#     ax1.set_title('Сентимент и Релевантность')
#     ax1.grid(True, alpha=0.3)
# 
#     # Add colorbar
#     cbar = plt.colorbar(scatter, ax=ax1)
#     cbar.set_label('Ожидаемое влияние на цену')
# 
#     # Plot expected price impact as bars
#     colors = ['green' if impact >= 0 else 'red' for impact in price_impacts]
#     ax2.bar(tickers, price_impacts, color=colors, alpha=0.7)
#     ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
# 
#     # Set labels and title
#     ax2.set_xlabel('Акции')
#     ax2.set_ylabel('Ожидаемое влияние на цену')
#     ax2.set_title('Прогнозируемое влияние на цены')
#     ax2.grid(True, alpha=0.3)
# 
#     # Rotate x-tick labels
#     plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
# 
#     # Add price impact values as text
#     for i, impact in enumerate(price_impacts):
#         ax2.annotate(f"{impact*100:+.1f}%",
#                    (i, impact),
#                    ha='center',
#                    va='bottom' if impact >= 0 else 'top',
#                    fontweight='bold')
# 
#     # Set overall title
#     fig.suptitle(title, fontsize=16)
# 
#     plt.tight_layout()
#     plt.subplots_adjust(top=0.9)
#     return fig
# 
# 
# def plot_portfolio_weights_comparison(traditional_weights: Dict, ai_weights: Dict,
#                                     optimized_weights: Dict) -> plt.Figure:
#     """
#     Plot comparison of portfolio weights from different methods.
# 
#     Args:
#         traditional_weights (dict): Traditional optimal weights
#         ai_weights (dict): AI-enhanced optimal weights
#         optimized_weights (dict): Final optimized weights
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     # Get common tickers
#     all_tickers = set()
#     for w_dict in [traditional_weights, ai_weights, optimized_weights]:
#         all_tickers.update(w_dict.keys())
#     tickers = sorted(list(all_tickers))
# 
#     # Create data for plotting
#     weights_data = {
#         'Traditional Max Sharpe': [traditional_weights.get(ticker, 0) for ticker in tickers],
#         'AI Max Sharpe': [ai_weights.get(ticker, 0) for ticker in tickers],
#         'Final Optimized': [optimized_weights.get(ticker, 0) for ticker in tickers]
#     }
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 8))
# 
#     # Set width of bars
#     bar_width = 0.25
#     r1 = np.arange(len(tickers))
#     r2 = [x + bar_width for x in r1]
#     r3 = [x + bar_width for x in r2]
# 
#     # Create bars
#     ax.bar(r1, weights_data['Traditional Max Sharpe'], width=bar_width, label='Традиционный', color='skyblue')
#     ax.bar(r2, weights_data['AI Max Sharpe'], width=bar_width, label='AI-улучшенный', color='salmon')
#     ax.bar(r3, weights_data['Final Optimized'], width=bar_width, label='Финальный', color='lightgreen')
# 
#     # Add labels and legend
#     ax.set_xlabel('Активы')
#     ax.set_ylabel('Вес в портфеле')
#     ax.set_title('Сравнение распределения весов портфеля')
#     ax.set_xticks([r + bar_width for r in range(len(tickers))])
#     ax.set_xticklabels(tickers, rotation=45, ha='right')
#     ax.legend()
# 
#     # Add value labels
#     for i, v1 in enumerate(weights_data['Traditional Max Sharpe']):
#         if v1 >= 0.05:  # Only show labels for weights >= 5%
#             ax.text(r1[i], v1 + 0.01, f'{v1:.1%}', ha='center', va='bottom', fontsize=8, rotation=90)
# 
#     for i, v2 in enumerate(weights_data['AI Max Sharpe']):
#         if v2 >= 0.05:
#             ax.text(r2[i], v2 + 0.01, f'{v2:.1%}', ha='center', va='bottom', fontsize=8, rotation=90)
# 
#     for i, v3 in enumerate(weights_data['Final Optimized']):
#         if v3 >= 0.05:
#             ax.text(r3[i], v3 + 0.01, f'{v3:.1%}', ha='center', va='bottom', fontsize=8, rotation=90)
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_volatility_forecast(volatility_forecast: pd.Series, title="Прогноз изменения волатильности") -> plt.Figure:
#     """
#     Plot volatility forecast adjustments.
# 
#     Args:
#         volatility_forecast (pd.Series): Volatility adjustment factors
#         title (str): Plot title
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     # Sort by adjustment factor
#     sorted_forecast = volatility_forecast.sort_values(ascending=False)
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Determine colors based on adjustment
#     colors = ['red' if v > 1.1 else 'green' if v < 0.9 else 'gray' for v in sorted_forecast]
# 
#     # Create bars
#     bars = ax.bar(sorted_forecast.index, sorted_forecast, color=colors, alpha=0.7)
# 
#     # Add reference line for "no change"
#     ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Без изменений')
# 
#     # Set labels and title
#     ax.set_xlabel('Акции')
#     ax.set_ylabel('Коэффициент изменения волатильности')
#     ax.set_title(title)
#     ax.grid(True, alpha=0.3)
# 
#     # Rotate x-tick labels
#     plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
# 
#     # Add value labels
#     for bar in bars:
#         height = bar.get_height()
#         if height >= 1.1 or height <= 0.9:  # Only label significant changes
#             ax.annotate(f'{height:.2f}x',
#                       xy=(bar.get_x() + bar.get_width()/2, height),
#                       xytext=(0, 3),  # 3 points vertical offset
#                       textcoords="offset points",
#                       ha='center', va='bottom',
#                       fontweight='bold')
# 
#     plt.tight_layout()
#     return fig
# 
# 
# # === USER INTERFACE FUNCTIONS ===
# 
# def generate_portfolio_report(results: Dict, output_format: str = 'html') -> str:
#     """
#     Generate a detailed portfolio analysis report.
# 
#     Args:
#         results (dict): Results from analyze_and_optimize_portfolio
#         output_format (str): Output format ('html', 'markdown', 'text')
# 
#     Returns:
#         str: Formatted report
#     """
#     tickers = list(results['optimized_portfolio']['weights'].keys())
#     optimal_weights = results['optimized_portfolio']['weights']
#     metrics = results['optimized_portfolio']['metrics']
#     news_weight = results.get('ai_determined_news_weight',
#                             results['optimized_portfolio'].get('news_blend_factor', 0.3))
# 
#     # Format metrics
#     expected_return = metrics['return'] * 100
#     expected_risk = metrics['risk'] * 100
#     sharpe_ratio = metrics['sharpe']
# 
#     # Traditional vs AI comparison
#     trad_sharpe = results['traditional_optimization']['max_sharpe']['sharpe_ratio']
#     ai_sharpe = results.get('ai_optimization', {}).get('max_sharpe', {}).get('sharpe_ratio', 0)
#     sharpe_improvement = ((sharpe_ratio - trad_sharpe) / trad_sharpe * 100) if trad_sharpe > 0 else 0
# 
#     # Create report content
#     if output_format == 'html':
#         # HTML report
#         report = f"""
#         <html>
#         <head>
#             <title>Smart Portfolio AI Report</title>
#             <style>
#                 body {{ font-family: Arial, sans-serif; line-height: 1.6; max-width: 1100px; margin: 0 auto; padding: 20px; }}
#                 h1, h2, h3 {{ color: #2c3e50; }}
#                 table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
#                 th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
#                 th {{ background-color: #f2f2f2; }}
#                 tr:nth-child(even) {{ background-color: #f9f9f9; }}
#                 .highlight {{ font-weight: bold; color: #27ae60; }}
#                 .warning {{ font-weight: bold; color: #e74c3c; }}
#                 .metrics {{ display: flex; justify-content: space-between; margin-bottom: 20px; }}
#                 .metric-box {{ padding: 15px; background-color: #f8f9fa; border-radius: 5px; width: 30%; text-align: center; }}
#                 .chart-container {{ margin: 20px 0; }}
#             </style>
#         </head>
#         <body>
#             <h1>Smart Portfolio AI Analysis Report</h1>
#             <p>Analysis performed on {len(tickers)} assets: {', '.join(tickers)}</p>
# 
#             <h2>Portfolio Performance Summary</h2>
#             <div class="metrics">
#                 <div class="metric-box">
#                     <h3>Expected Return</h3>
#                     <p class="highlight">{expected_return:.2f}%</p>
#                 </div>
#                 <div class="metric-box">
#                     <h3>Expected Risk</h3>
#                     <p class="{'highlight' if expected_risk < 15 else 'warning'}">{expected_risk:.2f}%</p>
#                 </div>
#                 <div class="metric-box">
#                     <h3>Sharpe Ratio</h3>
#                     <p class="{'highlight' if sharpe_ratio > 1 else 'warning'}">{sharpe_ratio:.2f}</p>
#                 </div>
#             </div>
# 
#             <h2>Optimized Portfolio Allocation</h2>
#             <table>
#                 <tr>
#                     <th>Asset</th>
#                     <th>Weight</th>
#                     <th>Allocation for $10,000</th>
#                 </tr>
#         """
# 
#         # Add rows for each asset
#         for ticker, weight in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True):
#             report += f"""
#                 <tr>
#                     <td>{ticker}</td>
#                     <td>{weight*100:.2f}%</td>
#                     <td>${weight*10000:.2f}</td>
#                 </tr>
#             """
# 
#         report += """
#             </table>
# 
#             <h2>AI Enhancement Impact</h2>
#         """
# 
#         if 'ai_determined_news_weight' in results:
#             report += f"""
#             <p>AI determined that news should influence the portfolio optimization by <span class="highlight">{news_weight*100:.1f}%</span>.</p>
#             """
# 
#         if 'custom_news_impact' in results:
#             report += f"""
#             <p>The provided news was analyzed and incorporated into the optimization strategy.</p>
#             """
# 
#         report += f"""
#             <p>The AI-enhanced approach resulted in a Sharpe ratio of <span class="highlight">{sharpe_ratio:.2f}</span>
#             compared to <span>{'%.2f' % trad_sharpe}</span> for the traditional approach,
#             representing a <span class="{'highlight' if sharpe_improvement > 0 else 'warning'}">{sharpe_improvement:.1f}%</span> {'improvement' if sharpe_improvement >= 0 else 'reduction'}.</p>
# 
#             <h2>Conclusion</h2>
#             <p>The Smart Portfolio AI analysis suggests a diversified portfolio with emphasis on
#             {', '.join([t for t, w in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True)[:3]])}.</p>
#             <p>This portfolio balances expected return and risk, while incorporating insights from {'provided news and ' if 'custom_news_impact' in results else ''}
#             AI analysis of market relationships.</p>
# 
#             <h3>Next Steps</h3>
#             <ul>
#                 <li>Review the suggested allocation and adjust based on your investment goals</li>
#                 <li>Consider the impact of news and market conditions on your investment decisions</li>
#                 <li>Rebalance periodically to maintain optimal allocation</li>
#             </ul>
# 
#             <p><em>Analysis generated by Smart Portfolio AI on {datetime.now().strftime('%Y-%m-%d %H:%M')}</em></p>
#         </body>
#         </html>
#         """
# 
#     elif output_format == 'markdown':
#         # Markdown report
#         report = f"""
#         # Smart Portfolio AI Analysis Report
# 
#         Analysis performed on {len(tickers)} assets: {', '.join(tickers)}
# 
#         ## Portfolio Performance Summary
# 
#         - **Expected Return**: {expected_return:.2f}%
#         - **Expected Risk**: {expected_risk:.2f}%
#         - **Sharpe Ratio**: {sharpe_ratio:.2f}
# 
#         ## Optimized Portfolio Allocation
# 
#         | Asset | Weight | Allocation for $10,000 |
#         |-------|--------|----------------------|
#         """
# 
#         # Add rows for each asset
#         for ticker, weight in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True):
#             report += f"| {ticker} | {weight*100:.2f}% | ${weight*10000:.2f} |\n"
# 
#         report += f"""
# 
#         ## AI Enhancement Impact
# 
#         """
# 
#         if 'ai_determined_news_weight' in results:
#             report += f"""
#         AI determined that news should influence the portfolio optimization by **{news_weight*100:.1f}%**.
# 
#         """
# 
#         if 'custom_news_impact' in results:
#             report += f"""
#         The provided news was analyzed and incorporated into the optimization strategy.
# 
#         """
# 
#         report += f"""
#         The AI-enhanced approach resulted in a Sharpe ratio of **{sharpe_ratio:.2f}**
#         compared to {trad_sharpe:.2f} for the traditional approach,
#         representing a **{sharpe_improvement:.1f}%** {'improvement' if sharpe_improvement >= 0 else 'reduction'}.
# 
#         ## Conclusion
# 
#         The Smart Portfolio AI analysis suggests a diversified portfolio with emphasis on
#         {', '.join([t for t, w in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True)[:3]])}.
# 
#         This portfolio balances expected return and risk, while incorporating insights from {'provided news and ' if 'custom_news_impact' in results else ''}
#         AI analysis of market relationships.
# 
#         ### Next Steps
# 
#         - Review the suggested allocation and adjust based on your investment goals
#         - Consider the impact of news and market conditions on your investment decisions
#         - Rebalance periodically to maintain optimal allocation
# 
#         *Analysis generated by Smart Portfolio AI on {datetime.now().strftime('%Y-%m-%d %H:%M')}*
#         """
# 
#     else:
#         # Plain text report
#         report = f"""
#         Smart Portfolio AI Analysis Report
#         ================================
# 
#         Analysis performed on {len(tickers)} assets: {', '.join(tickers)}
# 
#         Portfolio Performance Summary:
#         -----------------------------
#         Expected Return: {expected_return:.2f}%
#         Expected Risk: {expected_risk:.2f}%
#         Sharpe Ratio: {sharpe_ratio:.2f}
# 
#         Optimized Portfolio Allocation:
#         -----------------------------
#         """
# 
#         # Add rows for each asset
#         for ticker, weight in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True):
#             report += f"{ticker}: {weight*100:.2f}% (${weight*10000:.2f} for $10,000 investment)\n"
# 
#         report += f"""
# 
#         AI Enhancement Impact:
#         -------------------
#         """
# 
#         if 'ai_determined_news_weight' in results:
#             report += f"""
#         AI determined that news should influence the portfolio optimization by {news_weight*100:.1f}%.
#         """
# 
#         if 'custom_news_impact' in results:
#             report += f"""
#         The provided news was analyzed and incorporated into the optimization strategy.
#         """
# 
#         report += f"""
#         The AI-enhanced approach resulted in a Sharpe ratio of {sharpe_ratio:.2f}
#         compared to {trad_sharpe:.2f} for the traditional approach,
#         representing a {sharpe_improvement:.1f}% {'improvement' if sharpe_improvement >= 0 else 'reduction'}.
# 
#         Conclusion:
#         ----------
#         The Smart Portfolio AI analysis suggests a diversified portfolio with emphasis on
#         {', '.join([t for t, w in sorted(optimal_weights.items(), key=lambda x: x[1], reverse=True)[:3]])}.
# 
#         This portfolio balances expected return and risk, while incorporating insights from {'provided news and ' if 'custom_news_impact' in results else ''}
#         AI analysis of market relationships.
# 
#         Next Steps:
#         ----------
#         - Review the suggested allocation and adjust based on your investment goals
#         - Consider the impact of news and market conditions on your investment decisions
#         - Rebalance periodically to maintain optimal allocation
# 
#         Analysis generated by Smart Portfolio AI on {datetime.now().strftime('%Y-%m-%d %H:%M')}
#         """
# 
#     return report
# 
# 
# # === MAIN FUNCTION FOR TESTING ===
# 
# def main():
#     """Main function to test the enhanced efficient frontier module."""
#     # Example tickers
#     tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'JPM']
# 
#     # Test analysis and optimization
#     start_date = '2022-01-01'
#     end_date = '2023-01-01'
# 
#     print(f"Analyzing portfolio for {len(tickers)} tickers from {start_date} to {end_date}...")
# 
#     # Example of custom news
#     custom_news = """
#     The Federal Reserve announced today that it will maintain interest rates at current levels,
#     citing concerns about persistent inflation despite recent economic data showing a slowdown in price increases.
#     The decision surprised some analysts who had expected a rate cut given recent market volatility and signs of cooling in the labor market.
#     Tech companies, particularly cloud service providers, may benefit from the stable interest rate environment as businesses continue their digital transformation initiatives.
#     Meanwhile, financial institutions' profit margins might remain under pressure without the anticipated rate changes.
#     """
# 
#     # Run analysis with custom news
#     results = analyze_and_optimize_portfolio(
#         tickers,
#         start_date,
#         end_date,
#         optimization_method='sharpe',
#         news_blend_factor=None,  # Let AI determine the optimal weight
#         num_portfolios=2000,     # Lower for testing
#         use_ai_volatility=True,
#         custom_news=custom_news
#     )
# 
#     print("\nAnalysis complete! Results summary:")
#     print(f"Traditional Max Sharpe: {results['traditional_optimization']['max_sharpe']['sharpe_ratio']:.4f}")
#     if 'ai_optimization' in results:
#         print(f"AI Max Sharpe: {results['ai_optimization']['max_sharpe']['sharpe_ratio']:.4f}")
# 
#     print(f"Final Optimized Sharpe: {results['optimized_portfolio']['metrics']['sharpe']:.4f}")
# 
#     if 'ai_determined_news_weight' in results:
#         print(f"AI-determined news weight: {results['ai_determined_news_weight']*100:.1f}%")
# 
#     # Generate report
#     report = generate_portfolio_report(results, 'markdown')
#     print("\nGenerated Portfolio Report (excerpt):")
#     print(report[:500] + "...\n")
# 
#     return results
# 
# 
# if __name__ == '__main__':
#     main()                    # Set relationship in matrix (symmetric)
#                     relationship_matrix[i, j] = correlation
#                     relationship_matrix[j, i] = correlation
# 
#                     print(f"Analyzed {ticker1}-{ticker2}: correlation = {correlation:.2f}")
# 
#                 except (json.JSONDecodeError, ValueError) as e:
#                     logger.error(f"Error parsing response for {ticker1}-{ticker2}: {str(e)}")
#                     # Use a neutral correlation
#                     relationship_matrix[i, j] = 0.0
#                     relationship_matrix[j, i] = 0.0
# 
#             except Exception as e:
#                 logger.error(f"API error for {ticker1}-{ticker2}: {str(e)}")
#                 # Use a neutral correlation
#                 relationship_matrix[i, j] = 0.0
#                 relationship_matrix[j, i] = 0.0
# 
#             # Respect API rate limits
#             time.sleep(2)
# 
#     # Create DataFrame from matrix
#     relationship_df = pd.DataFrame(
#         relationship_matrix,
#         index=tickers,
#         columns=tickers
#     )
# 
#     return relationship_df
# 
# 
# def blend_correlation_matrices(historical_corr: pd.DataFrame, news_corr: pd.DataFrame,
#                               blend_factor: float = 0.5) -> pd.DataFrame:
#     """
#     Blend historical and news-based correlation matrices.
# 
#     Args:
#         historical_corr (pd.DataFrame): Historical correlation matrix
#         news_corr (pd.DataFrame): News-based correlation matrix
#         blend_factor (float): Weight for the news correlation (0-1)
# 
#     Returns:
#         pd.DataFrame: Blended correlation matrix
#     """
#     # Ensure consistent indexing
#     common_tickers = historical_corr.index.intersection(news_corr.index)
# 
#     if len(common_tickers) == 0:
#         logger.warning("No common tickers between correlation matrices. Returning historical correlation.")
#         return historical_corr
# 
#     # Align both matrices
#     hist_corr_aligned = historical_corr.loc[common_tickers, common_tickers]
#     news_corr_aligned = news_corr.loc[common_tickers, common_tickers]
# 
#     # Blend matrices using weighted average
#     blended_corr = (1 - blend_factor) * hist_corr_aligned + blend_factor * news_corr_aligned
# 
#     # Ensure diagonal is 1 and matrix is symmetric
#     np.fill_diagonal(blended_corr.values, 1.0)
#     blended_corr = (blended_corr + blended_corr.T) / 2
# 
#     return blended_corr
# 
# 
# def create_smart_covariance_matrix(returns: pd.DataFrame, blended_corr: pd.DataFrame,
#                                  volatility_forecast_adj: Optional[pd.Series] = None) -> pd.DataFrame:
#     """
#     Create a covariance matrix from returns data and blended correlation matrix.
# 
#     Args:
#         returns (pd.DataFrame): Historical returns
#         blended_corr (pd.DataFrame): Blended correlation matrix
#         volatility_forecast_adj (pd.Series, optional): Volatility forecast adjustments
# 
#     Returns:
#         pd.DataFrame: Smart covariance matrix
#     """
#     # Calculate standard deviations from historical returns
#     std_devs = returns.std()
# 
#     # Adjust volatilities if forecast provided
#     if volatility_forecast_adj is not None:
#         for ticker in volatility_forecast_adj.index:
#             if ticker in std_devs.index:
#                 std_devs[ticker] *= volatility_forecast_adj[ticker]
# 
#     # Create covariance matrix from correlation and std devs
#     n = len(blended_corr)
#     cov_matrix = pd.DataFrame(
#         np.zeros((n, n)),
#         index=blended_corr.index,
#         columns=blended_corr.columns
#     )
# 
#     for i, ticker1 in enumerate(blended_corr.index):
#         for j, ticker2 in enumerate(blended_corr.columns):
#             if ticker1 in std_devs.index and ticker2 in std_devs.index:
#                 cov_matrix.loc[ticker1, ticker2] = (
#                     blended_corr.loc[ticker1, ticker2] *
#                     std_devs[ticker1] *
#                     std_devs[ticker2]
#                 )
# 
#     return cov_matrix
# 
# 
# def forecast_volatility_changes(tickers: List[str], days_back: int = 30,
#                                api_key: Optional[str] = None) -> pd.Series:
#     """
#     Use Anthropic API to forecast changes in volatility for each ticker.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         days_back (int): Number of days of news to analyze
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         pd.Series: Volatility adjustment factors
#     """
#     client = initialize_anthropic_client(api_key)
# 
#     volatility_adjustments = {}
# 
#     for ticker in tickers:
#         try:
#             # Get news for the ticker
#             try:
#                 from module_news_sentiment import get_company_news
#                 news_articles = get_company_news(ticker, days_back=days_back)
#             except:
#                 news_articles = []
# 
#             # Prepare news summary
#             news_summary = ""
#             for i, article in enumerate(news_articles[:5]):  # Limit to top 5 articles
#                 news_summary += f"Article {i+1}: {article.get('title', 'No title')}\n"
#                 news_summary += f"Source: {article.get('source', {}).get('name', 'Unknown')}\n"
#                 news_summary += f"Description: {article.get('description', 'No description')}\n\n"
# 
#             if not news_summary:
#                 news_summary = "No recent news articles found."
# 
#             # Create prompt
#             prompt = f"""As a financial risk analyst, assess the likely change in volatility for {ticker} based on recent news:
# 
# Recent news summary:
# {news_summary}
# 
# Based on this news, should the historical volatility be adjusted up or down?
# Provide a multiplier where:
# - 1.0 means no change to historical volatility
# - >1.0 means increased volatility (e.g., 1.2 means 20% higher)
# - <1.0 means decreased volatility (e.g., 0.9 means 10% lower)
# 
# Return a float between 0.5 and 2.0, representing the volatility adjustment factor.
# """
# 
#             # Call API
#             response = client.messages.create(
#                 model="claude-3-haiku-20240307",
#                 max_tokens=100,
#                 temperature=0.0,
#                 system="You are a financial risk analyst providing volatility forecasts. Return a single number representing a volatility adjustment factor between 0.5 and 2.0.",
#                 messages=[
#                     {"role": "user", "content": prompt}
#                 ]
#             )
# 
#             response_text = response.content[0].text.strip()
# 
#             # Extract volatility adjustment factor
#             try:
#                 # Try to find a number in the response
#                 import re
#                 number_pattern = r'\d+\.\d+|\d+\,\d+|\d+'
#                 number_matches = re.findall(number_pattern, response_text)
# 
#                 if number_matches:
#                     # Get the first number
#                     adjustment = float(number_matches[0].replace(',', '.'))
#                     # Ensure it's within bounds
#                     adjustment = max(0.5, min(2.0, adjustment))
#                 else:
#                     # If no number found, estimate from text
#                     if "significantly higher" in response_text.lower() or "much higher" in response_text.lower():
#                         adjustment = 1.5
#                     elif "higher" in response_text.lower() or "increase" in response_text.lower():
#                         adjustment = 1.2
#                     elif "significantly lower" in response_text.lower() or "much lower" in response_text.lower():
#                         adjustment = 0.7
#                     elif "lower" in response_text.lower() or "decrease" in response_text.lower():
#                         adjustment = 0.9
#                     else:
#                         adjustment = 1.0  # No change
#             except:
#                 # Default to no change
#                 adjustment = 1.0
# 
#             volatility_adjustments[ticker] = adjustment
#             print(f"Volatility forecast for {ticker}: {adjustment:.2f}x")
# 
#         except Exception as e:
#             logger.error(f"Error forecasting volatility for {ticker}: {str(e)}")
#             volatility_adjustments[ticker] = 1.0  # Default to no change
# 
#         # Respect API rate limits
#         time.sleep(2)
# 
#     return pd.Series(volatility_adjustments)
# 
# 
# # === SMART PORTFOLIO OPTIMIZATION FUNCTIONS ===
# 
# def optimize_portfolio(returns: pd.DataFrame, cov_matrix: pd.DataFrame,
#                       method: str = 'sharpe', risk_free_rate: float = 0.0,
#                       target_return: Optional[float] = None,
#                       target_risk: Optional[float] = None) -> Tuple[np.ndarray, Dict]:
#     """
#     Optimize portfolio weights using various methods.
# 
#     Args:
#         returns (pd.DataFrame): Historical returns
#         cov_matrix (pd.DataFrame): Covariance matrix
#         method (str): Optimization method ('sharpe', 'min_risk', 'target_return', 'target_risk')
#         risk_free_rate (float): Risk-free rate
#         target_return (float, optional): Target return for 'target_return' method
#         target_risk (float, optional): Target risk for 'target_risk' method
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(returns.columns)
#     mean_returns = returns.mean()
# 
#     # Initial weights (equal allocation)
#     initial_weights = np.ones(num_assets) / num_assets
# 
#     # Constraints: weights sum to 1
#     constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Bounds: no short selling
#     bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     if method == 'sharpe':
#         # Maximize Sharpe ratio (minimize negative Sharpe)
#         def objective(weights):
#             port_return = np.dot(weights, mean_returns)
#             port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
#             return -(port_return - risk_free_rate) / port_risk
# 
#     elif method == 'min_risk':
#         # Minimize portfolio volatility
#         def objective(weights):
#             return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     elif method == 'target_return':
#         if target_return is None:
#             raise ValueError("target_return must be specified for 'target_return' method")
# 
#         # Minimize risk subject to target return
#         def objective(weights):
#             return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#         # Add constraint for target return
#         constraints.append({
#             'type': 'eq',
#             'fun': lambda weights: np.dot(weights, mean_returns) - target_return
#         })
# 
#     elif method == 'target_risk':
#         if target_risk is None:
#             raise ValueError("target_risk must be specified for 'target_risk' method")
# 
#         # Maximize return subject to target risk
#         def objective(weights):
#             return -np.dot(weights, mean_returns)
# 
#         # Add constraint for target risk
#         constraints.append({
#             'type': 'eq',
#             'fun': lambda weights: np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) - target_risk
#         })
# 
#     else:
#         raise ValueError(f"Unknown optimization method: {method}")
# 
#     # Optimize
#     result = minimize(
#         objective,
#         initial_weights,
#         method='SLSQP',
#         bounds=bounds,
#         constraints=constraints
#     )
# 
#     # Get optimal weights
#     optimal_weights = result['x']
# 
#     # Calculate performance metrics
#     optimal_return = np.dot(optimal_weights, mean_returns)
#     optimal_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
#     optimal_sharpe = (optimal_return - risk_free_rate) / optimal_risk if optimal_risk > 0 else 0
# 
#     performance_metrics = {
#         'return': optimal_return,
#         'risk': optimal_risk,
#         'sharpe': optimal_sharpe,
#         'success': result['success'],
#         'message': result['message']
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def analyze_and_optimize_portfolio(tickers: List[str], start_date: str, end_date: str,
#                                  optimization_method: str = 'sharpe',
#                                  risk_free_rate: float = 0.02,
#                                  news_blend_factor: Optional[float] = None,  # Now optional
#                                  num_portfolios: int = 5000,
#                                  use_ai_volatility: bool = True,
#                                  custom_news: Optional[str] = None,  # New parameter for custom news
#                                  api_key: Optional[str] = None) -> Dict:
#     """
#     Complete workflow for analyzing and optimizing a portfolio using both traditional and AI methods.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         start_date (str): Start date for historical data (YYYY-MM-DD)
#         end_date (str): End date for historical data (YYYY-MM-DD)
#         optimization_method (str): Optimization method ('sharpe', 'min_risk', 'target_return', 'target_risk')
#         risk_free_rate (float): Risk-free rate
#         news_blend_factor (float, optional): Weight of news-based correlations (0-1), if None will be determined by AI
#         num_portfolios (int): Number of random portfolios to generate
#         use_ai_volatility (bool): Whether to use AI to forecast volatility
#         custom_news (str, optional): Custom news text to analyze
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         dict: Complete analysis results
#     """
#     results = {}
# 
#     # Step 1: Load historical price data
#     print("Loading historical price data...")
#     price_data = load_stock_data(tickers, start_date, end_date)
# 
#     # Check if we have data for all tickers
#     valid_tickers = price_data.columns.tolist()
#     if len(valid_tickers) < len(tickers):
#         missing = set(tickers) - set(valid_tickers)
#         print(f"Warning: Missing data for tickers: {missing}")
#         tickers = valid_tickers
# 
#     # Calculate returns
#     returns = price_data.pct_change().dropna()
# 
#     # Step 2: Calculate historical correlation and covariance
#     print("Calculating historical correlation and covariance...")
#     historical_corr = returns.corr()
#     historical_cov = returns.cov()
# 
#     results['historical_correlation'] = historical_corr
#     results['historical_covariance'] = historical_cov
# 
#     # Step 3: Generate random portfolios using historical data
#     print("Generating efficient frontier with historical data...")
#     portfolio_returns, portfolio_volatilities, sharpe_ratios, weights_record = generate_random_portfolios(
#         returns, historical_cov, num_portfolios, risk_free_rate
#     )
# 
#     results['efficient_frontier'] = {
#         'returns': portfolio_returns,
#         'volatilities': portfolio_volatilities,
#         'sharpe_ratios': sharpe_ratios
#     }
# 
#     # Step 4: Display traditional optimization results
#     traditional_results = display_best_portfolios(
#         portfolio_returns, portfolio_volatilities, sharpe_ratios, weights_record, tickers
#     )
# 
#     results['traditional_optimization'] = traditional_results
# 
#     # Step 5: Process custom news if provided
#     if custom_news:
#         print("Analyzing custom news impact...")
#         news_impact = analyze_news_impact(custom_news, tickers, api_key)
#         results['custom_news_impact'] = news_impact
# 
#         # Update correlation matrix based on news
#         updated_corr = update_correlation_matrix_from_news(historical_corr, news_impact)
#         results['news_correlation'] = updated_corr
# 
#         # Print news impact summary
#         print("\nCustom News Impact Summary:")
#         for ticker, impact in news_impact.items():
#             print(f"{ticker}:")
#             print(f"  Relevance: {impact['relevance']}/10")
#             print(f"  Sentiment: {impact['sentiment']} (-5 to +5)")
#             print(f"  Price Impact: {impact['price_impact']}")
#             print(f"  Volatility Impact: {impact['volatility_impact']}")
#             print(f"  Explanation: {impact['explanation'][:100]}...")
# 
#     # Step 6: Get news-based correlation matrix (if no custom news provided)
#     else:
#         print("Analyzing news and company relationships with AI...")
#         try:
#             news_correlation = analyze_company_relationships(tickers, days_back=30, api_key=api_key)
#             results['news_correlation'] = news_correlation
#         except Exception as e:
#             logger.error(f"Error analyzing company relationships: {str(e)}")
#             print(f"Error analyzing company relationships: {str(e)}")
#             # Use historical correlation as fallback
#             news_correlation = historical_corr.copy()
#             results['news_correlation'] = news_correlation
# 
#     # Step 7: Determine optimal news blend factor if not provided
#     if news_blend_factor is None:
#         print("Determining optimal news weight with AI...")
#         try:
#             # Get some market conditions info
#             vol_index = None
#             try:
#                 # Try to get VIX data if available
#                 vix_data = load_stock_data(['^VIX'], start_date, end_date)
#                 if not vix_data.empty:
#                     vol_index = vix_data['^VIX'].iloc[-1]
#             except:
#                 pass
# 
#             market_conditions = {
#                 'recent_volatility': f"{'High' if vol_index and vol_index > 20 else 'Moderate' if vol_index and vol_index > 15 else 'Low'}",
#                 'tickers_analyzed': len(tickers),
#                 'average_stock_volatility': f"{returns.std().mean() * np.sqrt(252):.2%} (annualized)",
#                 'time_period': f"{start_date} to {end_date}",
#                 'ticker_list': ", ".join(tickers[:5]) + ("..." if len(tickers) > 5 else "")
#             }
# 
#             # Determine optimal weight
#             news_blend_factor = determine_optimal_news_weight(
#                 price_data, tickers, market_conditions, api_key
#             )
# 
#             results['ai_determined_news_weight'] = news_blend_factor
# 
#         except Exception as e:
#             logger.error(f"Error determining news weight: {str(e)}")
#             print(f"Error determining news weight: {str(e)}")
#             # Use default if determination fails
#             news_blend_factor = 0.3
#             results['ai_determined_news_weight'] = news_blend_factor
#             print(f"Using default news weight: {news_blend_factor}")
# 
#     # Step 8: Blend correlation matrices
#     print(f"Blending correlation matrices (news weight: {news_blend_factor:.2%})...")
#     blended_correlation = blend_correlation_matrices(
#         historical_corr, news_correlation, blend_factor=news_blend_factor
#     )
#     results['blended_correlation'] = blended_correlation
# 
#     # Step 9: Get AI volatility forecasts if requested
#     if use_ai_volatility:
#         try:
#             print("Forecasting volatility changes with AI...")
#             volatility_forecast = forecast_volatility_changes(tickers, days_back=30, api_key=api_key)
#             results['volatility_forecast'] = volatility_forecast
#         except Exception as e:
#             logger.error(f"Error forecasting volatility: {str(e)}")
#             print(f"Error forecasting volatility: {str(e)}")
#             volatility_forecast = pd.Series(1.0, index=tickers)  # Default: no change
#             results['volatility_forecast'] = volatility_forecast
#     else:
#         volatility_forecast = pd.Series(1.0, index=tickers)  # Default: no change
# 
#     # Step 10: Create smart covariance matrix
#     print("Creating smart covariance matrix...")
#     smart_covariance = create_smart_covariance_matrix(
#         returns, blended_correlation, volatility_forecast
#     )
#     results['smart_covariance'] = smart_covariance
# 
#     # Step 11: Generate random portfolios using smart covariance
#     print("Generating efficient frontier with AI-enhanced data...")
#     ai_portfolio_returns, ai_portfolio_volatilities, ai_sharpe_ratios, ai_weights_record = generate_random_portfolios(
#         returns, smart_covariance, num_portfolios, risk_free_rate
#     )
# 
#     results['ai_efficient_frontier'] = {
#         'returns': ai_portfolio_returns,
#         'volatilities': ai_portfolio_volatilities,
#         'sharpe_ratios': ai_sharpe_ratios
#     }
# 
#     # Step 12: Display AI-enhanced optimization results
#     ai_results = display_best_portfolios(
#         ai_portfolio_returns, ai_portfolio_volatilities, ai_sharpe_ratios, ai_weights_record, tickers
#     )
# 
#     results['ai_optimization'] = ai_results
# 
#     # Step 13: Optimize portfolio with specified method
#     print(f"Optimizing portfolio using {optimization_method} method...")
#     optimal_weights, performance_metrics = optimize_portfolio(
#         returns, smart_covariance, method=optimization_method, risk_free_rate=risk_free_rate
#     )
# 
#     results['optimized_portfolio'] = {
#         'weights': {ticker: weight for ticker, weight in zip(tickers, optimal_weights)},
#         'metrics': performance_metrics,
#         'news_blend_factor': news_blend_factor
#     }
# 
#     print("\nOptimized Portfolio Allocation:")
#     for ticker, weight in zip(tickers, optimal_weights):
#         print(f"{ticker}: {weight*100:.2f}%")
# 
#     print(f"\nExpected Return: {performance_metrics['return']*100:.2f}%")
#     print(f"Expected Risk: {performance_metrics['risk']*100:.2f}%")
#     print(f"Sharpe Ratio: {performance_metrics['sharpe']:.4f}")
#     print(f"News Influence: {news_blend_factor*100:.1f}%")
# 
#     return results
# 
# 
# # === VISUALIZATION FUNCTIONS FOR AI-ENHANCED ANALYSIS ===
# 
# def plot_correlation_comparison(historical_corr: pd.DataFrame, news_corr: pd.DataFrame,
#                                blended_corr: Optional[pd.DataFrame] = None) -> plt.Figure:
#     """
#     Plot comparison of historical and news-based correlation matrices.
# 
#     Args:
#         historical_corr (pd.DataFrame): Historical correlation matrix
#         news_corr (pd.DataFrame): News-based correlation matrix
#         blended_corr (pd.DataFrame, optional): Blended correlation matrix
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     n_plots = 3 if blended_corr is not None else 2
#     fig, axes = plt.subplots(1, n_plots, figsize=(n_plots*7, 6))
# 
#     # Plot historical correlation
#     sns.heatmap(historical_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                vmin=-1, vmax=1, center=0, ax=axes[0])
#     axes[0].set_title('Историческая корреляция')
# 
#     # Plot news-based correlation
#     sns.heatmap(news_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                vmin=-1, vmax=1, center=0, ax=axes[1])
#     axes[1].set_title('Корреляция на основе новостей (AI)')
# 
#     # Plot blended correlation if provided
#     if blended_corr is not None:
#         sns.heatmap(blended_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                    vmin=-1, vmax=1, center=0, ax=axes[2])
#         axes[2].set_title('Смешанная корреляция')
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_efficient_frontier_comparison(traditional_ef: Dict, ai_ef: Dict,
#                                      risk_free_rate: float = 0.02) -> plt.Figure:
#     """
#     Plot comparison of traditional and AI-enhanced efficient frontiers.
# 
#     Args:
#         traditional_ef (dict): Traditional efficient frontier data
#         ai_ef (dict): AI-enhanced efficient frontier data
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     fig, ax = plt.subplots(figsize=(10, 8))
# 
#     # Plot traditional efficient frontier
#     ax.scatter(traditional_ef['volatilities'], traditional_ef['returns'],
#               c=traditional_ef['sharpe_ratios'], cmap='viridis',
#               alpha=0.5, s=10, label='Традиционный анализ')
# 
#     # Find and mark optimal portfolios (traditional)
#     trad_max_sharpe_idx = np.argmax(traditional_ef['sharpe_ratios'])
#     trad_min_vol_idx = np.argmin(traditional_ef['volatilities'])
# 
#     ax.scatter(traditional_ef['volatilities'][trad_max_sharpe_idx],
#               traditional_ef['returns'][trad_max_sharpe_idx],
#               color='blue', marker='*', s=200,
#               label='Макс. Шарп (традиционный)')
# 
#     ax.scatter(traditional_ef['volatilities'][trad_min_vol_idx],
#               traditional_ef['returns'][trad_min_vol_idx],
#               color='blue', marker='o', s=150,
#               label='Мин. риск (традиционный)')
# 
#     # Plot AI-enhanced efficient frontier
#     ax.scatter(ai_ef['volatilities'], ai_ef['returns'],
#               c=ai_ef['sharpe_ratios'], cmap='plasma',
#               alpha=0.5, s=10, label='AI-улучшенный анализ')
# 
#     # Find and mark optimal portfolios (AI)
#     ai_max_sharpe_idx = np.argmax(ai_ef['sharpe_ratios'])
#     ai_min_vol_idx = np.argmin(ai_ef['volatilities'])
# 
#     ax.scatter(ai_ef['volatilities'][ai_max_sharpe_idx],
#               ai_ef['returns'][ai_max_sharpe_idx],
#               color='red', marker='*', s=200,
#               label='Макс. Шарп (AI)')
# 
#     ax.scatter(ai_ef['volatilities'][ai_min_vol_idx],
#               ai_ef['returns'][ai_min_vol_idx],
#               color='red', marker='o', s=150,
#               label='Мин. риск (AI)')
# 
#     # Add risk-free point and Capital Market Lines
#     ax.scatter([0], [risk_free_rate], color='black', marker='s', s=100, label='Безрисковый актив')
# 
#     # CML for traditional
#     trad_slope = (traditional_ef['returns'][trad_max_sharpe_idx] - risk_free_rate) / traditional_ef['volatilities'][trad_max_sharpe_idx]
#     x_range = np.linspace(0, max(traditional_ef['volatilities']) * 1.2, 100)
#     trad_cml = risk_free_rate + trad_slope * x_range
#     ax.plot(x_range, trad_cml, 'b--', alpha=0.7, label='CML (традиционный)')
# 
#     # CML for AI
#     ai_slope = (ai_ef['returns'][ai_max_sharpe_idx] - risk_free_rate) / ai_ef['volatilities'][ai_max_sharpe_idx]
#     ai_cml = risk_free_rate + ai_slope * x_range
#     ax.plot(x_range, ai_cml, 'r--', alpha=0.7, label='CML (AI)')
# 
#     # Set labels and title
#     ax.set_xlabel('Ожидаемый риск (волатильность)')
#     ax.set_ylabel('Ожидаемая доходность')
#     ax.set_title('Сравнение эффективных фронтов: традиционный vs. AI-улучшенный')
#     ax.grid(True, alpha=0.3)
#     ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_news_impact(news_impact: Dict[str, Dict[str, Any]], title: str = "Влияние новости на акции") -> plt.Figure:
#     """
#     Visualize the impact of news on stocks.
# 
#     Args:
#         news_impact (dict): News impact analysis from analyze_news_impact
#         title (str): Plot title
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     # Extract sentiment and relevance data
#     tickers = list(news_impact.keys())
#     sentiments = [news_impact[ticker]['sentiment'] for ticker in tickers]
#     relevances = [news_impact[ticker]['relevance'] for ticker in tickers]
# 
#     # Convert price impacts to numbers
#     price_impacts = []
#     for ticker in tickers:
#         impact_str = news_impact[ticker]['price_impact']
#         try:
#             # Extract number from string like "+2.5%" or "-1.2%"
#             impact_str = impact_str.replace('%', '').replace('+', '')
#             price_impacts.append(float(impact_str) / 100)
#         except:
#             price_impacts.append(0)
# 
#     # Create figure with two subplots
#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
# 
#     # Plot sentiment vs relevance
#     scatter = ax1.scatter(relevances, sentiments, c=price_impacts, cmap='RdYlGn',
#                          s=100, vmin=-0.05, vmax=0.05)
# 
#     # Add ticker labels
#     for i, ticker in enumerate(tickers):
#         ax1.annotate(ticker, (relevances[i], sentiments[i]),
#                     xytext=(5, 5), textcoords='offset points')
# 
#     # Add reference lines
#     ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
#     ax1.axvline(x=5, color='%%writefile module_efficient_frontier_and_correlation.py
# """
# Module for efficient frontier calculation and advanced correlation analysis.
# This module combines traditional portfolio theory with AI-powered news analysis
# to create a more accurate correlation matrix and optimize portfolio allocations.
# """
# 
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import anthropic
# import json
# import time
# import logging
# from datetime import datetime, timedelta
# from typing import List, Dict, Tuple, Optional, Union, Any
# from scipy.optimize import minimize
# 
# # Import other modules
# from module_investment_allocation import load_stock_data  # For loading stock data
# try:
#     from module_news_sentiment import get_company_news, analyze_company_sentiment  # For news analysis
# except ImportError:
#     pass  # Will handle this case later
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# # === TRADITIONAL PORTFOLIO OPTIMIZATION FUNCTIONS ===
# 
# def portfolio_volatility(weights: np.ndarray, cov_matrix: pd.DataFrame) -> float:
#     """
#     Calculate portfolio volatility based on weights and covariance matrix.
# 
#     Args:
#         weights (np.ndarray): Portfolio weights
#         cov_matrix (pd.DataFrame): Covariance matrix of returns
# 
#     Returns:
#         float: Portfolio volatility (standard deviation)
#     """
#     return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
# 
# def portfolio_return(weights: np.ndarray, returns: pd.DataFrame) -> float:
#     """
#     Calculate expected portfolio return based on weights and historical returns.
# 
#     Args:
#         weights (np.ndarray): Portfolio weights
#         returns (pd.DataFrame): Historical returns
# 
#     Returns:
#         float: Expected portfolio return
#     """
#     return np.sum(weights * returns.mean())
# 
# 
# def generate_random_portfolios(returns: pd.DataFrame, cov_matrix: Optional[pd.DataFrame] = None,
#                               num_portfolios: int = 10000, risk_free_rate: float = 0.0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
#     """
#     Generate random portfolios for efficient frontier analysis.
# 
#     Args:
#         returns (pd.DataFrame): Historical returns
#         cov_matrix (pd.DataFrame, optional): Covariance matrix (calculated from returns if not provided)
#         num_portfolios (int): Number of random portfolios to generate
#         risk_free_rate (float): Risk-free rate for Sharpe ratio calculation
# 
#     Returns:
#         tuple: (portfolio_returns, portfolio_volatilities, sharpe_ratios, weights_record)
#     """
#     # Calculate covariance matrix if not provided
#     if cov_matrix is None:
#         cov_matrix = returns.cov()
# 
#     portfolio_returns = []
#     portfolio_volatilities = []
#     sharpe_ratios = []
#     weights_record = []
# 
#     for _ in range(num_portfolios):
#         weights = np.random.random(len(returns.columns))
#         weights /= np.sum(weights)  # Normalize weights to sum to 1
# 
#         # Calculate portfolio metrics
#         port_return = portfolio_return(weights, returns)
#         port_volatility = portfolio_volatility(weights, cov_matrix)
# 
#         # Calculate Sharpe ratio
#         sharpe_ratio = (port_return - risk_free_rate) / port_volatility if port_volatility > 0 else 0
# 
#         # Store results
#         portfolio_returns.append(port_return)
#         portfolio_volatilities.append(port_volatility)
#         sharpe_ratios.append(sharpe_ratio)
#         weights_record.append(weights)
# 
#     return np.array(portfolio_returns), np.array(portfolio_volatilities), np.array(sharpe_ratios), np.array(weights_record)
# 
# 
# def plot_efficient_frontier(portfolio_returns: np.ndarray, portfolio_volatilities: np.ndarray,
#                            sharpe_ratios: np.ndarray, risk_free_rate: float = 0.0,
#                            show_optimal: bool = True, show_cml: bool = True) -> plt.Figure:
#     """
#     Plot the efficient frontier with optional Capital Market Line.
# 
#     Args:
#         portfolio_returns (np.ndarray): Array of portfolio returns
#         portfolio_volatilities (np.ndarray): Array of portfolio volatilities
#         sharpe_ratios (np.ndarray): Array of Sharpe ratios
#         risk_free_rate (float): Risk-free rate
#         show_optimal (bool): Whether to highlight optimal portfolios
#         show_cml (bool): Whether to show Capital Market Line
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     fig, ax = plt.subplots(figsize=(10, 6))
# 
#     # Create scatter plot of all generated portfolios
#     scatter = ax.scatter(portfolio_volatilities, portfolio_returns,
#                         c=sharpe_ratios, cmap='viridis',
#                         alpha=0.7, s=10)
# 
#     # Add colorbar
#     cbar = plt.colorbar(scatter)
#     cbar.set_label('Коэффициент Шарпа')
# 
#     if show_optimal:
#         # Find and mark the highest Sharpe ratio portfolio
#         max_sharpe_idx = np.argmax(sharpe_ratios)
#         ax.scatter(portfolio_volatilities[max_sharpe_idx], portfolio_returns[max_sharpe_idx],
#                   color='red', marker='*', s=300, label='Максимальный коэффициент Шарпа')
# 
#         # Find and mark the minimum volatility portfolio
#         min_vol_idx = np.argmin(portfolio_volatilities)
#         ax.scatter(portfolio_volatilities[min_vol_idx], portfolio_returns[min_vol_idx],
#                   color='green', marker='o', s=200, label='Минимальная волатильность')
# 
#     if show_cml and risk_free_rate is not None:
#         # Draw Capital Market Line
#         max_sharpe_idx = np.argmax(sharpe_ratios)
#         max_sharpe_return = portfolio_returns[max_sharpe_idx]
#         max_sharpe_vol = portfolio_volatilities[max_sharpe_idx]
# 
#         # Calculate slope of CML (Sharpe ratio of the optimal portfolio)
#         cml_slope = (max_sharpe_return - risk_free_rate) / max_sharpe_vol
# 
#         # Plot CML
#         x_range = np.linspace(0, max(portfolio_volatilities) * 1.2, 100)
#         cml = risk_free_rate + cml_slope * x_range
#         ax.plot(x_range, cml, 'r--', label='Capital Market Line')
# 
#         # Mark risk-free asset
#         ax.scatter([0], [risk_free_rate], color='black', marker='o', s=100, label='Безрисковый актив')
# 
#     # Set labels and title
#     ax.set_xlabel('Риск (Волатильность)')
#     ax.set_ylabel('Ожидаемая доходность')
#     ax.set_title('Эффективный фронт')
#     ax.grid(True, alpha=0.3)
#     ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_correlation_matrix(returns: pd.DataFrame, annotate: bool = True, cmap: str = 'coolwarm',
#                            title: str = 'Корреляционная матрица активов') -> plt.Figure:
#     """
#     Plot a correlation matrix heatmap for asset returns.
# 
#     Args:
#         returns (pd.DataFrame): Returns data with assets as columns
#         annotate (bool): Whether to annotate the heatmap with values
#         cmap (str): Matplotlib colormap name
#         title (str): Plot title
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     correlation_matrix = returns.corr()
# 
#     fig, ax = plt.subplots(figsize=(10, 8))
#     mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
# 
#     sns.heatmap(correlation_matrix, annot=annotate, cmap=cmap, linewidths=0.5,
#                mask=mask, vmin=-1, vmax=1, center=0, ax=ax)
# 
#     plt.title(title)
#     plt.tight_layout()
#     return fig
# 
# 
# def display_best_portfolios(portfolio_returns: np.ndarray, portfolio_volatilities: np.ndarray,
#                            sharpe_ratios: np.ndarray, weights_record: Optional[np.ndarray] = None,
#                            asset_names: Optional[List[str]] = None) -> Dict:
#     """
#     Calculate and display information about the best portfolios.
# 
#     Args:
#         portfolio_returns (np.ndarray): Array of portfolio returns
#         portfolio_volatilities (np.ndarray): Array of portfolio volatilities
#         sharpe_ratios (np.ndarray): Array of Sharpe ratios
#         weights_record (np.ndarray, optional): Array of weights for each portfolio
#         asset_names (list, optional): List of asset names for displaying weights
# 
#     Returns:
#         dict: Information about optimal portfolios
#     """
#     max_sharpe_idx = np.argmax(sharpe_ratios)
#     min_volatility_idx = np.argmin(portfolio_volatilities)
# 
#     # Print results
#     print("\nЛучший портфель по коэффициенту Шарпа:")
#     print(f"Доходность: {portfolio_returns[max_sharpe_idx] * 100:.2f}%")
#     print(f"Риск (волатильность): {portfolio_volatilities[max_sharpe_idx] * 100:.2f}%")
#     print(f"Коэффициент Шарпа: {sharpe_ratios[max_sharpe_idx]:.4f}")
# 
#     print("\nЛучший портфель по минимальной волатильности:")
#     print(f"Доходность: {portfolio_returns[min_volatility_idx] * 100:.2f}%")
#     print(f"Риск (волатильность): {portfolio_volatilities[min_volatility_idx] * 100:.2f}%")
#     print(f"Коэффициент Шарпа: {sharpe_ratios[min_volatility_idx]:.4f}")
# 
#     # If weights are provided, show the allocation
#     results = {
#         'max_sharpe': {
#             'return': portfolio_returns[max_sharpe_idx],
#             'volatility': portfolio_volatilities[max_sharpe_idx],
#             'sharpe_ratio': sharpe_ratios[max_sharpe_idx]
#         },
#         'min_volatility': {
#             'return': portfolio_returns[min_volatility_idx],
#             'volatility': portfolio_volatilities[min_volatility_idx],
#             'sharpe_ratio': sharpe_ratios[min_volatility_idx]
#         }
#     }
# 
#     if weights_record is not None:
#         max_sharpe_weights = weights_record[max_sharpe_idx]
#         min_vol_weights = weights_record[min_volatility_idx]
# 
#         results['max_sharpe']['weights'] = max_sharpe_weights
#         results['min_volatility']['weights'] = min_vol_weights
# 
#         if asset_names is not None:
#             print("\nРаспределение весов для портфеля с максимальным коэффициентом Шарпа:")
#             for i, asset in enumerate(asset_names):
#                 print(f"{asset}: {max_sharpe_weights[i]*100:.2f}%")
# 
#             print("\nРаспределение весов для портфеля с минимальной волатильностью:")
#             for i, asset in enumerate(asset_names):
#                 print(f"{asset}: {min_vol_weights[i]*100:.2f}%")
# 
#     return results
# 
# 
# # === AI-ENHANCED CORRELATION ANALYSIS FUNCTIONS ===
# 
# def initialize_anthropic_client(api_key: Optional[str] = None) -> anthropic.Anthropic:
#     """
#     Initialize the Anthropic client for API access.
# 
#     Args:
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         anthropic.Anthropic: Initialized Anthropic client
#     """
#     if api_key is None:
#         # Try to get API key from environment variables or configuration
#         try:
#             from module_news_sentiment import ANTHROPIC_API_KEY
#             api_key = ANTHROPIC_API_KEY
#         except (ImportError, AttributeError):
#             # Ask for API key if not found
#             api_key = input("Please enter your Anthropic API key: ")
# 
#     return anthropic.Anthropic(api_key=api_key)
# 
# 
# def determine_optimal_news_weight(historical_data: pd.DataFrame, tickers: List[str],
#                                 market_conditions: Dict[str, Any], api_key: Optional[str] = None) -> float:
#     """
#     Use Claude to determine the optimal weight for news-based analysis based on current market conditions.
# 
#     Args:
#         historical_data (pd.DataFrame): Historical price data
#         tickers (list): List of stock ticker symbols
#         market_conditions (dict): Dictionary with market conditions info
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         float: Optimal weight for news-based analysis (0-1)
#     """
#     client = initialize_anthropic_client(api_key)
# 
#     # Calculate some basic metrics
#     returns = historical_data.pct_change().dropna()
#     volatility = returns.std() * np.sqrt(252)  # Annualized
#     correlation = returns.corr()
#     avg_correlation = correlation.values[np.triu_indices_from(correlation.values, 1)].mean()
# 
#     # Prepare market conditions summary
#     market_summary = "\n".join([f"{key}: {value}" for key, value in market_conditions.items()])
# 
#     # Create prompt for Claude
#     prompt = f"""As a financial expert, determine the optimal weight to assign to news-based correlation analysis vs. historical correlation.
# 
# INFORMATION:
# 1. Portfolio consists of these assets: {', '.join(tickers)}
# 2. Average annualized volatility: {volatility.mean():.2%}
# 3. Average correlation between assets: {avg_correlation:.2f}
# 4. Current market conditions:
# {market_summary}
# 
# TASK:
# Determine what percentage weight should be given to news-based correlation analysis (vs. historical correlation) for optimal portfolio construction.
# - 0% means rely ONLY on historical correlations
# - 100% means rely ONLY on news-based correlations
# - Values in between blend both sources
# 
# Consider these factors:
# - Higher volatility typically means news has more impact
# - Market regime changes may reduce the reliability of historical data
# - Certain sectors are more news-sensitive than others
# - Recent significant news might warrant higher news weighting
# 
# Return a JSON object with:
# 1. "weight": numeric percentage (0-100)
# 2. "explanation": brief reasoning
# 3. "confidence": confidence level in your assessment (low/medium/high)
# """
# 
#     try:
#         # Call Anthropic API
#         response = client.messages.create(
#             model="claude-3-haiku-20240307",
#             max_tokens=500,
#             temperature=0.0,
#             system="You are a quantitative financial analyst specializing in determining optimal strategy parameters. Always return valid JSON with 'weight', 'explanation', and 'confidence' fields.",
#             messages=[
#                 {"role": "user", "content": prompt}
#             ]
#         )
# 
#         response_text = response.content[0].text
# 
#         # Parse JSON response
#         try:
#             # Find JSON in response
#             import re
#             json_pattern = r'({[\s\S]*?})'
#             json_match = re.search(json_pattern, response_text)
# 
#             if json_match:
#                 json_str = json_match.group(1)
#                 result = json.loads(json_str)
#                 weight = float(result.get('weight', 30)) / 100  # Convert percentage to 0-1 scale
#                 # Ensure value is in range [0, 1]
#                 weight = max(0.0, min(1.0, weight))
# 
#                 # Log the decision
#                 confidence = result.get('confidence', 'medium')
#                 explanation = result.get('explanation', 'No explanation provided')
#                 print(f"AI-determined news weight: {weight:.2%} (Confidence: {confidence})")
#                 print(f"Explanation: {explanation}")
# 
#                 return weight
#             else:
#                 # If no JSON found, estimate from text
#                 logger.warning(f"No JSON found in response. Estimating weight.")
#                 if "high" in response_text.lower() and "weight" in response_text.lower():
#                     return 0.6
#                 elif "moderate" in response_text.lower() and "weight" in response_text.lower():
#                     return 0.3
#                 elif "low" in response_text.lower() and "weight" in response_text.lower():
#                     return 0.1
#                 else:
#                     return 0.3  # Default moderate weight
# 
#         except (json.JSONDecodeError, ValueError) as e:
#             logger.error(f"Error parsing weight determination response: {str(e)}")
#             return 0.3  # Default if parsing fails
# 
#     except Exception as e:
#         logger.error(f"API error determining optimal weight: {str(e)}")
#         return 0.3  # Default if API fails
# 
# 
# def analyze_news_impact(news_text: str, tickers: List[str], api_key: Optional[str] = None) -> Dict[str, Dict[str, Any]]:
#     """
#     Analyze the potential impact of a specific news item on the stocks in the portfolio.
# 
#     Args:
#         news_text (str): The news text to analyze
#         tickers (list): List of stock ticker symbols
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         dict: Dictionary mapping tickers to their impact assessment
#     """
#     client = initialize_anthropic_client(api_key)
# 
#     # Get company names if possible
#     company_names = {}
#     for ticker in tickers:
#         try:
#             import yfinance as yf
#             company = yf.Ticker(ticker)
#             company_names[ticker] = company.info.get('shortName', ticker)
#         except:
#             company_names[ticker] = ticker
# 
#     # Create prompt for Claude
#     companies_str = ", ".join([f"{ticker} ({company_names[ticker]})" for ticker in tickers])
#     prompt = f"""As a financial analyst, assess the potential impact of this news on the following stocks: {companies_str}.
# 
# NEWS TEXT:
# {news_text}
# 
# For EACH company/ticker listed above, analyze:
# 1. Relevance: How directly relevant is this news to the company? (0-10 scale)
# 2. Sentiment: Is the implied sentiment positive or negative for the stock? (-5 to +5 scale)
# 3. Price Impact: Estimated percentage impact on stock price in the short term
# 4. Volatility Impact: Will this news likely increase or decrease stock volatility?
# 5. Correlation Impact: How might this news affect correlations with other stocks?
# 
# Return your analysis as a JSON object with each ticker as a key, like this:
# {{
#   "TICKER1": {{
#     "relevance": 8,
#     "sentiment": 4,
#     "price_impact": "+2.5%",
#     "volatility_impact": "increase",
#     "correlation_changes": [{"with": "TICKER2", "change": "increase"}],
#     "explanation": "Brief explanation of assessment"
#   }},
#   "TICKER2": {{ ... }}
# }}
# """
# 
#     try:
#         # Call Anthropic API
#         response = client.messages.create(
#             model="claude-3-haiku-20240307",
#             max_tokens=2000,
#             temperature=0.0,
#             system="You are a financial analyst specializing in news impact analysis. Always return valid JSON with your analysis for each requested ticker. Only include tickers that were specifically listed in the request.",
#             messages=[
#                 {"role": "user", "content": prompt}
#             ]
#         )
# 
#         response_text = response.content[0].text
# 
#         # Parse JSON response
#         try:
#             # Find JSON in response
#             import re
#             json_pattern = r'({[\s\S]*?})'
#             json_match = re.search(json_pattern, response_text)
# 
#             if json_match:
#                 json_str = json_match.group(1)
#                 results = json.loads(json_str)
# 
#                 # Validate and process results
#                 processed_results = {}
#                 for ticker in tickers:
#                     if ticker in results:
#                         # Extract and validate fields
#                         ticker_data = results[ticker]
#                         processed_results[ticker] = {
#                             'relevance': float(ticker_data.get('relevance', 0)),
#                             'sentiment': float(ticker_data.get('sentiment', 0)),
#                             'price_impact': ticker_data.get('price_impact', '0%'),
#                             'volatility_impact': ticker_data.get('volatility_impact', 'neutral'),
#                             'correlation_changes': ticker_data.get('correlation_changes', []),
#                             'explanation': ticker_data.get('explanation', 'No explanation provided')
#                         }
# 
#                 return processed_results
# 
#             else:
#                 # If no JSON found, return empty results
#                 logger.warning(f"No JSON found in news impact response.")
#                 return {ticker: {
#                     'relevance': 0,
#                     'sentiment': 0,
#                     'price_impact': '0%',
#                     'volatility_impact': 'neutral',
#                     'correlation_changes': [],
#                     'explanation': 'Failed to parse analysis'
#                 } for ticker in tickers}
# 
#         except (json.JSONDecodeError, ValueError) as e:
#             logger.error(f"Error parsing news impact response: {str(e)}")
#             return {ticker: {
#                 'relevance': 0,
#                 'sentiment': 0,
#                 'price_impact': '0%',
#                 'volatility_impact': 'neutral',
#                 'correlation_changes': [],
#                 'explanation': f'Error parsing analysis: {str(e)}'
#             } for ticker in tickers}
# 
#     except Exception as e:
#         logger.error(f"API error analyzing news impact: {str(e)}")
#         return {ticker: {
#             'relevance': 0,
#             'sentiment': 0,
#             'price_impact': '0%',
#             'volatility_impact': 'neutral',
#             'correlation_changes': [],
#             'explanation': f'API error: {str(e)}'
#         } for ticker in tickers}
# 
# 
# def update_correlation_matrix_from_news(correlation_matrix: pd.DataFrame,
#                                       news_impact: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
#     """
#     Update correlation matrix based on specific news impact analysis.
# 
#     Args:
#         correlation_matrix (pd.DataFrame): Existing correlation matrix
#         news_impact (dict): News impact analysis from analyze_news_impact
# 
#     Returns:
#         pd.DataFrame: Updated correlation matrix
#     """
#     updated_matrix = correlation_matrix.copy()
# 
#     # Process correlation changes from news impact
#     for ticker, impact in news_impact.items():
#         if ticker not in updated_matrix.index:
#             continue
# 
#         # Extract correlation changes
#         for change_info in impact.get('correlation_changes', []):
#             other_ticker = change_info.get('with')
#             change_direction = change_info.get('change')
# 
#             if other_ticker in updated_matrix.columns and change_direction:
#                 current_corr = updated_matrix.loc[ticker, other_ticker]
# 
#                 # Apply changes based on direction
#                 if change_direction == 'increase':
#                     # Increase correlation (move closer to 1 if positive, closer to -1 if negative)
#                     if current_corr >= 0:
#                         updated_matrix.loc[ticker, other_ticker] = min(1.0, current_corr + 0.1)
#                         updated_matrix.loc[other_ticker, ticker] = min(1.0, current_corr + 0.1)
#                     else:
#                         updated_matrix.loc[ticker, other_ticker] = max(-1.0, current_corr - 0.1)
#                         updated_matrix.loc[other_ticker, ticker] = max(-1.0, current_corr - 0.1)
# 
#                 elif change_direction == 'decrease':
#                     # Decrease correlation (move closer to 0)
#                     updated_matrix.loc[ticker, other_ticker] = current_corr * 0.9
#                     updated_matrix.loc[other_ticker, ticker] = current_corr * 0.9
# 
#                 elif change_direction == 'reverse':
#                     # Reverse correlation (flip sign but maintain magnitude)
#                     updated_matrix.loc[ticker, other_ticker] = -current_corr
#                     updated_matrix.loc[other_ticker, ticker] = -current_corr
# 
#     # Ensure diagonal is 1 and matrix is symmetric
#     np.fill_diagonal(updated_matrix.values, 1.0)
# 
#     return updated_matrix
# 
# 
# def analyze_company_relationships(tickers: List[str], company_names: Optional[List[str]] = None,
#                                 days_back: int = 30, api_key: Optional[str] = None) -> pd.DataFrame:
#     """
#     Analyze relationships between companies using Anthropic API and news data.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         company_names (list, optional): List of company names (will be fetched if not provided)
#         days_back (int): Number of days of news to analyze
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         pd.DataFrame: Relationship matrix between companies
#     """
#     client = initialize_anthropic_client(api_key)
# 
#     # Get company names if not provided
#     if company_names is None:
#         company_names = []
#         for ticker in tickers:
#             try:
#                 import yfinance as yf
#                 company = yf.Ticker(ticker)
#                 company_names.append(company.info.get('shortName', ticker))
#             except:
#                 company_names.append(ticker)
# 
#     num_companies = len(tickers)
#     relationship_matrix = np.zeros((num_companies, num_companies))
# 
#     # First, get sentiment for each company
#     company_sentiments = {}
#     company_summaries = {}
# 
#     print(f"Analyzing sentiment for {num_companies} companies...")
#     for i, (ticker, name) in enumerate(zip(tickers, company_names)):
#         try:
#             # Try to use the module_news_sentiment function if available
#             sentiment_data = analyze_company_sentiment(ticker, name, days_back=days_back)
#             company_sentiments[ticker] = sentiment_data
# 
#             # Extract key points for relationship analysis
#             if 'key_points' in sentiment_data:
#                 company_summaries[ticker] = ' '.join(sentiment_data['key_points'])
# 
#             print(f"Processed {i+1}/{num_companies}: {name} ({ticker})")
#             time.sleep(1)  # Respect API rate limits
# 
#         except Exception as e:
#             logger.error(f"Error analyzing sentiment for {ticker}: {str(e)}")
#             company_summaries[ticker] = f"Company {name} ({ticker})"
# 
#     # Now analyze relationships between pairs of companies
#     print("Analyzing relationships between companies...")
# 
#     for i in range(num_companies):
#         # Set diagonal to 1 (company perfectly correlated with itself)
#         relationship_matrix[i, i] = 1.0
# 
#         # Only analyze upper triangle to avoid redundant API calls
#         for j in range(i+1, num_companies):
#             ticker1, ticker2 = tickers[i], tickers[j]
#             name1, name2 = company_names[i], company_names[j]
# 
#             # Create context from summaries
#             company1_context = company_summaries.get(ticker1, f"Company {name1} ({ticker1})")
#             company2_context = company_summaries.get(ticker2, f"Company {name2} ({ticker2})")
# 
#             # Create API prompt
#             prompt = f"""As a financial analyst, evaluate the business relationship and correlation between these two companies based on the provided context:
# 
# Company 1: {name1} ({ticker1})
# Context: {company1_context}
# 
# Company 2: {name2} ({ticker2})
# Context: {company2_context}
# 
# On a scale from -1.0 to 1.0, what is the likely correlation between these companies' stock performances?
# - Values close to 1.0 indicate strong positive correlation (they move together)
# - Values close to 0 indicate little or no correlation
# - Values close to -1.0 indicate strong negative correlation (they move in opposite directions)
# 
# Consider factors such as:
# - Are they competitors or partners?
# - Do they operate in related industries or value chains?
# - Would they be similarly affected by economic conditions?
# - Do they have similar customer bases?
# 
# Return your correlation coefficient with a brief explanation in this format:
# {{
#   "correlation": [number between -1.0 and 1.0],
#   "explanation": "[brief explanation of reasoning]"
# }}
# """
# 
#             try:
#                 # Call Anthropic API
#                 response = client.messages.create(
#                     model="claude-3-haiku-20240307",
#                     max_tokens=300,
#                     temperature=0.0,
#                     system="You are a financial analyst providing objective correlation assessments between companies. Always respond with valid JSON containing 'correlation' (a float between -1.0 and 1.0) and 'explanation' (a string).",
#                     messages=[
#                         {"role": "user", "content": prompt}
#                     ]
#                 )
# 
#                 response_text = response.content[0].text
# 
#                 # Try to parse JSON
#                 try:
#                     # Find JSON in response
#                     import re
#                     json_pattern = r'({[\s\S]*?})'
#                     json_match = re.search(json_pattern, response_text)
# 
#                     if json_match:
#                         json_str = json_match.group(1)
#                         result = json.loads(json_str)
#                         correlation = float(result.get('correlation', 0))
#                         # Ensure value is in range [-1, 1]
#                         correlation = max(-1.0, min(1.0, correlation))
#                     else:
#                         # If no JSON found, estimate from text
#                         logger.warning(f"No JSON found in response for {ticker1}-{ticker2}. Estimating correlation.")
#                         if "strong positive" in response_text.lower():
#                             correlation = 0.8
#                         elif "positive" in response_text.lower():
#                             correlation = 0.5
#                         elif "strong negative" in response_text.lower():
#                             correlation = -0.8
#                         elif "negative" in response_text.lower():
#                             correlation = -0.5
#                         elif "no correlation" in response_text.lower() or "little correlation" in response_text.lower():
#                             correlation = 0.0
#                         else:
#                             correlation = 0.0  # Default if can't determine
# 
#                     # Set relationship in matrix (symmetric)
#                     relationship_matrix[i, j] = correlation
#                     relationship_matrix[j, i] = correlation
# 
#                     print(f"Analyzed {ticker1}-{ticker2}: correlation = {correlation:.2f}")
# 
#                 except (json.JSONDecodeError, ValueError) as e:
#                     logger.error(f"Error parsing response for {ticker1}-{ticker2}: {str(e)}")
#                     # Use a neutral correlation
#                     relationship_matrix[i, j] = 0.0
#                     relationship_matrix[j, i] = 0.0
# 
#             except Exception as e:
#                 logger.error(f"API error for {ticker1}-{ticker2}: {str(e)}")
#                 # Use a neutral correlation
#                 relationship_matrix[i, j] = 0.0
#                 relationship_matrix[j, i] = 0.0
# 
#             # Respect API rate limits
#             time.sleep(2)
# 
#     # Create DataFrame from matrix
#     relationship_df = pd.DataFrame(
#         relationship_matrix,
#         index=tickers,
#         columns=tickers
#     )
# 
#     return relationship_df
# 
# 
# def blend_correlation_matrices(historical_corr: pd.DataFrame, news_corr: pd.DataFrame,
#                               blend_factor: float = 0.5) -> pd.DataFrame:
#     """
#     Blend historical and news-based correlation matrices.
# 
#     Args:
#         historical_corr (pd.DataFrame): Historical correlation matrix
#         news_corr (pd.DataFrame): News-based correlation matrix
#         blend_factor (float): Weight for the news correlation (0-1)
# 
#     Returns:
#         pd.DataFrame: Blended correlation matrix
#     """
#     # Ensure consistent indexing
#     common_tickers = historical_corr.index.intersection(news_corr.index)
# 
#     if len(common_tickers) == 0:
#         logger.warning("No common tickers between correlation matrices. Returning historical correlation.")
#         return historical_corr
# 
#     # Align both matrices
#     hist_corr_aligned = historical_corr.loc[common_tickers, common_tickers]
#     news_corr_aligned = news_corr.loc[common_tickers, common_tickers]
# 
#     # Blend matrices using weighted average
#     blended_corr = (1 - blend_factor) * hist_corr_aligned + blend_factor * news_corr_aligned
# 
#     # Ensure diagonal is 1 and matrix is symmetric
#     np.fill_diagonal(blended_corr.values, 1.0)
#     blended_corr = (blended_corr + blended_corr.T) / 2
# 
#     return blended_corr
# 
# 
# def create_smart_covariance_matrix(returns: pd.DataFrame, blended_corr: pd.DataFrame,
#                                  volatility_forecast_adj: Optional[pd.Series] = None) -> pd.DataFrame:
#     """
#     Create a covariance matrix from returns data and blended correlation matrix.
# 
#     Args:
#         returns (pd.DataFrame): Historical returns
#         blended_corr (pd.DataFrame): Blended correlation matrix
#         volatility_forecast_adj (pd.Series, optional): Volatility forecast adjustments
# 
#     Returns:
#         pd.DataFrame: Smart covariance matrix
#     """
#     # Calculate standard deviations from historical returns
#     std_devs = returns.std()
# 
#     # Adjust volatilities if forecast provided
#     if volatility_forecast_adj is not None:
#         for ticker in volatility_forecast_adj.index:
#             if ticker in std_devs.index:
#                 std_devs[ticker] *= volatility_forecast_adj[ticker]
# 
#     # Create covariance matrix from correlation and std devs
#     n = len(blended_corr)
#     cov_matrix = pd.DataFrame(
#         np.zeros((n, n)),
#         index=blended_corr.index,
#         columns=blended_corr.columns
#     )
# 
#     for i, ticker1 in enumerate(blended_corr.index):
#         for j, ticker2 in enumerate(blended_corr.columns):
#             if ticker1 in std_devs.index and ticker2 in std_devs.index:
#                 cov_matrix.loc[ticker1, ticker2] = (
#                     blended_corr.loc[ticker1, ticker2] *
#                     std_devs[ticker1] *
#                     std_devs[ticker2]
#                 )
# 
#     return cov_matrix
# 
# 
# def forecast_volatility_changes(tickers: List[str], days_back: int = 30,
#                                api_key: Optional[str] = None) -> pd.Series:
#     """
#     Use Anthropic API to forecast changes in volatility for each ticker.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         days_back (int): Number of days of news to analyze
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         pd.Series: Volatility adjustment factors
#     """
#     client = initialize_anthropic_client(api_key)
# 
#     volatility_adjustments = {}
# 
#     for ticker in tickers:
#         try:
#             # Get news for the ticker
#             try:
#                 from module_news_sentiment import get_company_news
#                 news_articles = get_company_news(ticker, days_back=days_back)
#             except:
#                 news_articles = []
# 
#             # Prepare news summary
#             news_summary = ""
#             for i, article in enumerate(news_articles[:5]):  # Limit to top 5 articles
#                 news_summary += f"Article {i+1}: {article.get('title', 'No title')}\n"
#                 news_summary += f"Source: {article.get('source', {}).get('name', 'Unknown')}\n"
#                 news_summary += f"Description: {article.get('description', 'No description')}\n\n"
# 
#             if not news_summary:
#                 news_summary = "No recent news articles found."
# 
#             # Create prompt
#             prompt = f"""As a financial risk analyst, assess the likely change in volatility for {ticker} based on recent news:
# 
# Recent news summary:
# {news_summary}
# 
# Based on this news, should the historical volatility be adjusted up or down?
# Provide a multiplier where:
# - 1.0 means no change to historical volatility
# - >1.0 means increased volatility (e.g., 1.2 means 20% higher)
# - <1.0 means decreased volatility (e.g., 0.9 means 10% lower)
# 
# Return a float between 0.5 and 2.0, representing the volatility adjustment factor.
# """
# 
#             # Call API
#             response = client.messages.create(
#                 model="claude-3-haiku-20240307",
#                 max_tokens=100,
#                 temperature=0.0,
#                 system="You are a financial risk analyst providing volatility forecasts. Return a single number representing a volatility adjustment factor between 0.5 and 2.0.",
#                 messages=[
#                     {"role": "user", "content": prompt}
#                 ]
#             )
# 
#             response_text = response.content[0].text.strip()
# 
#             # Extract volatility adjustment factor
#             try:
#                 # Try to find a number in the response
#                 import re
#                 number_pattern = r'\d+\.\d+|\d+\,\d+|\d+'
#                 number_matches = re.findall(number_pattern, response_text)
# 
#                 if number_matches:
#                     # Get the first number
#                     adjustment = float(number_matches[0].replace(',', '.'))
#                     # Ensure it's within bounds
#                     adjustment = max(0.5, min(2.0, adjustment))
#                 else:
#                     # If no number found, estimate from text
#                     if "significantly higher" in response_text.lower() or "much higher" in response_text.lower():
#                         adjustment = 1.5
#                     elif "higher" in response_text.lower() or "increase" in response_text.lower():
#                         adjustment = 1.2
#                     elif "significantly lower" in response_text.lower() or "much lower" in response_text.lower():
#                         adjustment = 0.7
#                     elif "lower" in response_text.lower() or "decrease" in response_text.lower():
#                         adjustment = 0.9
#                     else:
#                         adjustment = 1.0  # No change
#             except:
#                 # Default to no change
#                 adjustment = 1.0
# 
#             volatility_adjustments[ticker] = adjustment
#             print(f"Volatility forecast for {ticker}: {adjustment:.2f}x")
# 
#         except Exception as e:
#             logger.error(f"Error forecasting volatility for {ticker}: {str(e)}")
#             volatility_adjustments[ticker] = 1.0  # Default to no change
# 
#         # Respect API rate limits
#         time.sleep(2)
# 
#     return pd.Series(volatility_adjustments)
# 
# 
# # === SMART PORTFOLIO OPTIMIZATION FUNCTIONS ===
# 
# def optimize_portfolio(returns: pd.DataFrame, cov_matrix: pd.DataFrame,
#                       method: str = 'sharpe', risk_free_rate: float = 0.0,
#                       target_return: Optional[float] = None,
#                       target_risk: Optional[float] = None) -> Tuple[np.ndarray, Dict]:
#     """
#     Optimize portfolio weights using various methods.
# 
#     Args:
#         returns (pd.DataFrame): Historical returns
#         cov_matrix (pd.DataFrame): Covariance matrix
#         method (str): Optimization method ('sharpe', 'min_risk', 'target_return', 'target_risk')
#         risk_free_rate (float): Risk-free rate
#         target_return (float, optional): Target return for 'target_return' method
#         target_risk (float, optional): Target risk for 'target_risk' method
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(returns.columns)
#     mean_returns = returns.mean()
# 
#     # Initial weights (equal allocation)
#     initial_weights = np.ones(num_assets) / num_assets
# 
#     # Constraints: weights sum to 1
#     constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Bounds: no short selling
#     bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     if method == 'sharpe':
#         # Maximize Sharpe ratio (minimize negative Sharpe)
#         def objective(weights):
#             port_return = np.dot(weights, mean_returns)
#             port_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
#             return -(port_return - risk_free_rate) / port_risk
# 
#     elif method == 'min_risk':
#         # Minimize portfolio volatility
#         def objective(weights):
#             return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     elif method == 'target_return':
#         if target_return is None:
#             raise ValueError("target_return must be specified for 'target_return' method")
# 
#         # Minimize risk subject to target return
#         def objective(weights):
#             return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#         # Add constraint for target return
#         constraints.append({
#             'type': 'eq',
#             'fun': lambda weights: np.dot(weights, mean_returns) - target_return
#         })
# 
#     elif method == 'target_risk':
#         if target_risk is None:
#             raise ValueError("target_risk must be specified for 'target_risk' method")
# 
#         # Maximize return subject to target risk
#         def objective(weights):
#             return -np.dot(weights, mean_returns)
# 
#         # Add constraint for target risk
#         constraints.append({
#             'type': 'eq',
#             'fun': lambda weights: np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) - target_risk
#         })
# 
#     else:
#         raise ValueError(f"Unknown optimization method: {method}")
# 
#     # Optimize
#     result = minimize(
#         objective,
#         initial_weights,
#         method='SLSQP',
#         bounds=bounds,
#         constraints=constraints
#     )
# 
#     # Get optimal weights
#     optimal_weights = result['x']
# 
#     # Calculate performance metrics
#     optimal_return = np.dot(optimal_weights, mean_returns)
#     optimal_risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))
#     optimal_sharpe = (optimal_return - risk_free_rate) / optimal_risk if optimal_risk > 0 else 0
# 
#     performance_metrics = {
#         'return': optimal_return,
#         'risk': optimal_risk,
#         'sharpe': optimal_sharpe,
#         'success': result['success'],
#         'message': result['message']
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def analyze_and_optimize_portfolio(tickers: List[str], start_date: str, end_date: str,
#                                  optimization_method: str = 'sharpe',
#                                  risk_free_rate: float = 0.02,
#                                  news_blend_factor: Optional[float] = None,  # Now optional
#                                  num_portfolios: int = 5000,
#                                  use_ai_volatility: bool = True,
#                                  custom_news: Optional[str] = None,  # New parameter for custom news
#                                  api_key: Optional[str] = None) -> Dict:
#     """
#     Complete workflow for analyzing and optimizing a portfolio using both traditional and AI methods.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         start_date (str): Start date for historical data (YYYY-MM-DD)
#         end_date (str): End date for historical data (YYYY-MM-DD)
#         optimization_method (str): Optimization method ('sharpe', 'min_risk', 'target_return', 'target_risk')
#         risk_free_rate (float): Risk-free rate
#         news_blend_factor (float, optional): Weight of news-based correlations (0-1), if None will be determined by AI
#         num_portfolios (int): Number of random portfolios to generate
#         use_ai_volatility (bool): Whether to use AI to forecast volatility
#         custom_news (str, optional): Custom news text to analyze
#         api_key (str, optional): Anthropic API key
# 
#     Returns:
#         dict: Complete analysis results
#     """
#     results = {}
# 
#     # Step 1: Load historical price data
#     print("Loading historical price data...")
#     price_data = load_stock_data(tickers, start_date, end_date)
# 
#     # Check if we have data for all tickers
#     valid_tickers = price_data.columns.tolist()
#     if len(valid_tickers) < len(tickers):
#         missing = set(tickers) - set(valid_tickers)
#         print(f"Warning: Missing data for tickers: {missing}")
#         tickers = valid_tickers
# 
#     # Calculate returns
#     returns = price_data.pct_change().dropna()
# 
#     # Step 2: Calculate historical correlation and covariance
#     print("Calculating historical correlation and covariance...")
#     historical_corr = returns.corr()
#     historical_cov = returns.cov()
# 
#     results['historical_correlation'] = historical_corr
#     results['historical_covariance'] = historical_cov
# 
#     # Step 3: Generate random portfolios using historical data
#     print("Generating efficient frontier with historical data...")
#     portfolio_returns, portfolio_volatilities, sharpe_ratios, weights_record = generate_random_portfolios(
#         returns, historical_cov, num_portfolios, risk_free_rate
#     )
# 
#     results['efficient_frontier'] = {
#         'returns': portfolio_returns,
#         'volatilities': portfolio_volatilities,
#         'sharpe_ratios': sharpe_ratios
#     }
# 
#     # Step 4: Display traditional optimization results
#     traditional_results = display_best_portfolios(
#         portfolio_returns, portfolio_volatilities, sharpe_ratios, weights_record, tickers
#     )
# 
#     results['traditional_optimization'] = traditional_results
# 
#     # Step 5: Process custom news if provided
#     if custom_news:
#         print("Analyzing custom news impact...")
#         news_impact = analyze_news_impact(custom_news, tickers, api_key)
#         results['custom_news_impact'] = news_impact
# 
#         # Update correlation matrix based on news
#         updated_corr = update_correlation_matrix_from_news(historical_corr, news_impact)
#         results['news_correlation'] = updated_corr
# 
#         # Print news impact summary
#         print("\nCustom News Impact Summary:")
#         for ticker, impact in news_impact.items():
#             print(f"{ticker}:")
#             print(f"  Relevance: {impact['relevance']}/10")
#             print(f"  Sentiment: {impact['sentiment']} (-5 to +5)")
#             print(f"  Price Impact: {impact['price_impact']}")
#             print(f"  Volatility Impact: {impact['volatility_impact']}")
#             print(f"  Explanation: {impact['explanation'][:100]}...")
# 
#     # Step 6: Get news-based correlation matrix (if no custom news provided)
#     else:
#         print("Analyzing news and company relationships with AI...")
#         try:
#             news_correlation = analyze_company_relationships(tickers, days_back=30, api_key=api_key)
#             results['news_correlation'] = news_correlation
#         except Exception as e:
#             logger.error(f"Error analyzing company relationships: {str(e)}")
#             print(f"Error analyzing company relationships: {str(e)}")
#             # Use historical correlation as fallback
#             news_correlation = historical_corr.copy()
#             results['news_correlation'] = news_correlation
# 
#     # Step 7: Determine optimal news blend factor if not provided
#     if news_blend_factor is None:
#         print("Determining optimal news weight with AI...")
#         try:
#             # Get some market conditions info
#             vol_index = None
#             try:
#                 # Try to get VIX data if available
#                 vix_data = load_stock_data(['^VIX'], start_date, end_date)
#                 if not vix_data.empty:
#                     vol_index = vix_data['^VIX'].iloc[-1]
#             except:
#                 pass
# 
#             market_conditions = {
#                 'recent_volatility': f"{'High' if vol_index and vol_index > 20 else 'Moderate' if vol_index and vol_index > 15 else 'Low'}",
#                 'tickers_analyzed': len(tickers),
#                 'average_stock_volatility': f"{returns.std().mean() * np.sqrt(252):.2%} (annualized)",
#                 'time_period': f"{start_date} to {end_date}",
#                 'ticker_list': ", ".join(tickers[:5]) + ("..." if len(tickers) > 5 else "")
#             }
# 
#             # Determine optimal weight
#             news_blend_factor = determine_optimal_news_weight(
#                 price_data, tickers, market_conditions, api_key
#             )
# 
#             results['ai_determined_news_weight'] = news_blend_factor
# 
#         except Exception as e:
#             logger.error(f"Error determining news weight: {str(e)}")
#             print(f"Error determining news weight: {str(e)}")
#             # Use default if determination fails
#             news_blend_factor = 0.3
#             results['ai_determined_news_weight'] = news_blend_factor
#             print(f"Using default news weight: {news_blend_factor}")
# 
#     # Step 8: Blend correlation matrices
#     print(f"Blending correlation matrices (news weight: {news_blend_factor:.2%})...")
#     blended_correlation = blend_correlation_matrices(
#         historical_corr, news_correlation, blend_factor=news_blend_factor
#     )
#     results['blended_correlation'] = blended_correlation
# 
#     # Step 9: Get AI volatility forecasts if requested
#     if use_ai_volatility:
#         try:
#             print("Forecasting volatility changes with AI...")
#             volatility_forecast = forecast_volatility_changes(tickers, days_back=30, api_key=api_key)
#             results['volatility_forecast'] = volatility_forecast
#         except Exception as e:
#             logger.error(f"Error forecasting volatility: {str(e)}")
#             print(f"Error forecasting volatility: {str(e)}")
#             volatility_forecast = pd.Series(1.0, index=tickers)  # Default: no change
#             results['volatility_forecast'] = volatility_forecast
#     else:
#         volatility_forecast = pd.Series(1.0, index=tickers)  # Default: no change
# 
#     # Step 10: Create smart covariance matrix
#     print("Creating smart covariance matrix...")
#     smart_covariance = create_smart_covariance_matrix(
#         returns, blended_correlation, volatility_forecast
#     )
#     results['smart_covariance'] = smart_covariance
# 
#     # Step 11: Generate random portfolios using smart covariance
#     print("Generating efficient frontier with AI-enhanced data...")
#     ai_portfolio_returns, ai_portfolio_volatilities, ai_sharpe_ratios, ai_weights_record = generate_random_portfolios(
#         returns, smart_covariance, num_portfolios, risk_free_rate
#     )
# 
#     results['ai_efficient_frontier'] = {
#         'returns': ai_portfolio_returns,
#         'volatilities': ai_portfolio_volatilities,
#         'sharpe_ratios': ai_sharpe_ratios
#     }
# 
#     # Step 12: Display AI-enhanced optimization results
#     ai_results = display_best_portfolios(
#         ai_portfolio_returns, ai_portfolio_volatilities, ai_sharpe_ratios, ai_weights_record, tickers
#     )
# 
#     results['ai_optimization'] = ai_results
# 
#     # Step 13: Optimize portfolio with specified method
#     print(f"Optimizing portfolio using {optimization_method} method...")
#     optimal_weights, performance_metrics = optimize_portfolio(
#         returns, smart_covariance, method=optimization_method, risk_free_rate=risk_free_rate
#     )
# 
#     results['optimized_portfolio'] = {
#         'weights': {ticker: weight for ticker, weight in zip(tickers, optimal_weights)},
#         'metrics': performance_metrics,
#         'news_blend_factor': news_blend_factor
#     }
# 
#     print("\nOptimized Portfolio Allocation:")
#     for ticker, weight in zip(tickers, optimal_weights):
#         print(f"{ticker}: {weight*100:.2f}%")
# 
#     print(f"\nExpected Return: {performance_metrics['return']*100:.2f}%")
#     print(f"Expected Risk: {performance_metrics['risk']*100:.2f}%")
#     print(f"Sharpe Ratio: {performance_metrics['sharpe']:.4f}")
#     print(f"News Influence: {news_blend_factor*100:.1f}%")
# 
#     return results
# 
# 
# # === VISUALIZATION FUNCTIONS FOR AI-ENHANCED ANALYSIS ===
# 
# def plot_correlation_comparison(historical_corr: pd.DataFrame, news_corr: pd.DataFrame,
#                                blended_corr: Optional[pd.DataFrame] = None) -> plt.Figure:
#     """
#     Plot comparison of historical and news-based correlation matrices.
# 
#     Args:
#         historical_corr (pd.DataFrame): Historical correlation matrix
#         news_corr (pd.DataFrame): News-based correlation matrix
#         blended_corr (pd.DataFrame, optional): Blended correlation matrix
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     n_plots = 3 if blended_corr is not None else 2
#     fig, axes = plt.subplots(1, n_plots, figsize=(n_plots*7, 6))
# 
#     # Plot historical correlation
#     sns.heatmap(historical_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                vmin=-1, vmax=1, center=0, ax=axes[0])
#     axes[0].set_title('Историческая корреляция')
# 
#     # Plot news-based correlation
#     sns.heatmap(news_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                vmin=-1, vmax=1, center=0, ax=axes[1])
#     axes[1].set_title('Корреляция на основе новостей (AI)')
# 
#     # Plot blended correlation if provided
#     if blended_corr is not None:
#         sns.heatmap(blended_corr, annot=True, cmap='coolwarm', linewidths=0.5,
#                    vmin=-1, vmax=1, center=0, ax=axes[2])
#         axes[2].set_title('Смешанная корреляция')
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_efficient_frontier_comparison(traditional_ef: Dict, ai_ef: Dict,
#                                      risk_free_rate: float = 0.02) -> plt.Figure:
#     """
#     Plot comparison of traditional and AI-enhanced efficient frontiers.
# 
#     Args:
#         traditional_ef (dict): Traditional efficient frontier data
#         ai_ef (dict): AI-enhanced efficient frontier data
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     fig, ax = plt.subplots(figsize=(10, 8))
# 
#     # Plot traditional efficient frontier
#     ax.scatter(traditional_ef['volatilities'], traditional_ef['returns'],
#               c=traditional_ef['sharpe_ratios'], cmap='viridis',
#               alpha=0.5, s=10, label='Традиционный анализ')
# 
#     # Find and mark optimal portfolios (traditional)
#     trad_max_sharpe_idx = np.argmax(traditional_ef['sharpe_ratios'])
#     trad_min_vol_idx = np.argmin(traditional_ef['volatilities'])
# 
#     ax.scatter(traditional_ef['volatilities'][trad_max_sharpe_idx],
#               traditional_ef['returns'][trad_max_sharpe_idx],
#               color='blue', marker='*', s=200,
#               label='Макс. Шарп (традиционный)')
# 
#     ax.scatter(traditional_ef['volatilities'][trad_min_vol_idx],
#               traditional_ef['returns'][trad_min_vol_idx],
#               color='blue', marker='o', s=150,
#               label='Мин. риск (традиционный)')
# 
#     # Plot AI-enhanced efficient frontier
#     ax.scatter(ai_ef['volatilities'], ai_ef['returns'],
#               c=ai_ef['sharpe_ratios'], cmap='plasma',
#               alpha=0.5, s=10, label='AI-улучшенный анализ')
# 
#     # Find and mark optimal portfolios (AI)
#     ai_max_sharpe_idx = np.argmax(ai_ef['sharpe_ratios'])
#     ai_min_vol_idx = np.argmin(ai_ef['volatilities'])
# 
#     ax.scatter(ai_ef['volatilities'][ai_max_sharpe_idx],
#               ai_ef['returns'][ai_max_sharpe_idx],
#               color='red', marker='*', s=200,
#               label='Макс. Шарп (AI)')
# 
#     ax.scatter(ai_ef['volatilities'][ai_min_vol_idx],
#               ai_ef['returns'][ai_min_vol_idx],
#               color='red', marker='o', s=150,
#               label='Мин. риск (AI)')
# 
#     # Add risk-free point and Capital Market Lines
#     ax.scatter([0], [risk_free_rate], color='black', marker='s', s=100, label='Безрисковый актив')
# 
#     # CML for traditional
#     trad_slope = (traditional_ef['returns'][trad_max_sharpe_idx] - risk_free_rate) / traditional_ef['volatilities'][trad_max_sharpe_idx]
#     x_range = np.linspace(0, max(traditional_ef['volatilities']) * 1.2, 100)
#     trad_cml = risk_free_rate + trad_slope * x_range
#     ax.plot(x_range, trad_cml, 'b--', alpha=0.7, label='CML (традиционный)')
# 
#     # CML for AI
#     ai_slope = (ai_ef['returns'][ai_max_sharpe_idx] - risk_free_rate) / ai_ef['volatilities'][ai_max_sharpe_idx]
#     ai_cml = risk_free_rate + ai_slope * x_range
#     ax.plot(x_range, ai_cml, 'r--', alpha=0.7, label='CML (AI)')
# 
#     # Set labels and title
#     ax.set_xlabel('Ожидаемый риск (волатильность)')
#     ax.set_ylabel('Ожидаемая доходность')
#     ax.set_title('Сравнение эффективных фронтов: традиционный vs. AI-улучшенный')
#     ax.grid(True, alpha=0.3)
#     ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_news_impact(news_impact: Dict[str, Dict[str, Any]], title: str = "Влияние новости на акции") -> plt.Figure:
#     """
#     Visualize the impact of news on stocks.
# 
#     Args:
#         news_impact (dict): News impact analysis from analyze_news_impact
#         title (str): Plot title
# 
#     Returns:
#         plt.Figure: The matplotlib figure
#     """
#     # Extract sentiment and relevance data
#     tickers = list(news_impact.keys())
#     sentiments = [news_impact[ticker]['sentiment'] for ticker in tickers]
#     relevances = [news_impact[ticker]['relevance'] for ticker in tickers]
# 
#     # Convert price impacts to numbers
#     price_impacts = []
#     for ticker in tickers:
#         impact_str = news_impact[ticker]['price_impact']
#         try:
#             # Extract number from string like "+2.5%" or "-1.2%"
#             impact_str = impact_str.replace('%', '').replace('+', '')
#             price_impacts.append(float(impact_str) / 100)
#         except:
#             price_impacts.append(0)
# 
#     # Create figure with two subplots
#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
# 
#     # Plot sentiment vs relevance
#     scatter = ax1.scatter(relevances, sentiments, c=price_impacts, cmap='RdYlGn',
#                          s=100, vmin=-0.05, vmax=0.05)
# 
#     # Add ticker labels
#     for i, ticker in enumerate(tickers):
#         ax1.annotate(ticker, (relevances[i], sentiments[i]),
#                     xytext=(5, 5), textcoords='offset points')
# 
#     # Add reference lines
#     ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
#     ax1.axvline(x=5, color='gray', linestyle='--', alpha=0.7)
# 
#     # Set labels and title
#     ax1.set_xlabel('Релевантность новости (0-10)')
#     ax1.set_ylabel('Сентимент (-5 to +5)')
#     ax1.set_title('Сентимент и Релевантность')
#     ax1.grid(True, alpha=0.3)
# 
#     # Add colorbar
#     cbar = plt.colorbar(scatter, ax=ax1)
#     cbar.set_label('Ожидаемое влияние на цену')
# 
#     # Plot expected price impact as bars
#     colors = ['green' if impact >= 0 else 'red' for impact in price_impacts]
#     ax2.bar(tickers, price_impacts, color=colors, alpha=0.7)
#     ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
# 
#     # Set labels and title
#     ax2.set_xlabel('Акции')
#     ax2.set_ylabel('Ожидаемое влияние на цену')
#     ax2.set_title('Прогнозируемое влияние на цены')
#     ax2.grid(True, alpha=0.3)
# 
#     # Rotate x-tick labels
#     plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
# 
#     # Add price impact values as text
#     for i, impact in enumerate(price_impacts):
#         ax2.annotate(f"{impact*100:+.1f}%",
#                    (i, impact),
#                    ha='center',
#                    va='bottom' if impact >= 0 else 'top',
#                    fontweight='bold')
# 
#     # Set overall title
#     fig.suptitle(title, fontsize=16)
# 
#     plt.tight_layout()
#     plt.subplots_adjust(top=0.9)
#     return fig

"""## smart_portfolio_optimizer.py"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile smart_portfolio_optimizer.py
# """
# Advanced Portfolio Optimization Module
# 
# This module extends traditional portfolio optimization
# with advanced AI, machine learning, and multi-factor analysis techniques.
# """
# 
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import anthropic
# import json
# import logging
# from typing import Dict, List, Any, Optional
# 
# # Machine Learning Libraries
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import mean_absolute_error, mean_squared_error
# 
# # Import existing modules
# try:
#     from module_efficient_frontier_and_correlation import (
#         blend_correlation_matrices,
#         initialize_anthropic_client
#     )
# except ImportError:
#     logging.warning("Could not import existing correlation module")
# 
# class AdvancedPortfolioOptimizer:
#     def __init__(
#         self,
#         tickers: List[str],
#         start_date: str,
#         end_date: str,
#         api_key: Optional[str] = None
#     ):
#         """
#         Initialize Advanced Portfolio Optimizer
# 
#         Args:
#             tickers (list): Stock ticker symbols
#             start_date (str): Start of historical data period
#             end_date (str): End of historical data period
#             api_key (str, optional): Anthropic API key
#         """
#         self.tickers = tickers
#         self.start_date = start_date
#         self.end_date = end_date
#         self.api_key = api_key
# 
#         # Initialize data containers
#         self.historical_data = None
#         self.returns = None
#         self.correlation_matrices = {}
#         self.news_impacts = {}
# 
#     def load_data(self):
#         """
#         Load and preprocess historical stock data
#         """
#         from module_investment_allocation import load_stock_data
# 
#         self.historical_data = load_stock_data(self.tickers, self.start_date, self.end_date)
#         self.returns = self.historical_data.pct_change().dropna()
# 
#     def enhance_ai_analysis(self, news_text: str) -> Dict[str, Any]:
#         """
#         Multilevel news analysis with enhanced AI insights
# 
#         Args:
#             news_text (str): News text to analyze
# 
#         Returns:
#             dict: Comprehensive news impact analysis
#         """
#         client = initialize_anthropic_client(self.api_key)
# 
#         # Comprehensive news analysis prompt
#         prompt = f"""Perform a comprehensive multilevel analysis of the following news:
# 
# News Text:
# {news_text}
# 
# Please provide a detailed analysis covering:
# 
# 1. Direct Company Impact:
# - Specific impact on each company
# - Relevance score (0-10)
# - Short-term price movement prediction
# 
# 2. Market Sentiment:
# - Overall market mood
# - Potential sector-wide implications
# - Macroeconomic signal strength
# 
# 3. Correlation and Systemic Effects:
# - Potential changes in inter-company correlations
# - Cross-sector impact assessment
# - Volatility implications
# 
# Return a structured JSON with comprehensive insights.
# """
# 
#         try:
#             response = client.messages.create(
#                 model="claude-3-haiku-20240307",
#                 max_tokens=2000,
#                 temperature=0.2,
#                 system="You are an advanced financial analyst providing multilevel news impact analysis.",
#                 messages=[{"role": "user", "content": prompt}]
#             )
# 
#             # Parse JSON response
#             response_text = response.content[0].text
#             analysis = json.loads(response_text)
# 
#             return analysis
# 
#         except Exception as e:
#             logging.error(f"AI analysis error: {e}")
#             return {}
# 
#     def advanced_correlation_matrix(
#         self,
#         historical_corr: pd.DataFrame,
#         news_corr: pd.DataFrame,
#         external_factors: Optional[Dict] = None
#     ) -> pd.DataFrame:
#         """
#         Create an advanced correlation matrix incorporating multiple factors
# 
#         Args:
#             historical_corr (DataFrame): Historical correlation matrix
#             news_corr (DataFrame): News-based correlation matrix
#             external_factors (dict, optional): Additional correlation influencers
# 
#         Returns:
#             DataFrame: Enhanced correlation matrix
#         """
#         # Default external factors if not provided
#         if external_factors is None:
#             external_factors = self._compute_external_factors()
# 
#         # Blend correlation matrices with external factor weights
#         advanced_corr = blend_correlation_matrices(
#             historical_corr,
#             news_corr,
#             blend_factor=0.3  # Adjustable news weight
#         )
# 
#         # Adjust correlation based on external factors
#         for factor, weight in external_factors.items():
#             advanced_corr *= (1 + weight)
# 
#         return advanced_corr
# 
#     def _compute_external_factors(self) -> Dict[str, float]:
#         """
#         Compute external factors affecting market correlations
# 
#         Returns:
#             dict: External correlation influencing factors
#         """
#         external_factors = {
#             'market_volatility': self._get_market_volatility(),
#             'economic_policy_uncertainty': self._get_economic_uncertainty(),
#             'global_risk_sentiment': self._get_global_risk_sentiment()
#         }
# 
#         return external_factors
# 
#     def _get_market_volatility(self) -> float:
#         """
#         Estimate current market volatility
# 
#         Returns:
#             float: Volatility factor
#         """
#         try:
#             # Try to get VIX data or compute from historical returns
#             from module_investment_allocation import load_stock_data
#             vix_data = load_stock_data(['^VIX'], self.start_date, self.end_date)
# 
#             if not vix_data.empty:
#                 return vix_data['^VIX'].mean() / 100  # Normalize
# 
#             # Fallback to returns-based volatility
#             return self.returns.std().mean() * np.sqrt(252)
#         except:
#             return 0.15  # Default volatility
# 
#     def _get_economic_uncertainty(self) -> float:
#         """
#         Estimate economic policy uncertainty
# 
#         Returns:
#             float: Uncertainty factor
#         """
#         # Placeholder for more complex economic uncertainty calculation
#         # In a real-world scenario, you'd fetch Economic Policy Uncertainty Index
#         return 0.1  # Default moderate uncertainty
# 
#     def _get_global_risk_sentiment(self) -> float:
#         """
#         Estimate global market risk sentiment
# 
#         Returns:
#             float: Risk sentiment factor
#         """
#         # Placeholder for global risk sentiment calculation
#         # Could involve analyzing global indices, bond yields, etc.
#         return 0.05  # Default moderate risk sentiment
# 
#     def ml_volatility_forecast(self) -> pd.Series:
#         """
#         Machine learning based volatility forecasting
# 
#         Returns:
#             Series: Volatility predictions for each stock
#         """
#         # Prepare features
#         returns = self.returns
# 
#         # Add technical indicators as features
#         returns['rolling_mean'] = returns.mean(axis=1)
#         returns['rolling_std'] = returns.std(axis=1)
# 
#         # Prepare target variable (future volatility)
#         target = returns.shift(-1).std(axis=1)
# 
#         # Drop NaN rows
#         data = returns.dropna()
#         target = target.loc[data.index]
# 
#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(
#             data.drop(columns=['rolling_mean', 'rolling_std']),
#             target,
#             test_size=0.2
#         )
# 
#         # Scale features
#         scaler = StandardScaler()
#         X_train_scaled = scaler.fit_transform(X_train)
#         X_test_scaled = scaler.transform(X_test)
# 
#         # Train Random Forest Regressor
#         rf_model = RandomForestRegressor(
#             n_estimators=100,
#             random_state=42
#         )
#         rf_model.fit(X_train_scaled, y_train)
# 
#         # Predict volatility
#         volatility_predictions = pd.Series(
#             rf_model.predict(X_test_scaled),
#             index=X_test.index
#         )
# 
#         # Print model performance
#         y_pred = rf_model.predict(X_test_scaled)
#         mae = mean_absolute_error(y_test, y_pred)
#         mse = mean_squared_error(y_test, y_pred)
# 
#         logging.info(f"Volatility Forecast MAE: {mae}")
#         logging.info(f"Volatility Forecast MSE: {mse}")
# 
#         return volatility_predictions
# 
#     def dynamic_risk_adjustment(
#         self,
#         portfolio_metrics: Dict,
#         market_conditions: Dict
#     ) -> Dict:
#         """
#         Dynamically adjust portfolio risk based on market conditions
# 
#         Args:
#             portfolio_metrics (dict): Current portfolio performance metrics
#             market_conditions (dict): Current market conditions
# 
#         Returns:
#             dict: Adjusted portfolio metrics
#         """
#         # Risk multipliers based on market conditions
#         risk_factors = {
#             'volatility': market_conditions.get('volatility', 1.0),
#             'economic_uncertainty': market_conditions.get('economic_uncertainty', 1.0),
#             'global_risk_sentiment': market_conditions.get('global_risk_sentiment', 1.0)
#         }
# 
#         # Compute risk adjustment
#         risk_multiplier = np.prod(list(risk_factors.values()))
# 
#         adjusted_metrics = portfolio_metrics.copy()
#         adjusted_metrics['risk'] *= risk_multiplier
#         adjusted_metrics['sharpe'] /= risk_multiplier
# 
#         return adjusted_metrics
# 
#     def optimize_portfolio(self) -> Dict:
#         """
#         Comprehensive portfolio optimization workflow
# 
#         Returns:
#             dict: Optimized portfolio details
#         """
#         # Load data
#         self.load_data()
# 
#         # Compute historical correlation
#         historical_corr = self.returns.corr()
# 
#         # Get news impact (optional, can be passed externally)
#         news_impact = self.enhance_ai_analysis(
#             "Global market trends and economic outlook"
#         )
# 
#         # Create advanced correlation matrix
#         advanced_corr = self.advanced_correlation_matrix(
#             historical_corr,
#             news_impact.get('correlation_matrix', historical_corr)
#         )
# 
#         # Forecast volatility
#         volatility_forecast = self.ml_volatility_forecast()
# 
#         # Placeholder for existing portfolio optimization method
#         from module_efficient_frontier_and_correlation import optimize_portfolio
# 
#         optimal_weights, performance_metrics = optimize_portfolio(
#             self.returns,
#             advanced_corr,
#             method='sharpe'
#         )
# 
#         # Dynamic risk adjustment
#         adjusted_metrics = self.dynamic_risk_adjustment(
#             performance_metrics,
#             {
#                 'volatility': self._get_market_volatility(),
#                 'economic_uncertainty': self._get_economic_uncertainty(),
#                 'global_risk_sentiment': self._get_global_risk_sentiment()
#             }
#         )
# 
#         return {
#             'optimal_weights': dict(zip(self.tickers, optimal_weights)),
#             'performance_metrics': adjusted_metrics,
#             'advanced_correlation_matrix': advanced_corr,
#             'volatility_forecast': volatility_forecast
#         }
# 
# def main():
#     """
#     Example usage of Advanced Portfolio Optimizer
#     """
#     tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META']
#     optimizer = AdvancedPortfolioOptimizer(
#         tickers,
#         start_date='2022-01-01',
#         end_date='2023-12-31'
#     )
# 
#     results = optimizer.optimize_portfolio()
# 
#     print("Optimal Portfolio Weights:")
#     for ticker, weight in results['optimal_weights'].items():
#         print(f"{ticker}: {weight*100:.2f}%")
# 
#     print("\nPerformance Metrics:")
#     for metric, value in results['performance_metrics'].items():
#         print(f"{metric}: {value}")
# 
# if __name__ == "__main__":
#     main()

"""## module_optimization.py

Задача: Оптимизация портфеля с использованием метода минимизации риска.

Основная функция:

Расчет средней годовой доходности и ковариационной матрицы доходности для всех активов в портфеле.

Оптимизация весов активов в портфеле для минимизации риска (дисперсии).

Методы в модуле:

calculate_returns(data): Рассчитывает среднюю доходность и ковариационную матрицу для данных активов.

optimize_portfolio(mean_returns, cov_matrix): Оптимизирует веса активов в портфеле, минимизируя риск (дисперсию).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_optimization.py
# """
# Module for portfolio optimization.
# Provides functions for portfolio optimization using various techniques
# such as Mean-Variance Optimization, Risk Parity, Black-Litterman model,
# and other advanced portfolio construction methods.
# """
# 
# import numpy as np
# import pandas as pd
# from scipy.optimize import minimize
# import matplotlib.pyplot as plt
# import logging
# from typing import List, Dict, Tuple, Optional, Union, Callable
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# # ==== Mean-Variance Optimization Functions ====
# 
# def portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):
#     """
#     Calculate negative Sharpe ratio for portfolio optimization (minimization).
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         float: Negative Sharpe ratio
#     """
#     portfolio_return = np.sum(weights * mean_returns)
#     portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     # Avoid division by zero
#     if portfolio_volatility == 0:
#         portfolio_volatility = 1e-6
# 
#     sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_volatility
#     return -sharpe_ratio  # Negative for minimization
# 
# 
# def min_volatility(weights, mean_returns, cov_matrix):
#     """
#     Calculate portfolio volatility (standard deviation).
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
# 
#     Returns:
#         float: Portfolio volatility
#     """
#     return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
# 
# def portfolio_return(weights, mean_returns):
#     """
#     Calculate portfolio expected return.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         mean_returns (pandas.Series/numpy.array): Expected returns
# 
#     Returns:
#         float: Portfolio expected return
#     """
#     return np.sum(weights * mean_returns)
# 
# 
# def optimize_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate=0.02, constraints=None, bounds=None):
#     """
#     Optimize portfolio for maximum Sharpe ratio.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         risk_free_rate (float): Risk-free rate
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(mean_returns)
#     args = (mean_returns, cov_matrix, risk_free_rate)
# 
#     # Default constraints require weights to sum to 1
#     if constraints is None:
#         constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     result = minimize(portfolio_performance, init_guess, args=args, method='SLSQP',
#                      bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
# 
#     # Calculate optimal portfolio metrics
#     optimal_weights = result['x']
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - risk_free_rate) / optimal_volatility
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def optimize_minimum_volatility(mean_returns, cov_matrix, constraints=None, bounds=None):
#     """
#     Optimize portfolio for minimum volatility.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(mean_returns)
#     args = (mean_returns, cov_matrix)
# 
#     # Default constraints require weights to sum to 1
#     if constraints is None:
#         constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     result = minimize(min_volatility, init_guess, args=args, method='SLSQP',
#                     bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
# 
#     # Calculate optimal portfolio metrics
#     optimal_weights = result['x']
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - 0.02) / optimal_volatility  # Using 2% as default risk-free rate
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def optimize_target_return(mean_returns, cov_matrix, target_return, constraints=None, bounds=None):
#     """
#     Optimize portfolio for minimum volatility given a target return.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         target_return (float): Target portfolio return
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(mean_returns)
#     args = (mean_returns, cov_matrix)
# 
#     # Create constraints list if None provided
#     if constraints is None:
#         constraints = []
# 
#     # Add constraint that weights sum to 1
#     constraints.append({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
# 
#     # Add constraint for target return
#     constraints.append({'type': 'eq', 'fun': lambda x: portfolio_return(x, mean_returns) - target_return})
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     result = minimize(min_volatility, init_guess, args=args, method='SLSQP',
#                     bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
#         if "Positive directional derivative for linesearch" in result['message']:
#             logger.warning("Target return may be too high or too low for the given assets")
# 
#     # Calculate optimal portfolio metrics
#     optimal_weights = result['x']
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - 0.02) / optimal_volatility  # Using 2% as default risk-free rate
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def optimize_target_volatility(mean_returns, cov_matrix, target_volatility, constraints=None, bounds=None):
#     """
#     Optimize portfolio for maximum return given a target volatility.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         target_volatility (float): Target portfolio volatility
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(mean_returns)
# 
#     # Define objective function to maximize return (minimize negative return)
#     def objective(weights):
#         return -portfolio_return(weights, mean_returns)
# 
#     # Define volatility constraint function
#     def volatility_constraint(weights):
#         return min_volatility(weights, mean_returns, cov_matrix) - target_volatility
# 
#     # Create constraints list if None provided
#     if constraints is None:
#         constraints = []
# 
#     # Add constraint that weights sum to 1
#     constraints.append({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
# 
#     # Add constraint for target volatility
#     constraints.append({'type': 'eq', 'fun': volatility_constraint})
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     result = minimize(objective, init_guess, method='SLSQP',
#                     bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
#         if "Positive directional derivative for linesearch" in result['message']:
#             logger.warning("Target volatility may be too high or too low for the given assets")
# 
#     # Calculate optimal portfolio metrics
#     optimal_weights = result['x']
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - 0.02) / optimal_volatility  # Using 2% as default risk-free rate
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# def get_efficient_frontier(mean_returns, cov_matrix, num_points=50, risk_free_rate=0.02):
#     """
#     Generate points along the efficient frontier.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         num_points (int): Number of points to generate
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         tuple: (returns, volatilities, weights_list)
#     """
#     # Optimize for max Sharpe and min volatility to find the range
#     min_vol_weights, min_vol_metrics = optimize_minimum_volatility(mean_returns, cov_matrix)
#     max_sharpe_weights, max_sharpe_metrics = optimize_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate)
# 
#     min_return = min_vol_metrics['return']
#     max_return = max(max_sharpe_metrics['return'], max(mean_returns))
# 
#     # Create a range of target returns
#     target_returns = np.linspace(min_return, max_return, num_points)
# 
#     efficient_returns = []
#     efficient_volatilities = []
#     weights_list = []
# 
#     # Find optimal weights for each target return
#     for target in target_returns:
#         try:
#             weights, metrics = optimize_target_return(mean_returns, cov_matrix, target)
#             efficient_returns.append(metrics['return'])
#             efficient_volatilities.append(metrics['volatility'])
#             weights_list.append(weights)
#         except:
#             logger.warning(f"Could not find optimal weights for target return {target:.4f}")
# 
#     return np.array(efficient_returns), np.array(efficient_volatilities), weights_list
# 
# 
# # ==== Equal Risk Contribution (Risk Parity) ====
# 
# def calculate_risk_contribution(weights, cov_matrix):
#     """
#     Calculate risk contribution of each asset to portfolio volatility.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
# 
#     Returns:
#         numpy.array: Risk contribution of each asset
#     """
#     # Calculate portfolio volatility
#     portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     # Calculate marginal risk contribution
#     marginal_risk_contrib = np.dot(cov_matrix, weights) / portfolio_vol
# 
#     # Calculate risk contribution
#     risk_contrib = weights * marginal_risk_contrib
# 
#     return risk_contrib
# 
# 
# def risk_contribution_objective(weights, args):
#     """
#     Objective function for equal risk contribution portfolio.
#     Minimizes variance of risk contributions.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         args (tuple): (cov_matrix, target_risk_contrib)
# 
#     Returns:
#         float: Sum of squared risk contribution differences
#     """
#     cov_matrix, target_risk_contrib = args
# 
#     # Calculate current risk contributions
#     risk_contrib = calculate_risk_contribution(weights, cov_matrix)
# 
#     # Calculate sum of squared differences from target
#     risk_contrib_diff = risk_contrib - target_risk_contrib
#     return np.sum(risk_contrib_diff ** 2)
# 
# 
# def optimize_risk_parity(cov_matrix, constraints=None, bounds=None):
#     """
#     Optimize portfolio for equal risk contribution (risk parity).
# 
#     Args:
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = cov_matrix.shape[0]
# 
#     # Target: each asset contributes equally to risk
#     target_risk_contrib = np.ones(num_assets) / num_assets
# 
#     # Default constraints require weights to sum to 1
#     if constraints is None:
#         constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     args = (cov_matrix, target_risk_contrib)
#     result = minimize(risk_contribution_objective, init_guess, args=args, method='SLSQP',
#                      bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
# 
#     # Get optimal weights
#     optimal_weights = result['x']
# 
#     # Calculate performance metrics
#     if isinstance(cov_matrix, pd.DataFrame):
#         mean_returns = pd.Series(np.ones(num_assets) * 0.05, index=cov_matrix.index)  # Default mean returns
#     else:
#         mean_returns = np.ones(num_assets) * 0.05  # Default mean returns
# 
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - 0.02) / optimal_volatility  # Using 2% as default risk-free rate
# 
#     # Calculate final risk contributions
#     risk_contrib = calculate_risk_contribution(optimal_weights, cov_matrix)
#     risk_contrib_percent = risk_contrib / np.sum(risk_contrib)
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe,
#         'risk_contributions': risk_contrib,
#         'risk_contrib_percent': risk_contrib_percent
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# # ==== Maximum Diversification ====
# 
# def diversification_ratio(weights, cov_matrix):
#     """
#     Calculate diversification ratio.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
# 
#     Returns:
#         float: Diversification ratio
#     """
#     # Calculate weighted sum of individual volatilities
#     asset_vols = np.sqrt(np.diag(cov_matrix))
#     weighted_vols_sum = np.sum(weights * asset_vols)
# 
#     # Calculate portfolio volatility
#     portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     # Calculate diversification ratio
#     div_ratio = weighted_vols_sum / portfolio_vol
# 
#     return div_ratio
# 
# 
# def negative_diversification_ratio(weights, cov_matrix):
#     """
#     Calculate negative diversification ratio for optimization (minimization).
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
# 
#     Returns:
#         float: Negative diversification ratio
#     """
#     return -diversification_ratio(weights, cov_matrix)
# 
# 
# def optimize_maximum_diversification(cov_matrix, constraints=None, bounds=None):
#     """
#     Optimize portfolio for maximum diversification.
# 
#     Args:
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = cov_matrix.shape[0]
# 
#     # Default constraints require weights to sum to 1
#     if constraints is None:
#         constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Initial guess - equal weights
#     init_guess = np.ones(num_assets) / num_assets
# 
#     # Optimize
#     result = minimize(negative_diversification_ratio, init_guess, args=(cov_matrix,), method='SLSQP',
#                      bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
# 
#     # Get optimal weights
#     optimal_weights = result['x']
# 
#     # Calculate performance metrics
#     if isinstance(cov_matrix, pd.DataFrame):
#         mean_returns = pd.Series(np.ones(num_assets) * 0.05, index=cov_matrix.index)  # Default mean returns
#     else:
#         mean_returns = np.ones(num_assets) * 0.05  # Default mean returns
# 
#     optimal_return = portfolio_return(optimal_weights, mean_returns)
#     optimal_volatility = min_volatility(optimal_weights, mean_returns, cov_matrix)
#     optimal_sharpe = (optimal_return - 0.02) / optimal_volatility  # Using 2% as default risk-free rate
#     optimal_div_ratio = diversification_ratio(optimal_weights, cov_matrix)
# 
#     # Return weights and metrics
#     performance_metrics = {
#         'return': optimal_return,
#         'volatility': optimal_volatility,
#         'sharpe': optimal_sharpe,
#         'diversification_ratio': optimal_div_ratio
#     }
# 
#     return optimal_weights, performance_metrics
# 
# 
# # ==== Black-Litterman Model ====
# 
# def black_litterman_returns(cov_matrix, market_weights, risk_aversion=2.5, tau=0.05,
#                            views=None, pick_matrix=None, uncertainty=None):
#     """
#     Calculate expected returns using the Black-Litterman model.
# 
#     Args:
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         market_weights (numpy.array): Market capitalization weights
#         risk_aversion (float): Risk aversion coefficient
#         tau (float): Scaling factor for uncertainty
#         views (numpy.array, optional): Vector of views on expected returns
#         pick_matrix (numpy.array, optional): Matrix that picks assets for views
#         uncertainty (numpy.array, optional): Uncertainty of views
# 
#     Returns:
#         numpy.array: Black-Litterman expected returns
#     """
#     # Calculate implied equilibrium returns
#     implied_returns = risk_aversion * np.dot(cov_matrix, market_weights)
# 
#     # If no views, just return implied returns
#     if views is None:
#         return implied_returns
# 
#     # Define pick matrix if not provided
#     if pick_matrix is None:
#         pick_matrix = np.eye(len(market_weights))
# 
#     # Define uncertainty if not provided
#     if uncertainty is None:
#         uncertainty = np.diag(tau * np.diag(np.dot(pick_matrix, np.dot(cov_matrix, pick_matrix.T))))
# 
#     # Calculate posterior expected returns
#     scaled_cov = tau * cov_matrix
#     term1 = np.linalg.inv(scaled_cov)
#     term2 = np.dot(pick_matrix.T, np.dot(np.linalg.inv(uncertainty), pick_matrix))
#     term3 = np.dot(term1, implied_returns)
#     term4 = np.dot(pick_matrix.T, np.dot(np.linalg.inv(uncertainty), views))
# 
#     posterior_mean = np.dot(np.linalg.inv(term1 + term2), term3 + term4)
# 
#     return posterior_mean
# 
# 
# def optimize_black_litterman(cov_matrix, market_weights, risk_aversion=2.5, tau=0.05,
#                             views=None, pick_matrix=None, uncertainty=None,
#                             constraints=None, bounds=None):
#     """
#     Optimize portfolio using Black-Litterman model.
# 
#     Args:
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         market_weights (numpy.array): Market capitalization weights
#         risk_aversion (float): Risk aversion coefficient
#         tau (float): Scaling factor for uncertainty
#         views (numpy.array, optional): Vector of views on expected returns
#         pick_matrix (numpy.array, optional): Matrix that picks assets for views
#         uncertainty (numpy.array, optional): Uncertainty of views
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     # Calculate Black-Litterman expected returns
#     bl_returns = black_litterman_returns(cov_matrix, market_weights, risk_aversion, tau,
#                                         views, pick_matrix, uncertainty)
# 
#     # Optimize portfolio using BL returns
#     return optimize_sharpe_ratio(bl_returns, cov_matrix, risk_free_rate=0.02,
#                                constraints=constraints, bounds=bounds)
# 
# 
# # ==== Robust Optimization ====
# 
# def robust_optimization(mean_returns, cov_matrix, uncertainty=0.1, risk_free_rate=0.02,
#                        constraints=None, bounds=None):
#     """
#     Optimize portfolio with robustness to estimation errors.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         uncertainty (float): Uncertainty level for return estimates
#         risk_free_rate (float): Risk-free rate
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     # Create adjusted returns with uncertainty discount
#     adjusted_returns = mean_returns - uncertainty * np.sqrt(np.diag(cov_matrix))
# 
#     # Optimize using adjusted returns
#     return optimize_sharpe_ratio(adjusted_returns, cov_matrix, risk_free_rate,
#                                constraints=constraints, bounds=bounds)
# 
# 
# # ==== Hierarchical Risk Parity ====
# 
# def get_cluster_var(cov, cluster_items):
#     """
#     Calculate variance of a cluster.
# 
#     Args:
#         cov (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         cluster_items (list): Items in the cluster
# 
#     Returns:
#         float: Cluster variance
#     """
#     cov_slice = cov.iloc[cluster_items, cluster_items] if isinstance(cov, pd.DataFrame) else cov[cluster_items][:, cluster_items]
#     weights = np.ones(len(cluster_items)) / len(cluster_items)  # Equal weights within cluster
#     return np.dot(weights, np.dot(cov_slice, weights))
# 
# 
# def get_quasi_diag(link):
#     """
#     Sort clustered items by distance.
# 
#     Args:
#         link (numpy.array): Linkage matrix from hierarchical clustering
# 
#     Returns:
#         list: Sorted items
#     """
#     from scipy.cluster.hierarchy import to_tree
# 
#     # Initialize
#     link = link.astype(int)
#     sorted_items = [x for x in range(link.shape[0] + 1)]
# 
#     # Recursive function to extract sorted items
#     def get_children(node):
#         if node.is_leaf():
#             return [node.id]
#         else:
#             left = get_children(node.get_left())
#             right = get_children(node.get_right())
#             return left + right
# 
#     # Get the tree and sort
#     root = to_tree(link)
#     sorted_items = get_children(root)
# 
#     return sorted_items
# 
# 
# def hierarchical_risk_parity(cov_matrix, corr_matrix=None):
#     """
#     Optimize portfolio using Hierarchical Risk Parity.
# 
#     Args:
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         corr_matrix (pandas.DataFrame/numpy.ndarray, optional): Correlation matrix
# 
#     Returns:
#         numpy.array: Optimal weights
#     """
#     from scipy.cluster.hierarchy import linkage
#     from scipy.spatial.distance import squareform
# 
#     # Convert to DataFrame if numpy array
#     if isinstance(cov_matrix, np.ndarray):
#         cov_matrix = pd.DataFrame(cov_matrix)
# 
#     # Calculate correlation matrix if not provided
#     if corr_matrix is None:
#         corr_matrix = cov_matrix.corr()
# 
#     # Calculate distance matrix
#     distance_matrix = np.sqrt((1 - corr_matrix) / 2)
#     distance_condensed = squareform(distance_matrix)
# 
#     # Hierarchical clustering
#     link = linkage(distance_condensed, 'single')
# 
#     # Sort items
#     sorted_items = get_quasi_diag(link)
# 
#     # Get asset names if available
#     assets = cov_matrix.index.tolist() if isinstance(cov_matrix, pd.DataFrame) else list(range(cov_matrix.shape[0]))
#     sorted_assets = [assets[i] for i in sorted_items]
# 
#     # Initialize weights
#     weights = pd.Series(1, index=sorted_assets)
#     clustered_alphas = [sorted_assets]
# 
#     # Recursive bisection and weight assignment
#     while len(clustered_alphas) > 0:
#         clustered_alphas = [cluster[start:end] for cluster in clustered_alphas
#                           for start, end in ((0, len(cluster) // 2), (len(cluster) // 2, len(cluster)))
#                           if len(cluster) > 1]
# 
#         for cluster in clustered_alphas:
#             # Get cluster variance
#             cluster_var = get_cluster_var(cov_matrix, cluster)
#             # Update weights
#             weights[cluster] *= len(cluster) / cluster_var
# 
#     # Normalize weights
#     weights = weights / weights.sum()
# 
#     return weights.values
# 
# 
# # ==== Constrained Portfolio Optimization ====
# 
# def optimize_portfolio_with_sector_constraints(mean_returns, cov_matrix, sector_mapper,
#                                              min_sector_weight=0.0, max_sector_weight=1.0,
#                                              risk_free_rate=0.02):
#     """
#     Optimize portfolio with sector constraints.
# 
#     Args:
#         mean_returns (pandas.Series): Expected returns
#         cov_matrix (pandas.DataFrame): Covariance matrix
#         sector_mapper (dict): Mapping of assets to sectors
#         min_sector_weight (float/dict): Minimum weight per sector
#         max_sector_weight (float/dict): Maximum weight per sector
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     # Get assets and sectors
#     assets = mean_returns.index.tolist()
#     sectors = list(set(sector_mapper.values()))
# 
#     # Convert min/max weights to dictionaries if not already
#     if not isinstance(min_sector_weight, dict):
#         min_sector_weight = {sector: min_sector_weight for sector in sectors}
#     if not isinstance(max_sector_weight, dict):
#         max_sector_weight = {sector: max_sector_weight for sector in sectors}
# 
#     # Create sector constraints
#     constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]  # Weights sum to 1
# 
#     for sector in sectors:
#         # Get assets in this sector
#         sector_assets = [i for i, asset in enumerate(assets) if sector_mapper[asset] == sector]
# 
#         # Skip if no assets in this sector
#         if not sector_assets:
#             continue
# 
#         # Create sector weight calculation function
#         def sector_weight(weights, sector_indices=sector_assets):
#             return np.sum(weights[sector_indices])
# 
#         # Add minimum weight constraint for this sector
#         if min_sector_weight[sector] > 0:
#             constraints.append({
#                 'type': 'ineq',
#                 'fun': lambda weights, sector_indices=sector_assets: sector_weight(weights, sector_indices) - min_sector_weight[sector]
#             })
# 
#         # Add maximum weight constraint for this sector
#         if max_sector_weight[sector] < 1:
#             constraints.append({
#                 'type': 'ineq',
#                 'fun': lambda weights, sector_indices=sector_assets: max_sector_weight[sector] - sector_weight(weights, sector_indices)
#             })
# 
#     # Set bounds for weights
#     bounds = tuple((0, 1) for _ in range(len(assets)))
# 
#     # Optimize portfolio
#     return optimize_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate, constraints=constraints, bounds=bounds)
# 
# 
# def optimize_with_constraints(mean_returns, cov_matrix, constraints_list, risk_free_rate=0.02, bounds=None):
#     """
#     Optimize portfolio with custom constraints.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         constraints_list (list): List of constraint dictionaries
#         risk_free_rate (float): Risk-free rate
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     num_assets = len(mean_returns)
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Make sure constraints include weights sum to 1
#     sum_constraint = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
#     if constraints_list is None:
#         constraints_list = [sum_constraint]
#     else:
#         constraints_list.append(sum_constraint)
# 
#     # Optimize portfolio
#     return optimize_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate, constraints=constraints_list, bounds=bounds)
# 
# 
# # ==== Factor-Based Optimization ====
# 
# def optimize_factor_portfolio(returns, factor_exposures, factor_returns=None, factor_covariance=None,
#                              specific_returns=None, risk_free_rate=0.02, constraints=None, bounds=None):
#     """
#     Optimize portfolio based on factor exposures.
# 
#     Args:
#         returns (pandas.DataFrame): Asset returns
#         factor_exposures (pandas.DataFrame): Asset exposures to factors
#         factor_returns (pandas.DataFrame, optional): Factor returns
#         factor_covariance (pandas.DataFrame, optional): Factor covariance matrix
#         specific_returns (pandas.Series, optional): Asset-specific returns
#         risk_free_rate (float): Risk-free rate
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     # Calculate factor returns if not provided
#     if factor_returns is None:
#         # Regress asset returns on factor exposures
#         factor_returns = pd.DataFrame(index=returns.index, columns=factor_exposures.columns)
# 
#         for date in returns.index:
#             if date in factor_exposures.index:
#                 # Use exposures from this date
#                 exposures = factor_exposures.loc[date]
#             else:
#                 # Use most recent exposures
#                 exposures = factor_exposures.iloc[factor_exposures.index.get_indexer([date], method='pad')[0]]
# 
#             # Regress returns on exposures
#             asset_returns = returns.loc[date]
#             # Create design matrix
#             X = exposures.values.reshape(-1, len(exposures))
#             y = asset_returns.values
#             # Calculate factor returns using least squares
#             try:
#                 factor_returns.loc[date] = np.linalg.lstsq(X, y, rcond=None)[0]
#             except:
#                 # If regression fails, use zeros
#                 factor_returns.loc[date] = np.zeros(len(exposures))
# 
#     # Calculate factor covariance if not provided
#     if factor_covariance is None:
#         factor_covariance = factor_returns.cov()
# 
#     # Calculate specific returns if not provided
#     if specific_returns is None:
#         specific_returns = pd.Series(0.01, index=returns.columns)
# 
#     # Create factor-based covariance matrix
#     factor_cov = factor_exposures @ factor_covariance @ factor_exposures.T
#     specific_cov = np.diag(specific_returns)
#     total_cov = factor_cov + specific_cov
# 
#     # Calculate expected returns
#     expected_returns = factor_exposures @ factor_returns.mean()
# 
#     # Optimize portfolio
#     return optimize_sharpe_ratio(expected_returns, total_cov, risk_free_rate, constraints=constraints, bounds=bounds)
# 
# 
# # ==== Rebalancing Functions ====
# 
# def calculate_rebalancing_trades(current_weights, target_weights, portfolio_value):
#     """
#     Calculate trades needed to rebalance portfolio.
# 
#     Args:
#         current_weights (numpy.array): Current portfolio weights
#         target_weights (numpy.array): Target portfolio weights
#         portfolio_value (float): Current portfolio value
# 
#     Returns:
#         tuple: (trade_weights, trade_amounts)
#     """
#     # Calculate weight differences
#     weight_diff = target_weights - current_weights
# 
#     # Calculate trade amounts
#     trade_amounts = weight_diff * portfolio_value
# 
#     return weight_diff, trade_amounts
# 
# 
# def optimize_rebalancing(current_weights, target_weights, portfolio_value,
#                         transaction_costs=0.001, constraints=None, bounds=None):
#     """
#     Optimize rebalancing to minimize transaction costs.
# 
#     Args:
#         current_weights (numpy.array): Current portfolio weights
#         target_weights (numpy.array): Target portfolio weights
#         portfolio_value (float): Current portfolio value
#         transaction_costs (float/numpy.array): Transaction costs per asset
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, trade_amounts, transaction_cost)
#     """
#     num_assets = len(current_weights)
# 
#     # Convert transaction costs to array if scalar
#     if isinstance(transaction_costs, (int, float)):
#         transaction_costs = np.ones(num_assets) * transaction_costs
# 
#     # Define objective function to minimize transaction costs
#     def objective(weights):
#         trades = weights - current_weights
#         return np.sum(np.abs(trades) * transaction_costs)
# 
#     # Default constraints require weights to sum to 1
#     if constraints is None:
#         constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]
# 
#     # Default bounds are between 0 and 1 for all assets (long-only)
#     if bounds is None:
#         bounds = tuple((0, 1) for _ in range(num_assets))
# 
#     # Optimize
#     result = minimize(objective, current_weights, method='SLSQP',
#                     bounds=bounds, constraints=constraints)
# 
#     # Check if optimization was successful
#     if not result['success']:
#         logger.warning(f"Optimization failed: {result['message']}")
# 
#     # Calculate optimal weights, trades, and transaction cost
#     optimal_weights = result['x']
#     trade_weights, trade_amounts = calculate_rebalancing_trades(current_weights, optimal_weights, portfolio_value)
#     transaction_cost = objective(optimal_weights)
# 
#     return optimal_weights, trade_amounts, transaction_cost
# 
# 
# # ==== Multi-Period Optimization ====
# 
# def optimize_multi_period(mean_returns, cov_matrix, risk_free_rate=0.02,
#                          decay_factor=0.95, num_periods=10, constraints=None, bounds=None):
#     """
#     Optimize portfolio over multiple periods with decaying expected returns.
# 
#     Args:
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         risk_free_rate (float): Risk-free rate
#         decay_factor (float): Decay factor for expected returns
#         num_periods (int): Number of periods to optimize over
#         constraints (dict/list): Additional constraints
#         bounds (tuple): Bounds for weights
# 
#     Returns:
#         tuple: (optimal_weights, performance_metrics)
#     """
#     # Initialize
#     weights_over_time = []
#     metrics_over_time = []
#     current_returns = mean_returns.copy()
# 
#     # Optimize for each period
#     for period in range(num_periods):
#         # Optimize portfolio for current period
#         weights, metrics = optimize_sharpe_ratio(current_returns, cov_matrix, risk_free_rate,
#                                                constraints=constraints, bounds=bounds)
# 
#         # Store results
#         weights_over_time.append(weights)
#         metrics_over_time.append(metrics)
# 
#         # Decay expected returns for next period
#         current_returns = current_returns * decay_factor
# 
#     # Calculate average weights and metrics
#     avg_weights = np.mean(weights_over_time, axis=0)
#     avg_return = np.mean([metrics['return'] for metrics in metrics_over_time])
#     avg_volatility = np.mean([metrics['volatility'] for metrics in metrics_over_time])
#     avg_sharpe = np.mean([metrics['sharpe'] for metrics in metrics_over_time])
# 
#     # Normalize average weights to sum to 1
#     avg_weights = avg_weights / np.sum(avg_weights)
# 
#     # Calculate performance metrics for average weights
#     final_return = portfolio_return(avg_weights, mean_returns)
#     final_volatility = min_volatility(avg_weights, mean_returns, cov_matrix)
#     final_sharpe = (final_return - risk_free_rate) / final_volatility
# 
#     # Return average weights and metrics
#     performance_metrics = {
#         'return': final_return,
#         'volatility': final_volatility,
#         'sharpe': final_sharpe,
#         'avg_return': avg_return,
#         'avg_volatility': avg_volatility,
#         'avg_sharpe': avg_sharpe,
#         'weights_over_time': weights_over_time,
#         'metrics_over_time': metrics_over_time
#     }
# 
#     return avg_weights, performance_metrics
# 
# 
# # ==== Utility Functions ====
# 
# def get_risk_free_rate(date=None):
#     """
#     Get risk-free rate from Treasury data or default value.
# 
#     Args:
#         date (str/datetime, optional): Date for risk-free rate
# 
#     Returns:
#         float: Risk-free rate
#     """
#     # Default risk-free rate
#     default_rate = 0.02
# 
#     try:
#         # Try to get Treasury data if available
#         import pandas_datareader.data as web
# 
#         if date is None:
#             date = datetime.now()
# 
#         # Convert to datetime if string
#         if isinstance(date, str):
#             date = pd.to_datetime(date)
# 
#         # Get 3-month Treasury bill rate
#         start_date = date - timedelta(days=5)
#         end_date = date
#         treasury = web.DataReader('DGS3MO', 'fred', start_date, end_date)
# 
#         # Get most recent rate
#         rate = treasury.iloc[-1, 0] / 100  # Convert from percentage to decimal
# 
#         return rate
#     except:
#         # Return default if data not available
#         logger.warning(f"Could not fetch Treasury data. Using default risk-free rate of {default_rate:.2%}")
#         return default_rate
# 
# 
# def compare_portfolio_weights(weights_dict, title="Сравнение весов портфелей"):
#     """
#     Compare weights of different portfolios.
# 
#     Args:
#         weights_dict (dict): Dictionary of portfolio weights {name: weights}
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Get number of portfolios and assets
#     num_portfolios = len(weights_dict)
# 
#     # Get asset names
#     for name, weights in weights_dict.items():
#         if hasattr(weights, 'index'):
#             asset_names = weights.index
#             break
#     else:
#         # If no index found, use generic asset names
#         num_assets = len(next(iter(weights_dict.values())))
#         asset_names = [f"Asset {i+1}" for i in range(num_assets)]
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 8))
# 
#     # Set width of bars
#     bar_width = 0.8 / num_portfolios
# 
#     # Set positions of bars on x-axis
#     positions = np.arange(len(asset_names))
# 
#     # Plot bars for each portfolio
#     for i, (name, weights) in enumerate(weights_dict.items()):
#         # Calculate offset
#         offset = (i - num_portfolios / 2 + 0.5) * bar_width
# 
#         # Plot bars
#         bars = ax.bar(positions + offset, weights, bar_width, label=name, alpha=0.7)
# 
#         # Add value labels
#         for bar in bars:
#             height = bar.get_height()
#             if height >= 0.05:  # Only show labels for weights >= 5%
#                 ax.annotate(f'{height:.1%}',
#                           xy=(bar.get_x() + bar.get_width() / 2, height),
#                           xytext=(0, 3),
#                           textcoords="offset points",
#                           ha='center', va='bottom',
#                           fontsize=8)
# 
#     # Set x-axis labels
#     ax.set_xticks(positions)
#     ax.set_xticklabels(asset_names, rotation=45, ha='right')
# 
#     # Set labels and title
#     ax.set_xlabel('Активы')
#     ax.set_ylabel('Веса')
#     ax.set_title(title)
# 
#     # Add legend
#     ax.legend()
# 
#     # Adjust layout
#     plt.tight_layout()
# 
#     return fig
# 
# 
# def compare_portfolio_performance(performance_metrics_dict, title="Сравнение показателей портфелей"):
#     """
#     Compare performance metrics of different portfolios.
# 
#     Args:
#         performance_metrics_dict (dict): Dictionary of performance metrics {name: metrics}
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Extract metrics for comparison
#     metrics_to_compare = ['return', 'volatility', 'sharpe']
# 
#     # Create data for plotting
#     portfolio_names = list(performance_metrics_dict.keys())
#     returns = [performance_metrics_dict[name]['return'] for name in portfolio_names]
#     volatilities = [performance_metrics_dict[name]['volatility'] for name in portfolio_names]
#     sharpe_ratios = [performance_metrics_dict[name]['sharpe'] for name in portfolio_names]
# 
#     # Create figure
#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 6))
# 
#     # Plot returns
#     bars1 = ax1.bar(portfolio_names, returns, color='skyblue')
#     ax1.set_ylabel('Ожидаемая доходность')
#     ax1.set_title('Доходность')
#     ax1.tick_params(axis='x', rotation=45)
# 
#     # Add value labels
#     for bar in bars1:
#         height = bar.get_height()
#         ax1.annotate(f'{height:.2%}',
#                     xy=(bar.get_x() + bar.get_width() / 2, height),
#                     xytext=(0, 3),
#                     textcoords="offset points",
#                     ha='center', va='bottom')
# 
#     # Plot volatilities
#     bars2 = ax2.bar(portfolio_names, volatilities, color='salmon')
#     ax2.set_ylabel('Ожидаемая волатильность')
#     ax2.set_title('Риск')
#     ax2.tick_params(axis='x', rotation=45)
# 
#     # Add value labels
#     for bar in bars2:
#         height = bar.get_height()
#         ax2.annotate(f'{height:.2%}',
#                     xy=(bar.get_x() + bar.get_width() / 2, height),
#                     xytext=(0, 3),
#                     textcoords="offset points",
#                     ha='center', va='bottom')
# 
#     # Plot Sharpe ratios
#     bars3 = ax3.bar(portfolio_names, sharpe_ratios, color='lightgreen')
#     ax3.set_ylabel('Коэффициент Шарпа')
#     ax3.set_title('Коэффициент Шарпа')
#     ax3.tick_params(axis='x', rotation=45)
# 
#     # Add value labels
#     for bar in bars3:
#         height = bar.get_height()
#         ax3.annotate(f'{height:.2f}',
#                     xy=(bar.get_x() + bar.get_width() / 2, height),
#                     xytext=(0, 3),
#                     textcoords="offset points",
#                     ha='center', va='bottom')
# 
#     # Set overall title
#     fig.suptitle(title, fontsize=16)
# 
#     # Adjust layout
#     plt.tight_layout()
#     plt.subplots_adjust(top=0.9)
# 
#     return fig
# 
# 
# # ==== Testing Functions ====
# 
# def test_optimization_module():
#     """Test function for the optimization module."""
#     print("\n=== Тестирование модуля оптимизации ===")
# 
#     # Generate sample data
#     np.random.seed(42)
# 
#     # Sample returns and covariance
#     mean_returns = pd.Series([0.10, 0.15, 0.05, 0.12], index=['AAPL', 'MSFT', 'GOOGL', 'AMZN'])
# 
#     # Create covariance matrix with realistic correlations
#     cov_data = np.array([
#         [0.04, 0.02, 0.01, 0.02],
#         [0.02, 0.05, 0.02, 0.03],
#         [0.01, 0.02, 0.03, 0.01],
#         [0.02, 0.03, 0.01, 0.04]
#     ])
#     cov_matrix = pd.DataFrame(cov_data, index=mean_returns.index, columns=mean_returns.index)
# 
#     # Test optimizations
#     print("\nТест оптимизации максимального коэффициента Шарпа:")
#     max_sharpe_weights, max_sharpe_metrics = optimize_sharpe_ratio(mean_returns, cov_matrix)
#     print(f"Веса: {[f'{w:.2f}' for w in max_sharpe_weights]}")
#     print(f"Доходность: {max_sharpe_metrics['return']:.2%}")
#     print(f"Волатильность: {max_sharpe_metrics['volatility']:.2%}")
#     print(f"Коэффициент Шарпа: {max_sharpe_metrics['sharpe']:.2f}")
# 
#     print("\nТест оптимизации минимальной волатильности:")
#     min_vol_weights, min_vol_metrics = optimize_minimum_volatility(mean_returns, cov_matrix)
#     print(f"Веса: {[f'{w:.2f}' for w in min_vol_weights]}")
#     print(f"Доходность: {min_vol_metrics['return']:.2%}")
#     print(f"Волатильность: {min_vol_metrics['volatility']:.2%}")
#     print(f"Коэффициент Шарпа: {min_vol_metrics['sharpe']:.2f}")
# 
#     print("\nТест оптимизации с целевой доходностью:")
#     target_return = 0.12
#     target_ret_weights, target_ret_metrics = optimize_target_return(mean_returns, cov_matrix, target_return)
#     print(f"Целевая доходность: {target_return:.2%}")
#     print(f"Веса: {[f'{w:.2f}' for w in target_ret_weights]}")
#     print(f"Доходность: {target_ret_metrics['return']:.2%}")
#     print(f"Волатильность: {target_ret_metrics['volatility']:.2%}")
#     print(f"Коэффициент Шарпа: {target_ret_metrics['sharpe']:.2f}")
# 
#     print("\nТест равного распределения риска (Risk Parity):")
#     rp_weights, rp_metrics = optimize_risk_parity(cov_matrix)
#     print(f"Веса: {[f'{w:.2f}' for w in rp_weights]}")
#     print(f"Доходность: {rp_metrics['return']:.2%}")
#     print(f"Волатильность: {rp_metrics['volatility']:.2%}")
#     print(f"Коэффициент Шарпа: {rp_metrics['sharpe']:.2f}")
#     print(f"Распределение риска: {[f'{r:.2%}' for r in rp_metrics['risk_contrib_percent']]}")
# 
#     print("\nТест максимальной диверсификации:")
#     max_div_weights, max_div_metrics = optimize_maximum_diversification(cov_matrix)
#     print(f"Веса: {[f'{w:.2f}' for w in max_div_weights]}")
#     print(f"Доходность: {max_div_metrics['return']:.2%}")
#     print(f"Волатильность: {max_div_metrics['volatility']:.2%}")
#     print(f"Коэффициент Шарпа: {max_div_metrics['sharpe']:.2f}")
#     print(f"Коэффициент диверсификации: {max_div_metrics['diversification_ratio']:.2f}")
# 
#     print("\nСравнение различных методов оптимизации:")
#     # Compare portfolio weights
#     weights_dict = {
#         'Max Sharpe': max_sharpe_weights,
#         'Min Volatility': min_vol_weights,
#         'Target Return': target_ret_weights,
#         'Risk Parity': rp_weights,
#         'Max Diversification': max_div_weights
#     }
# 
#     fig1 = compare_portfolio_weights(weights_dict)
#     plt.close(fig1)
# 
#     # Compare portfolio performance
#     performance_dict = {
#         'Max Sharpe': max_sharpe_metrics,
#         'Min Volatility': min_vol_metrics,
#         'Target Return': target_ret_metrics,
#         'Risk Parity': rp_metrics,
#         'Max Diversification': max_div_metrics
#     }
# 
#     fig2 = compare_portfolio_performance(performance_dict)
#     plt.close(fig2)
# 
#     print("\n=== Тестирование завершено успешно ===")
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_optimization_module()

"""## module_visualization.py

Задача: Визуализация результатов и вывод информации о портфеле.

Основная функция:

Визуализация данных о доходности и риске портфеля на графике.

Вывод информации о результатах оптимизации (веса активов, ожидаемая доходность, риск).

Методы в модуле:

plot_results(weights, mean_returns, cov_matrix): Строит график риска и доходности портфеля и выводит информацию о его характеристиках.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_visualization.py
# """
# Module for portfolio visualization.
# Provides comprehensive functions for visualizing portfolio performance,
# risk metrics, historical data, and optimization results.
# """
# 
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# from matplotlib import cm
# from matplotlib.ticker import FuncFormatter
# import logging
# from datetime import datetime, timedelta
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# def plot_correlation_matrix(returns_data, title="Корреляционная матрица активов"):
#     """
#     Plot a correlation matrix heatmap for asset returns.
# 
#     Args:
#         returns_data (pandas.DataFrame): Returns data for assets
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Calculate correlation matrix
#     correlation = returns_data.corr()
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 8))
# 
#     # Create heatmap
#     mask = np.triu(np.ones_like(correlation, dtype=bool))
#     cmap = sns.diverging_palette(230, 20, as_cmap=True)
# 
#     sns.heatmap(
#         correlation,
#         mask=mask,
#         cmap=cmap,
#         annot=True,
#         fmt=".2f",
#         center=0,
#         square=True,
#         linewidths=.5,
#         cbar_kws={"shrink": .8},
#         ax=ax
#     )
# 
#     # Set title
#     ax.set_title(title, fontsize=14, pad=20)
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_efficient_frontier(mean_returns, cov_matrix, num_portfolios=5000, risk_free_rate=0.02,
#                             show_assets=True, show_sharpe=True, show_min_vol=True,
#                             optimized_weights=None, marker_labels=None):
#     """
#     Plot the efficient frontier with optional asset points, Sharpe ratio, and min volatility portfolios.
# 
#     Args:
#         mean_returns (pandas.Series): Series of expected returns
#         cov_matrix (pandas.DataFrame): Covariance matrix
#         num_portfolios (int): Number of random portfolios to generate
#         risk_free_rate (float): Risk-free rate
#         show_assets (bool): Whether to show individual assets
#         show_sharpe (bool): Whether to show maximum Sharpe ratio portfolio
#         show_min_vol (bool): Whether to show minimum volatility portfolio
#         optimized_weights (numpy.array, optional): Weights of an optimized portfolio to highlight
#         marker_labels (list, optional): Labels for special portfolios
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Import optimization functions here to avoid circular imports
#     from module_optimization import portfolio_return, portfolio_performance, min_volatility
#     from scipy.optimize import minimize
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 7))
# 
#     # Get asset names
#     assets = mean_returns.index if isinstance(mean_returns, pd.Series) else [f"Asset {i+1}" for i in range(len(mean_returns))]
#     num_assets = len(assets)
# 
#     # Generate random portfolios
#     results = np.zeros((4, num_portfolios))
#     weights_record = np.zeros((num_portfolios, num_assets))
# 
#     for i in range(num_portfolios):
#         weights = np.random.random(num_assets)
#         weights /= np.sum(weights)
#         weights_record[i] = weights
# 
#         # Calculate portfolio metrics
#         portfolio_ret = portfolio_return(weights, mean_returns)
#         portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
#         results[0, i] = portfolio_ret
#         results[1, i] = portfolio_vol
#         results[2, i] = (portfolio_ret - risk_free_rate) / portfolio_vol  # Sharpe ratio
#         results[3, i] = i
# 
#     # Plot random portfolios
#     scatter = ax.scatter(results[1, :], results[0, :],
#                         c=results[2, :], cmap='viridis',
#                         marker='o', s=10, alpha=0.3)
# 
#     # Show color bar
#     cbar = plt.colorbar(scatter)
#     cbar.set_label('Коэффициент Шарпа')
# 
#     # Optimize for maximum Sharpe ratio
#     if show_sharpe:
#         constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
#         bounds = tuple((0, 1) for _ in range(num_assets))
#         initial_weights = np.array(num_assets * [1. / num_assets])
# 
#         max_sharpe = minimize(portfolio_performance,
#                               initial_weights,
#                               args=(mean_returns, cov_matrix, risk_free_rate),
#                               method='SLSQP',
#                               bounds=bounds,
#                               constraints=constraints)
# 
#         max_sharpe_weights = max_sharpe['x']
#         max_sharpe_ret = portfolio_return(max_sharpe_weights, mean_returns)
#         max_sharpe_vol = min_volatility(max_sharpe_weights, mean_returns, cov_matrix)
#         max_sharpe_sharpe = (max_sharpe_ret - risk_free_rate) / max_sharpe_vol
# 
#         # Plot maximum Sharpe ratio portfolio
#         ax.scatter(max_sharpe_vol, max_sharpe_ret,
#                   marker='*', color='red', s=200,
#                   label='Максимальный коэффициент Шарпа')
# 
#         # Annotate
#         if marker_labels is None or "max_sharpe" in marker_labels:
#             ax.annotate(f'Макс. Шарп: {max_sharpe_sharpe:.2f}',
#                       xy=(max_sharpe_vol, max_sharpe_ret),
#                       xytext=(max_sharpe_vol+0.005, max_sharpe_ret+0.005),
#                       fontsize=10)
# 
#     # Optimize for minimum volatility
#     if show_min_vol:
#         constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
#         bounds = tuple((0, 1) for _ in range(num_assets))
#         initial_weights = np.array(num_assets * [1. / num_assets])
# 
#         min_vol = minimize(min_volatility,
#                           initial_weights,
#                           args=(mean_returns, cov_matrix),
#                           method='SLSQP',
#                           bounds=bounds,
#                           constraints=constraints)
# 
#         min_vol_weights = min_vol['x']
#         min_vol_ret = portfolio_return(min_vol_weights, mean_returns)
#         min_vol_vol = min_volatility(min_vol_weights, mean_returns, cov_matrix)
#         min_vol_sharpe = (min_vol_ret - risk_free_rate) / min_vol_vol
# 
#         # Plot minimum volatility portfolio
#         ax.scatter(min_vol_vol, min_vol_ret,
#                   marker='o', color='green', s=200,
#                   label='Минимальная волатильность')
# 
#         # Annotate
#         if marker_labels is None or "min_vol" in marker_labels:
#             ax.annotate(f'Мин. волатильность: {min_vol_vol:.2f}',
#                       xy=(min_vol_vol, min_vol_ret),
#                       xytext=(min_vol_vol+0.005, min_vol_ret-0.015),
#                       fontsize=10)
# 
#     # Plot individual assets
#     if show_assets:
#         for i, asset in enumerate(assets):
#             asset_return = mean_returns.iloc[i] if isinstance(mean_returns, pd.Series) else mean_returns[i]
#             asset_vol = np.sqrt(cov_matrix.iloc[i, i]) if isinstance(cov_matrix, pd.DataFrame) else np.sqrt(cov_matrix[i, i])
# 
#             ax.scatter(asset_vol, asset_return,
#                       marker='D', s=100,
#                       label=asset)
# 
#             ax.annotate(asset,
#                       xy=(asset_vol, asset_return),
#                       xytext=(asset_vol+0.002, asset_return),
#                       fontsize=9)
# 
#     # Plot optimized portfolio if provided
#     if optimized_weights is not None:
#         opt_ret = portfolio_return(optimized_weights, mean_returns)
#         opt_vol = min_volatility(optimized_weights, mean_returns, cov_matrix)
#         opt_sharpe = (opt_ret - risk_free_rate) / opt_vol
# 
#         ax.scatter(opt_vol, opt_ret,
#                   marker='X', color='magenta', s=200,
#                   label='Оптимизированный портфель')
# 
#         # Annotate
#         ax.annotate(f'Опт. портфель: {opt_sharpe:.2f}',
#                   xy=(opt_vol, opt_ret),
#                   xytext=(opt_vol+0.005, opt_ret+0.005),
#                   fontsize=10)
# 
#     # Set labels and title
#     ax.set_xlabel('Волатильность (стандартное отклонение)')
#     ax.set_ylabel('Ожидаемая доходность')
#     ax.set_title('Эффективный фронт портфеля', fontsize=14)
# 
#     # Set percentage format for y-axis
#     ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.1%}'.format(y)))
#     ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.1%}'.format(x)))
# 
#     # Add legend
#     ax.legend()
# 
#     # Add capital allocation line if showing max Sharpe
#     if show_sharpe:
#         # Get min and max x values
#         x_min, x_max = ax.get_xlim()
# 
#         # Calculate capital allocation line
#         x_values = np.linspace(0, x_max * 1.2, 100)
#         y_values = risk_free_rate + max_sharpe_sharpe * x_values
# 
#         # Plot capital allocation line
#         ax.plot(x_values, y_values, 'r--', label='CAL')
# 
#         # Update plot limits
#         ax.set_xlim(0, x_max * 1.1)
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_portfolio_weights(weights, labels=None, title="Распределение портфеля", threshold=0.01):
#     """
#     Plot portfolio weights as a pie chart.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         labels (list, optional): Asset labels
#         title (str): Plot title
#         threshold (float): Minimum weight to show separately
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Initialize weights and labels
#     weights = np.array(weights)
#     if labels is None:
#         labels = [f"Asset {i+1}" for i in range(len(weights))]
# 
#     # Combine small weights into 'Other'
#     other_mask = weights < threshold
#     if np.any(other_mask):
#         other_weight = np.sum(weights[other_mask])
#         plot_weights = weights[~other_mask]
#         plot_labels = [label for i, label in enumerate(labels) if not other_mask[i]]
# 
#         if other_weight > 0:
#             plot_weights = np.append(plot_weights, other_weight)
#             plot_labels.append('Другие')
#     else:
#         plot_weights = weights
#         plot_labels = labels
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 7))
# 
#     # Create pie chart with percentages
#     wedges, texts, autotexts = ax.pie(
#         plot_weights,
#         labels=None,
#         autopct='%1.1f%%',
#         startangle=90,
#         wedgeprops={'edgecolor': 'w', 'linewidth': 1},
#         textprops={'fontsize': 10}
#     )
# 
#     # Enhance appearance of percentages
#     for autotext in autotexts:
#         autotext.set_color('white')
#         autotext.set_fontweight('bold')
# 
#     # Add legend
#     ax.legend(
#         wedges,
#         [f"{label} ({weight:.1%})" for label, weight in zip(plot_labels, plot_weights)],
#         title="Активы",
#         loc="center left",
#         bbox_to_anchor=(1, 0, 0.5, 1)
#     )
# 
#     # Set title
#     ax.set_title(title, fontsize=14)
# 
#     # Equal aspect ratio ensures the pie chart is circular
#     ax.set_aspect('equal')
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_portfolio_comparison(portfolios, metrics, labels=None, title="Сравнение портфелей"):
#     """
#     Plot a comparison of multiple portfolios using a radar chart.
# 
#     Args:
#         portfolios (list): List of portfolios to compare (each portfolio is a list of metrics)
#         metrics (list): List of metric names
#         labels (list, optional): Portfolio labels
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Set labels if not provided
#     if labels is None:
#         labels = [f"Portfolio {i+1}" for i in range(len(portfolios))]
# 
#     # Number of metrics
#     num_metrics = len(metrics)
# 
#     # Compute angles for each metric
#     angles = np.linspace(0, 2*np.pi, num_metrics, endpoint=False).tolist()
#     angles += angles[:1]  # Close the loop
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))
# 
#     # Add metrics to the chart
#     for i, portfolio in enumerate(portfolios):
#         # Close the loop for this portfolio
#         values = portfolio.tolist() if hasattr(portfolio, 'tolist') else portfolio
#         values += values[:1]
# 
#         # Plot portfolio
#         ax.plot(angles, values, linewidth=2, label=labels[i])
#         ax.fill(angles, values, alpha=0.1)
# 
#     # Set metric labels
#     ax.set_xticks(angles[:-1])
#     ax.set_xticklabels(metrics)
# 
#     # Add labels
#     ax.set_title(title, fontsize=14, pad=20)
#     ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_return_distribution(returns, bins=50, title="Распределение доходности"):
#     """
#     Plot the distribution of returns.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         bins (int): Number of bins for histogram
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 6))
# 
#     # Check if we have a Series or DataFrame
#     if isinstance(returns, pd.DataFrame):
#         # Plot histogram for each column
#         for column in returns.columns:
#             sns.histplot(returns[column], bins=bins, kde=True, label=column, alpha=0.5, ax=ax)
#     else:
#         # Plot histogram for the Series
#         sns.histplot(returns, bins=bins, kde=True, ax=ax)
# 
#     # Add vertical line at mean return
#     if isinstance(returns, pd.DataFrame):
#         for column in returns.columns:
#             ax.axvline(returns[column].mean(), color='r', linestyle='--', alpha=0.3)
#     else:
#         ax.axvline(returns.mean(), color='r', linestyle='--', label='Средняя доходность')
# 
#     # Add vertical line at zero
#     ax.axvline(0, color='k', linestyle='-', alpha=0.3, label='Нулевая доходность')
# 
#     # Set labels and title
#     ax.set_xlabel('Доходность')
#     ax.set_ylabel('Частота')
#     ax.set_title(title, fontsize=14)
# 
#     # Format x-axis as percentage
#     ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: '{:.1%}'.format(x)))
# 
#     # Add legend
#     ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_cumulative_returns(returns, initial_investment=1.0, title="Кумулятивная доходность"):
#     """
#     Plot cumulative returns over time.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         initial_investment (float): Initial investment amount
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Calculate cumulative returns
#     cumulative_returns = (1 + returns).cumprod() * initial_investment
# 
#     # Plot cumulative returns
#     if isinstance(returns, pd.DataFrame):
#         for column in cumulative_returns.columns:
#             ax.plot(cumulative_returns.index, cumulative_returns[column], label=column)
#     else:
#         ax.plot(cumulative_returns.index, cumulative_returns)
# 
#     # Set labels and title
#     ax.set_xlabel('Дата')
#     ax.set_ylabel('Стоимость')
#     ax.set_title(title, fontsize=14)
# 
#     # Format y-axis with currency
#     ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '${:,.0f}'.format(y)))
# 
#     # Add grid
#     ax.grid(alpha=0.3)
# 
#     # Add legend if we have multiple columns
#     if isinstance(returns, pd.DataFrame) and len(returns.columns) > 1:
#         ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_drawdown(returns, title="Просадка портфеля"):
#     """
#     Plot portfolio drawdown over time.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Calculate drawdown
#     cum_returns = (1 + returns).cumprod()
#     running_max = cum_returns.cummax()
#     drawdown = (cum_returns / running_max) - 1
# 
#     # Plot drawdown
#     if isinstance(returns, pd.DataFrame):
#         for column in drawdown.columns:
#             ax.plot(drawdown.index, drawdown[column], label=column)
#     else:
#         ax.plot(drawdown.index, drawdown)
# 
#     # Shade the areas below zero
#     ax.fill_between(drawdown.index, 0, drawdown.min().min(), alpha=0.1, color='red')
# 
#     # Set labels and title
#     ax.set_xlabel('Дата')
#     ax.set_ylabel('Просадка')
#     ax.set_title(title, fontsize=14)
# 
#     # Format y-axis as percentage
#     ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
# 
#     # Add grid
#     ax.grid(alpha=0.3)
# 
#     # Add legend if we have multiple columns
#     if isinstance(returns, pd.DataFrame) and len(returns.columns) > 1:
#         ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_rolling_volatility(returns, window=252, title="Скользящая волатильность"):
#     """
#     Plot rolling volatility over time.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         window (int): Rolling window size in days
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Calculate rolling volatility (annualized)
#     rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)
# 
#     # Plot rolling volatility
#     if isinstance(returns, pd.DataFrame):
#         for column in rolling_vol.columns:
#             ax.plot(rolling_vol.index, rolling_vol[column], label=column)
#     else:
#         ax.plot(rolling_vol.index, rolling_vol)
# 
#     # Set labels and title
#     ax.set_xlabel('Дата')
#     ax.set_ylabel('Годовая волатильность')
#     ax.set_title(f"{title} ({window} дней)", fontsize=14)
# 
#     # Format y-axis as percentage
#     ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
# 
#     # Add grid
#     ax.grid(alpha=0.3)
# 
#     # Add legend if we have multiple columns
#     if isinstance(returns, pd.DataFrame) and len(returns.columns) > 1:
#         ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_rolling_sharpe(returns, window=252, risk_free_rate=0.02, title="Скользящий коэффициент Шарпа"):
#     """
#     Plot rolling Sharpe ratio over time.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         window (int): Rolling window size in days
#         risk_free_rate (float): Annual risk-free rate
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Convert annual risk-free rate to daily
#     daily_rf = ((1 + risk_free_rate) ** (1/252)) - 1
# 
#     # Calculate excess returns
#     excess_returns = returns - daily_rf
# 
#     # Calculate rolling Sharpe ratio (annualized)
#     rolling_mean = excess_returns.rolling(window=window).mean() * 252
#     rolling_std = returns.rolling(window=window).std() * np.sqrt(252)
#     rolling_sharpe = rolling_mean / rolling_std
# 
#     # Plot rolling Sharpe ratio
#     if isinstance(returns, pd.DataFrame):
#         for column in rolling_sharpe.columns:
#             ax.plot(rolling_sharpe.index, rolling_sharpe[column], label=column)
#     else:
#         ax.plot(rolling_sharpe.index, rolling_sharpe)
# 
#     # Add horizontal line at zero
#     ax.axhline(y=0, color='r', linestyle='--', alpha=0.3)
# 
#     # Set labels and title
#     ax.set_xlabel('Дата')
#     ax.set_ylabel('Коэффициент Шарпа')
#     ax.set_title(f"{title} ({window} дней)", fontsize=14)
# 
#     # Add grid
#     ax.grid(alpha=0.3)
# 
#     # Add legend if we have multiple columns
#     if isinstance(returns, pd.DataFrame) and len(returns.columns) > 1:
#         ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_monthly_returns_heatmap(returns, title="Месячная доходность"):
#     """
#     Plot monthly returns as a heatmap.
# 
#     Args:
#         returns (pandas.Series/DataFrame): Returns data
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Resample returns to monthly
#     monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
# 
#     # Create a pivot table with years as rows and months as columns
#     if isinstance(returns, pd.DataFrame):
#         figs = []
# 
#         for column in returns.columns:
#             column_returns = returns[column]
#             monthly_column = column_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
# 
#             monthly_pivot = monthly_column.unstack(level=0) if isinstance(monthly_column, pd.DataFrame) else \
#                 pd.Series(monthly_column, name=column).groupby([lambda x: x.year, lambda x: x.month]).first().unstack()
# 
#             # Create figure
#             fig, ax = plt.subplots(figsize=(12, 8))
# 
#             # Create heatmap
#             sns.heatmap(
#                 monthly_pivot,
#                 annot=True,
#                 fmt=".1%",
#                 cmap="RdYlGn",
#                 center=0,
#                 linewidths=1,
#                 cbar=True,
#                 ax=ax
#             )
# 
#             # Set labels and title
#             ax.set_title(f"{title}: {column}", fontsize=14)
#             ax.set_ylabel('Год')
#             ax.set_xlabel('Месяц')
# 
#             # Custom month labels
#             month_labels = ['Янв', 'Фев', 'Мар', 'Апр', 'Май', 'Июн', 'Июл', 'Авг', 'Сен', 'Окт', 'Ноя', 'Дек']
#             if len(monthly_pivot.columns) == 12:
#                 ax.set_xticklabels(month_labels)
# 
#             figs.append(fig)
# 
#         # Return last figure if multiple were created
#         return figs[-1] if figs else None
#     else:
#         monthly_pivot = monthly_returns.groupby([lambda x: x.year, lambda x: x.month]).first().unstack()
# 
#         # Create figure
#         fig, ax = plt.subplots(figsize=(12, 8))
# 
#         # Create heatmap
#         sns.heatmap(
#             monthly_pivot,
#             annot=True,
#             fmt=".1%",
#             cmap="RdYlGn",
#             center=0,
#             linewidths=1,
#             cbar=True,
#             ax=ax
#         )
# 
#         # Set labels and title
#         ax.set_title(title, fontsize=14)
#         ax.set_ylabel('Год')
#         ax.set_xlabel('Месяц')
# 
#         # Custom month labels
#         month_labels = ['Янв', 'Фев', 'Мар', 'Апр', 'Май', 'Июн', 'Июл', 'Авг', 'Сен', 'Окт', 'Ноя', 'Дек']
#         if len(monthly_pivot.columns) == 12:
#             ax.set_xticklabels(month_labels)
# 
#         return fig
# 
# 
# def plot_asset_contribution(weights, mean_returns, cov_matrix, title="Вклад активов в портфель"):
#     """
#     Plot contribution of each asset to portfolio return and risk.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Get asset names
#     assets = mean_returns.index if isinstance(mean_returns, pd.Series) else [f"Asset {i+1}" for i in range(len(mean_returns))]
# 
#     # Calculate total portfolio return and risk
#     portfolio_return = np.sum(weights * mean_returns)
#     portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     # Calculate contribution to return
#     return_contribution = weights * mean_returns / portfolio_return
# 
#     # Calculate contribution to risk (marginal contribution to risk)
#     risk_contribution = np.array([])
#     for i in range(len(weights)):
#         # Calculate marginal contribution to risk
#         mcr = np.dot(cov_matrix[i], weights) / portfolio_volatility
#         # Calculate total contribution to risk
#         cr = weights[i] * mcr
#         risk_contribution = np.append(risk_contribution, cr)
# 
#     # Normalize risk contribution
#     risk_contribution = risk_contribution / np.sum(risk_contribution)
# 
#     # Create figure with two subplots
#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
# 
#     # Plot return contribution
#     return_bars = ax1.bar(assets, return_contribution, color='skyblue')
#     ax1.set_title('Вклад в доходность', fontsize=12)
#     ax1.set_ylabel('Доля от общей доходности')
#     ax1.tick_params(axis='x', rotation=45)
#     ax1.grid(alpha=0.3)
# 
#     # Add percentage labels
#     for bar in return_bars:
#         height = bar.get_height()
#         ax1.annotate(f'{height:.1%}',
#                    xy=(bar.get_x() + bar.get_width() / 2, height),
#                    xytext=(0, 3),
#                    textcoords="offset points",
#                    ha='center', va='bottom')
# 
#     # Plot risk contribution
#     risk_bars = ax2.bar(assets, risk_contribution, color='salmon')
#     ax2.set_title('Вклад в риск', fontsize=12)
#     ax2.set_ylabel('Доля от общего риска')
#     ax2.tick_params(axis='x', rotation=45)
#     ax2.grid(alpha=0.3)
# 
#     # Add percentage labels
#     for bar in risk_bars:
#         height = bar.get_height()
#         ax2.annotate(f'{height:.1%}',
#                    xy=(bar.get_x() + bar.get_width() / 2, height),
#                    xytext=(0, 3),
#                    textcoords="offset points",
#                    ha='center', va='bottom')
# 
#     # Set overall title
#     fig.suptitle(title, fontsize=14)
# 
#     plt.tight_layout()
#     plt.subplots_adjust(top=0.9)
#     return fig
# 
# 
# def plot_efficient_frontier_3d(mean_returns, cov_matrix, num_portfolios=5000, risk_free_rate=0.02):
#     """
#     Plot the efficient frontier in 3D with return, volatility, and Sharpe ratio.
# 
#     Args:
#         mean_returns (pandas.Series): Series of expected returns
#         cov_matrix (pandas.DataFrame): Covariance matrix
#         num_portfolios (int): Number of random portfolios to generate
#         risk_free_rate (float): Risk-free rate
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Import optimization functions here to avoid circular imports
#     from module_optimization import portfolio_return, portfolio_performance, min_volatility
#     from scipy.optimize import minimize
# 
#     # Get asset names
#     assets = mean_returns.index if isinstance(mean_returns, pd.Series) else [f"Asset {i+1}" for i in range(len(mean_returns))]
#     num_assets = len(assets)
# 
#     # Generate random portfolios
#     results = np.zeros((4, num_portfolios))
#     weights_record = np.zeros((num_portfolios, num_assets))
# 
#     for i in range(num_portfolios):
#         weights = np.random.random(num_assets)
#         weights /= np.sum(weights)
#         weights_record[i] = weights
# 
#         # Calculate portfolio metrics
#         portfolio_ret = portfolio_return(weights, mean_returns)
#         portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
#         sharpe = (portfolio_ret - risk_free_rate) / portfolio_vol
#         results[0, i] = portfolio_ret
#         results[1, i] = portfolio_vol
#         results[2, i] = sharpe
#         results[3, i] = i
# 
#     # Optimize for maximum Sharpe ratio
#     constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
#     bounds = tuple((0, 1) for _ in range(num_assets))
#     initial_weights = np.array(num_assets * [1. / num_assets])
# 
#     max_sharpe = minimize(portfolio_performance,
#                           initial_weights,
#                           args=(mean_returns, cov_matrix, risk_free_rate),
#                           method='SLSQP',
#                           bounds=bounds,
#                           constraints=constraints)
# 
#     max_sharpe_weights = max_sharpe['x']
#     max_sharpe_ret = portfolio_return(max_sharpe_weights, mean_returns)
#     max_sharpe_vol = min_volatility(max_sharpe_weights, mean_returns, cov_matrix)
#     max_sharpe_sharpe = (max_sharpe_ret - risk_free_rate) / max_sharpe_vol
# 
#     # Optimize for minimum volatility
#     min_vol = minimize(min_volatility,
#                       initial_weights,
#                       args=(mean_returns, cov_matrix),
#                       method='SLSQP',
#                       bounds=bounds,
#                       constraints=constraints)
# 
#     min_vol_weights = min_vol['x']
#     min_vol_ret = portfolio_return(min_vol_weights, mean_returns)
#     min_vol_vol = min_volatility(min_vol_weights, mean_returns, cov_matrix)
#     min_vol_sharpe = (min_vol_ret - risk_free_rate) / min_vol_vol
# 
#     # Create 3D figure
#     fig = plt.figure(figsize=(12, 10))
#     ax = fig.add_subplot(111, projection='3d')
# 
#     # Plot random portfolios
#     scatter = ax.scatter(results[1, :], results[0, :], results[2, :],
#                          c=results[2, :], cmap='viridis',
#                          marker='o', s=10, alpha=0.3)
# 
#     # Plot maximum Sharpe ratio and minimum volatility portfolios
#     ax.scatter(max_sharpe_vol, max_sharpe_ret, max_sharpe_sharpe,
#                color='red', marker='*', s=200, label='Max Sharpe')
#     ax.scatter(min_vol_vol, min_vol_ret, min_vol_sharpe,
#                color='green', marker='o', s=200, label='Min Volatility')
# 
#     # Plot individual assets
#     for i, asset in enumerate(assets):
#         asset_return = mean_returns.iloc[i] if isinstance(mean_returns, pd.Series) else mean_returns[i]
#         asset_vol = np.sqrt(cov_matrix.iloc[i, i]) if isinstance(cov_matrix, pd.DataFrame) else np.sqrt(cov_matrix[i, i])
#         asset_sharpe = (asset_return - risk_free_rate) / asset_vol
# 
#         ax.scatter(asset_vol, asset_return, asset_sharpe,
#                    marker='D', s=100, label=asset)
# 
#     # Set labels and title
#     ax.set_xlabel('Волатильность')
#     ax.set_ylabel('Доходность')
#     ax.set_zlabel('Коэффициент Шарпа')
#     ax.set_title('Эффективный фронт в 3D', fontsize=14)
# 
#     # Add legend
#     ax.legend()
# 
#     # Set view angle
#     ax.view_init(30, 60)
# 
#     return fig
# 
# 
# def plot_asset_price_and_volume(price_data, volume_data=None, ticker=None, title=None):
#     """
#     Plot asset price and volume over time.
# 
#     Args:
#         price_data (pandas.Series): Asset price data
#         volume_data (pandas.Series, optional): Asset volume data
#         ticker (str, optional): Asset ticker
#         title (str, optional): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure and primary axis
#     fig = plt.figure(figsize=(14, 8))
# 
#     if volume_data is not None:
#         # Create subplot with shared x-axis
#         gs = fig.add_gridspec(2, 1, height_ratios=[3, 1], hspace=0.05)
#         ax1 = fig.add_subplot(gs[0])
#         ax2 = fig.add_subplot(gs[1], sharex=ax1)
#     else:
#         ax1 = fig.add_subplot(111)
# 
#     # Plot price data
#     ax1.plot(price_data.index, price_data, color='blue', linewidth=2)
# 
#     # Set labels and title for price plot
#     ax1.set_ylabel('Цена', fontsize=12)
#     ax1.grid(True, alpha=0.3)
# 
#     if volume_data is not None:
#         # Plot volume data as bar chart
#         ax2.bar(volume_data.index, volume_data, color='gray', alpha=0.5)
# 
#         # Set labels for volume plot
#         ax2.set_xlabel('Дата', fontsize=12)
#         ax2.set_ylabel('Объем', fontsize=12)
#         ax2.grid(True, alpha=0.3)
# 
#         # Format volume y-axis with K/M suffix
#         def volume_formatter(x, pos):
#             if x >= 1e6:
#                 return f'{x/1e6:.1f}M'
#             elif x >= 1e3:
#                 return f'{x/1e3:.0f}K'
#             else:
#                 return f'{x:.0f}'
# 
#         ax2.yaxis.set_major_formatter(FuncFormatter(volume_formatter))
#     else:
#         ax1.set_xlabel('Дата', fontsize=12)
# 
#     # Set overall title
#     if title:
#         fig.suptitle(title, fontsize=14)
#     elif ticker:
#         fig.suptitle(f"Динамика цены {ticker}", fontsize=14)
# 
#     plt.tight_layout()
#     if volume_data is not None:
#         plt.subplots_adjust(top=0.95)
# 
#     return fig
# 
# 
# def plot_comparative_performance(returns_data, benchmark_returns=None, initial_investment=1.0, title="Сравнительная доходность"):
#     """
#     Plot comparative performance of portfolio against a benchmark or other portfolios.
# 
#     Args:
#         returns_data (pandas.DataFrame/Series): Returns data
#         benchmark_returns (pandas.Series, optional): Benchmark returns data
#         initial_investment (float): Initial investment amount
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure
#     fig, ax = plt.subplots(figsize=(12, 6))
# 
#     # Convert Series to DataFrame if needed
#     if isinstance(returns_data, pd.Series):
#         returns_data = pd.DataFrame(returns_data, columns=['Portfolio'])
# 
#     # Calculate cumulative returns
#     cumulative_returns = (1 + returns_data).cumprod() * initial_investment
# 
#     # Plot portfolio returns
#     for column in cumulative_returns.columns:
#         ax.plot(cumulative_returns.index, cumulative_returns[column], label=column, linewidth=2)
# 
#     # Add benchmark if provided
#     if benchmark_returns is not None:
#         benchmark_cum_returns = (1 + benchmark_returns).cumprod() * initial_investment
#         ax.plot(benchmark_cum_returns.index, benchmark_cum_returns, label='Benchmark', color='black', linestyle='--', linewidth=2)
# 
#     # Set labels and title
#     ax.set_xlabel('Дата', fontsize=12)
#     ax.set_ylabel('Стоимость', fontsize=12)
#     ax.set_title(title, fontsize=14)
# 
#     # Format y-axis with currency
#     ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '${:,.0f}'.format(y)))
# 
#     # Add grid
#     ax.grid(True, alpha=0.3)
# 
#     # Add legend
#     ax.legend()
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_risk_contribution(weights, cov_matrix, labels=None, title="Вклад в риск портфеля"):
#     """
#     Plot risk contribution of each asset in the portfolio.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         labels (list, optional): Asset labels
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Get asset labels
#     if labels is None:
#         labels = [f"Asset {i+1}" for i in range(len(weights))]
# 
#     # Calculate portfolio volatility
#     portfolio_vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
# 
#     # Calculate marginal contribution to risk
#     mcr = np.dot(cov_matrix, weights) / portfolio_vol
# 
#     # Calculate risk contribution
#     risk_contrib = weights * mcr
# 
#     # Calculate percentage contribution
#     percent_contrib = risk_contrib / portfolio_vol
# 
#     # Create figure
#     fig, ax = plt.subplots(figsize=(10, 6))
# 
#     # Create stacked bar for risk contribution
#     ax.bar(["Портфель"], portfolio_vol, color='gray', alpha=0.3)
# 
#     # Create a bottom value to stack the bars
#     bottom = 0
# 
#     # Create color map
#     cmap = plt.cm.get_cmap('tab20', len(weights))
# 
#     # Add bars for each asset
#     for i, (asset, contrib) in enumerate(zip(labels, risk_contrib)):
#         ax.bar(["Портфель"], contrib, bottom=bottom, label=f"{asset} ({percent_contrib[i]:.1%})", color=cmap(i))
#         bottom += contrib
# 
#     # Set labels and title
#     ax.set_ylabel('Вклад в риск (волатильность)', fontsize=12)
#     ax.set_title(title, fontsize=14)
# 
#     # Add legend
#     ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_portfolio_metrics_dashboard(returns, benchmark_returns=None, risk_free_rate=0.02, window=252):
#     """
#     Create a comprehensive dashboard of portfolio performance metrics.
# 
#     Args:
#         returns (pandas.Series): Portfolio returns
#         benchmark_returns (pandas.Series, optional): Benchmark returns
#         risk_free_rate (float): Risk-free rate
#         window (int): Rolling window size for metrics
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Calculate cumulative returns
#     cum_returns = (1 + returns).cumprod()
# 
#     if benchmark_returns is not None:
#         bench_cum_returns = (1 + benchmark_returns).cumprod()
# 
#     # Calculate drawdown
#     running_max = cum_returns.cummax()
#     drawdown = (cum_returns / running_max) - 1
# 
#     # Calculate rolling metrics
#     rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)
#     daily_rf = ((1 + risk_free_rate) ** (1/252)) - 1
#     excess_returns = returns - daily_rf
#     rolling_sharpe = (excess_returns.rolling(window=window).mean() * 252) / rolling_vol
# 
#     if benchmark_returns is not None:
#         rolling_beta = returns.rolling(window=window).cov(benchmark_returns) / benchmark_returns.rolling(window=window).var()
#         tracking_error = (returns - benchmark_returns).rolling(window=window).std() * np.sqrt(252)
#         information_ratio = ((returns - benchmark_returns).rolling(window=window).mean() * 252) / tracking_error
# 
#     # Create dashboard figure
#     fig = plt.figure(figsize=(15, 12))
#     gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
# 
#     # Cumulative returns plot
#     ax1 = fig.add_subplot(gs[0, 0])
#     ax1.plot(cum_returns.index, cum_returns, label='Portfolio', color='blue', linewidth=2)
#     if benchmark_returns is not None:
#         ax1.plot(bench_cum_returns.index, bench_cum_returns, label='Benchmark', color='gray', linestyle='--', linewidth=2)
#     ax1.set_title('Кумулятивная доходность', fontsize=12)
#     ax1.set_ylabel('Рост')
#     ax1.grid(True, alpha=0.3)
#     ax1.legend()
# 
#     # Drawdown plot
#     ax2 = fig.add_subplot(gs[0, 1])
#     ax2.plot(drawdown.index, drawdown, color='red', linewidth=2)
#     ax2.fill_between(drawdown.index, 0, drawdown, color='red', alpha=0.3)
#     ax2.set_title('Просадка', fontsize=12)
#     ax2.set_ylabel('Просадка')
#     ax2.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
#     ax2.grid(True, alpha=0.3)
# 
#     # Rolling volatility plot
#     ax3 = fig.add_subplot(gs[1, 0])
#     ax3.plot(rolling_vol.index, rolling_vol, color='purple', linewidth=2)
#     ax3.set_title(f'Скользящая волатильность ({window} дней)', fontsize=12)
#     ax3.set_ylabel('Годовая волатильность')
#     ax3.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
#     ax3.grid(True, alpha=0.3)
# 
#     # Rolling Sharpe ratio plot
#     ax4 = fig.add_subplot(gs[1, 1])
#     ax4.plot(rolling_sharpe.index, rolling_sharpe, color='green', linewidth=2)
#     ax4.axhline(y=0, color='red', linestyle='--', alpha=0.3)
#     ax4.set_title(f'Скользящий коэффициент Шарпа ({window} дней)', fontsize=12)
#     ax4.set_ylabel('Коэффициент Шарпа')
#     ax4.grid(True, alpha=0.3)
# 
#     if benchmark_returns is not None:
#         # Rolling beta plot
#         ax5 = fig.add_subplot(gs[2, 0])
#         ax5.plot(rolling_beta.index, rolling_beta, color='orange', linewidth=2)
#         ax5.axhline(y=1, color='black', linestyle='--', alpha=0.3)
#         ax5.set_title(f'Скользящая бета ({window} дней)', fontsize=12)
#         ax5.set_ylabel('Бета')
#         ax5.grid(True, alpha=0.3)
# 
#         # Information ratio plot
#         ax6 = fig.add_subplot(gs[2, 1])
#         ax6.plot(information_ratio.index, information_ratio, color='brown', linewidth=2)
#         ax6.axhline(y=0, color='red', linestyle='--', alpha=0.3)
#         ax6.set_title(f'Скользящий коэффициент информации ({window} дней)', fontsize=12)
#         ax6.set_ylabel('Коэффициент информации')
#         ax6.grid(True, alpha=0.3)
#     else:
#         # Monthly returns heatmap if no benchmark
#         ax5 = fig.add_subplot(gs[2, :])
# 
#         # Create monthly returns
#         monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
#         monthly_pivot = monthly_returns.groupby([lambda x: x.year, lambda x: x.month]).first().unstack()
# 
#         # Create heatmap
#         sns.heatmap(
#             monthly_pivot,
#             annot=True,
#             fmt=".1%",
#             cmap="RdYlGn",
#             center=0,
#             linewidths=1,
#             cbar=True,
#             ax=ax5
#         )
# 
#         ax5.set_title('Месячная доходность', fontsize=12)
#         ax5.set_ylabel('Год')
#         ax5.set_xlabel('Месяц')
# 
#         # Custom month labels
#         month_labels = ['Янв', 'Фев', 'Мар', 'Апр', 'Май', 'Июн', 'Июл', 'Авг', 'Сен', 'Окт', 'Ноя', 'Дек']
#         if len(monthly_pivot.columns) == 12:
#             ax5.set_xticklabels(month_labels)
# 
#     # Set overall title
#     fig.suptitle('Комплексный анализ портфеля', fontsize=16, y=0.98)
# 
#     plt.tight_layout()
#     return fig
# 
# 
# def plot_results(weights, mean_returns, cov_matrix, risk_free_rate=0.02, labels=None):
#     """
#     Plot comprehensive portfolio analysis results.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         mean_returns (pandas.Series/numpy.array): Expected returns
#         cov_matrix (pandas.DataFrame/numpy.ndarray): Covariance matrix
#         risk_free_rate (float): Risk-free rate
#         labels (list, optional): Asset labels
# 
#     Returns:
#         list: List of generated figures
#     """
#     # Get asset labels
#     if labels is None:
#         labels = mean_returns.index if isinstance(mean_returns, pd.Series) else [f"Asset {i+1}" for i in range(len(weights))]
# 
#     # Calculate portfolio metrics
#     portfolio_return = np.sum(weights * mean_returns)
#     portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
#     sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_volatility
# 
#     # Create figures
#     figures = []
# 
#     # Portfolio weights pie chart
#     fig1 = plot_portfolio_weights(weights, labels, title="Распределение портфеля")
#     figures.append(fig1)
# 
#     # Risk contribution chart
#     fig2 = plot_risk_contribution(weights, cov_matrix, labels, title="Вклад в риск портфеля")
#     figures.append(fig2)
# 
#     # Asset contribution chart
#     fig3 = plot_asset_contribution(weights, mean_returns, cov_matrix, title="Вклад активов в портфель")
#     figures.append(fig3)
# 
#     # Efficient frontier plot
#     fig4 = plot_efficient_frontier(mean_returns, cov_matrix, risk_free_rate=risk_free_rate,
#                                  optimized_weights=weights, show_assets=True)
#     figures.append(fig4)
# 
#     # Create summary text for metrics
#     print("\nПортфельные метрики:")
#     print(f"Ожидаемая доходность: {portfolio_return:.2%}")
#     print(f"Ожидаемая волатильность: {portfolio_volatility:.2%}")
#     print(f"Коэффициент Шарпа: {sharpe_ratio:.2f}")
# 
#     return figures
# 
# 
# # Test function
# def test_visualization_module():
#     """Test function for the visualization module."""
#     print("\n=== Тестирование модуля визуализации ===")
# 
#     # Generate sample data
#     np.random.seed(42)
# 
#     # Sample price data
#     dates = pd.date_range(start='2020-01-01', periods=500, freq='B')
#     assets = ['AAPL', 'MSFT', 'AMZN', 'GOOGL']
# 
#     prices = pd.DataFrame(index=dates)
#     for asset in assets:
#         prices[asset] = 100 * (1 + np.random.normal(0.0001, 0.01, len(dates))).cumprod()
# 
#     # Calculate returns
#     returns = prices.pct_change().dropna()
# 
#     # Test correlation matrix
#     print("\nТест корреляционной матрицы:")
#     fig1 = plot_correlation_matrix(returns)
#     plt.close(fig1)
#     print("Корреляционная матрица успешно создана")
# 
#     # Test efficient frontier
#     print("\nТест эффективного фронта:")
#     mean_returns = returns.mean() * 252
#     cov_matrix = returns.cov() * 252
#     fig2 = plot_efficient_frontier(mean_returns, cov_matrix)
#     plt.close(fig2)
#     print("Эффективный фронт успешно создан")
# 
#     # Test portfolio weights
#     print("\nТест диаграммы весов портфеля:")
#     weights = np.array([0.25, 0.35, 0.15, 0.25])
#     fig3 = plot_portfolio_weights(weights, assets)
#     plt.close(fig3)
#     print("Диаграмма весов портфеля успешно создана")
# 
#     # Test cumulative returns
#     print("\nТест графика кумулятивной доходности:")
#     fig4 = plot_cumulative_returns(returns)
#     plt.close(fig4)
#     print("График кумулятивной доходности успешно создан")
# 
#     # Test drawdown
#     print("\nТест графика просадки:")
#     fig5 = plot_drawdown(returns)
#     plt.close(fig5)
#     print("График просадки успешно создан")
# 
#     # Test monthly returns heatmap
#     print("\nТест тепловой карты месячной доходности:")
#     fig6 = plot_monthly_returns_heatmap(returns['AAPL'])
#     plt.close(fig6)
#     print("Тепловая карта месячной доходности успешно создана")
# 
#     print("\n=== Тестирование завершено успешно ===")
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_visualization_module()

"""## module_data.py

Задача: Работа с данными, включая загрузку финансовых данных с внешних источников (например, Yahoo Finance).

Основная функция:

Загрузка данных по тикерам с Yahoo Finance через yfinance.

Предоставление данных в виде датафрейма с закрытыми ценами.

Методы в модуле:

download_data(tickers, start_date, end_date): Загружает данные для указанных тикеров за заданный период времени.

Пример использования: Этот модуль будет использоваться для загрузки исторических данных для расчетов доходности и оптимизации.

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_data.py
# """
# Module for loading and managing financial data from various sources.
# This module provides functions to fetch stock data from multiple APIs and
# handles data cleaning and preprocessing.
# """
# !pip install yfinance
# !pip install newsapi-python
# !pip install pytrends
# 
# import numpy as np
# import pandas as pd
# import yfinance as yf
# import requests
# import time
# import concurrent.futures
# from datetime import datetime, timedelta
# import json
# import os
# import logging
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# # Global variables for API keys
# ALPHA_VANTAGE_API_KEY = None
# FRED_API_KEY = None
# FINNHUB_API_KEY = None
# NEWS_API_KEY = None
# 
# 
# def request_api_keys():
#     """
#     Request API keys from the user and save them for future use.
# 
#     Returns:
#         dict: Dictionary containing all API keys
#     """
#     global ALPHA_VANTAGE_API_KEY, FRED_API_KEY, FINNHUB_API_KEY, NEWS_API_KEY
# 
#     # Check if keys are already loaded
#     if all([ALPHA_VANTAGE_API_KEY, FRED_API_KEY, FINNHUB_API_KEY, NEWS_API_KEY]):
#         return {
#             'alpha_vantage': ALPHA_VANTAGE_API_KEY,
#             'fred': FRED_API_KEY,
#             'finnhub': FINNHUB_API_KEY,
#             'news_api': NEWS_API_KEY
#         }
# 
#     # Check if keys are saved in a file
#     if os.path.exists('api_keys.json'):
#         try:
#             with open('api_keys.json', 'r') as f:
#                 keys = json.load(f)
#                 ALPHA_VANTAGE_API_KEY = keys.get('alpha_vantage')
#                 FRED_API_KEY = keys.get('fred')
#                 FINNHUB_API_KEY = keys.get('finnhub')
#                 NEWS_API_KEY = keys.get('news_api')
# 
#                 print("API keys loaded from saved file.")
#                 return keys
#         except Exception as e:
#             logger.error(f"Error loading API keys from file: {str(e)}")
# 
#     # Request new keys from user
#     print("\n=== API Keys Configuration ===")
#     print("You'll need to provide API keys for various data sources.")
#     print("Some sources are optional but will enhance data availability.")
#     print("You can get these keys for free by registering at the respective websites.")
# 
#     # Alpha Vantage (required)
#     print("\n1. Alpha Vantage API (https://www.alphavantage.co/support/#api-key)")
#     ALPHA_VANTAGE_API_KEY = input("Enter your Alpha Vantage API key (required): ")
#     while not ALPHA_VANTAGE_API_KEY:
#         ALPHA_VANTAGE_API_KEY = input("Alpha Vantage API key is required. Please enter a valid key: ")
# 
#     # FRED (optional)
#     print("\n2. FRED API (https://fred.stlouisfed.org/docs/api/api_key.html)")
#     FRED_API_KEY = input("Enter your FRED API key (optional, press Enter to skip): ")
# 
#     # Finnhub (optional)
#     print("\n3. Finnhub API (https://finnhub.io/)")
#     FINNHUB_API_KEY = input("Enter your Finnhub API key (optional, press Enter to skip): ")
# 
#     # News API (optional)
#     print("\n4. News API (https://newsapi.org/)")
#     NEWS_API_KEY = input("Enter your News API key (optional, press Enter to skip): ")
# 
#     # Save keys to file
#     keys = {
#         'alpha_vantage': ALPHA_VANTAGE_API_KEY,
#         'fred': FRED_API_KEY,
#         'finnhub': FINNHUB_API_KEY,
#         'news_api': NEWS_API_KEY
#     }
# 
#     try:
#         with open('api_keys.json', 'w') as f:
#             json.dump(keys, f)
#         print("\nAPI keys saved successfully for future use.")
#     except Exception as e:
#         logger.error(f"Error saving API keys to file: {str(e)}")
# 
#     return keys
# 
# 
# # ===== DATA LOADING FUNCTIONS =====
# 
# def load_stock_data_from_yahoo(tickers, start_date=None, end_date=None, thread_count=4):
#     """
#     Load stock closing price data from Yahoo Finance with parallel processing for faster loading.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
#         thread_count (int): Number of threads for parallel downloads
# 
#     Returns:
#         pandas.DataFrame: DataFrame with adjusted closing prices for each ticker
#     """
#     if start_date is None:
#         start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')
#     if end_date is None:
#         end_date = datetime.now().strftime('%Y-%m-%d')
# 
#     logger.info(f"Загрузка данных из Yahoo Finance для {len(tickers)} тикеров")
# 
#     # Function to download data for a single ticker
#     def download_ticker(ticker):
#         try:
#             data = yf.download(ticker, start=start_date, end=end_date, progress=False)
#             if data.empty:
#                 logger.warning(f"Данные для {ticker} не найдены")
#                 return None
#             return pd.DataFrame({ticker: data['Adj Close']})
#         except Exception as e:
#             logger.error(f"Ошибка при загрузке {ticker}: {str(e)}")
#             return None
# 
#     # Use ThreadPoolExecutor for parallel downloads
#     all_data = []
#     with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:
#         results = executor.map(download_ticker, tickers)
#         for result in results:
#             if result is not None:
#                 all_data.append(result)
# 
#     if not all_data:
#         logger.error("Не удалось загрузить данные из Yahoo Finance")
#         return pd.DataFrame()
# 
#     # Combine all dataframes
#     combined_data = pd.concat(all_data, axis=1)
# 
#     # Handle missing values
#     if combined_data.isnull().any().any():
#         logger.info("Заполнение пропущенных значений методом forward fill")
#         combined_data = combined_data.fillna(method='ffill').fillna(method='bfill')
# 
#     return combined_data
# 
# 
# def get_stock_data_from_alpha_vantage(ticker):
#     """
#     Fetches stock data from Alpha Vantage API.
# 
#     Args:
#         ticker (str): Stock ticker symbol
# 
#     Returns:
#         dict: JSON data from Alpha Vantage or None if error
#     """
#     global ALPHA_VANTAGE_API_KEY
# 
#     if not ALPHA_VANTAGE_API_KEY:
#         request_api_keys()
# 
#     url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}"
# 
#     try:
#         response = requests.get(url)
#         data = response.json()
# 
#         if 'Time Series (Daily)' in data:
#             logger.info(f"Успешно получены данные для {ticker} из Alpha Vantage")
#             return data['Time Series (Daily)']
#         elif 'Error Message' in data:
#             logger.error(f"Ошибка Alpha Vantage для {ticker}: {data['Error Message']}")
#         elif 'Note' in data:
#             logger.warning(f"Достигнут лимит запросов Alpha Vantage: {data['Note']}")
#         return None
#     except Exception as e:
#         logger.error(f"Ошибка при получении данных для {ticker} из Alpha Vantage: {str(e)}")
#         return None
# 
# 
# def alpha_vantage_to_dataframe(ticker, alpha_data):
#     """
#     Converts Alpha Vantage data to pandas DataFrame.
# 
#     Args:
#         ticker (str): Stock ticker symbol
#         alpha_data (dict): Alpha Vantage data
# 
#     Returns:
#         pandas.DataFrame: DataFrame with closing prices
#     """
#     if not alpha_data:
#         return None
# 
#     dates = []
#     prices = []
# 
#     for date, values in alpha_data.items():
#         dates.append(date)
#         prices.append(float(values['4. close']))
# 
#     df = pd.DataFrame({ticker: prices}, index=pd.DatetimeIndex(dates))
#     return df.sort_index()
# 
# 
# def load_stock_data_from_finnhub(ticker, start_date=None, end_date=None):
#     """
#     Fetches stock data from Finnhub API.
# 
#     Args:
#         ticker (str): Stock ticker symbol
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with closing prices or None if error
#     """
#     global FINNHUB_API_KEY
# 
#     if not FINNHUB_API_KEY:
#         request_api_keys()
# 
#     if FINNHUB_API_KEY == "":
#         logger.warning("Finnhub API ключ не предоставлен, пропуск запроса")
#         return None
# 
#     if start_date is None:
#         start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')
#     if end_date is None:
#         end_date = datetime.now().strftime('%Y-%m-%d')
# 
#     # Convert dates to UNIX timestamps
#     start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())
#     end_timestamp = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp())
# 
#     url = f"https://finnhub.io/api/v1/stock/candle?symbol={ticker}&resolution=D&from={start_timestamp}&to={end_timestamp}&token={FINNHUB_API_KEY}"
# 
#     try:
#         response = requests.get(url)
#         data = response.json()
# 
#         if data.get('s') == 'ok':
#             df = pd.DataFrame({
#                 'timestamp': data['t'],
#                 'close': data['c']
#             })
#             df['date'] = pd.to_datetime(df['timestamp'], unit='s')
#             df = df.set_index('date')
#             return pd.DataFrame({ticker: df['close']})
#         else:
#             logger.error(f"Ошибка Finnhub для {ticker}: {data.get('s')}")
#             return None
#     except Exception as e:
#         logger.error(f"Ошибка при получении данных для {ticker} из Finnhub: {str(e)}")
#         return None
# 
# 
# def load_stock_data(tickers, start_date=None, end_date=None):
#     """
#     Comprehensive function to load stock data from multiple sources.
#     Tries Yahoo Finance first, then Alpha Vantage, then Finnhub as fallbacks.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with closing prices for each ticker
#     """
#     # Clean and standardize tickers
#     tickers = [ticker.strip().upper() for ticker in tickers]
# 
#     # Get data from Yahoo Finance first
#     data = load_stock_data_from_yahoo(tickers, start_date, end_date)
# 
#     # Check for missing tickers
#     loaded_tickers = data.columns.tolist()
#     missing_tickers = [ticker for ticker in tickers if ticker not in loaded_tickers]
# 
#     if missing_tickers:
#         logger.info(f"Попытка получить недостающие тикеры из Alpha Vantage: {', '.join(missing_tickers)}")
# 
#         for ticker in missing_tickers:
#             # Try Alpha Vantage
#             alpha_data = get_stock_data_from_alpha_vantage(ticker)
#             if alpha_data:
#                 alpha_df = alpha_vantage_to_dataframe(ticker, alpha_data)
#                 if alpha_df is not None:
#                     # Make sure the date ranges match
#                     if start_date:
#                         alpha_df = alpha_df[alpha_df.index >= start_date]
#                     if end_date:
#                         alpha_df = alpha_df[alpha_df.index <= end_date]
# 
#                     data = pd.concat([data, alpha_df], axis=1)
#                     loaded_tickers.append(ticker)
# 
#             # If Alpha Vantage failed, try Finnhub
#             elif FINNHUB_API_KEY and FINNHUB_API_KEY != "":
#                 logger.info(f"Попытка получить данные из Finnhub для {ticker}")
#                 finnhub_df = load_stock_data_from_finnhub(ticker, start_date, end_date)
#                 if finnhub_df is not None:
#                     data = pd.concat([data, finnhub_df], axis=1)
#                     loaded_tickers.append(ticker)
# 
#             # Respect API rate limits
#             time.sleep(12)
# 
#     # Final check for missing tickers
#     still_missing = [ticker for ticker in tickers if ticker not in loaded_tickers]
#     if still_missing:
#         logger.warning(f"Не удалось загрузить данные для следующих тикеров: {', '.join(still_missing)}")
# 
#     # Reindex to ensure complete date range
#     if not data.empty:
#         full_date_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='B')
#         data = data.reindex(full_date_range)
#         data = data.fillna(method='ffill').fillna(method='bfill')
# 
#     return data
# 
# 
# # ===== MACROECONOMIC DATA FUNCTIONS =====
# 
# def load_fred_data(series_ids, start_date=None, end_date=None):
#     """
#     Load macroeconomic data from Federal Reserve Economic Data (FRED).
# 
#     Args:
#         series_ids (dict): Dictionary mapping FRED series IDs to their descriptions
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with macroeconomic indicators
#     """
#     global FRED_API_KEY
# 
#     if not FRED_API_KEY:
#         request_api_keys()
# 
#     if not FRED_API_KEY or FRED_API_KEY == "":
#         logger.error("FRED API ключ не предоставлен, пропуск загрузки макроэкономических данных")
#         return pd.DataFrame()
# 
#     if start_date is None:
#         start_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')
#     if end_date is None:
#         end_date = datetime.now().strftime('%Y-%m-%d')
# 
#     data = {}
# 
#     for series_id, description in series_ids.items():
#         url = f"https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={FRED_API_KEY}&file_type=json&observation_start={start_date}&observation_end={end_date}"
# 
#         try:
#             response = requests.get(url)
#             result = response.json()
# 
#             if 'observations' in result:
#                 observations = result['observations']
#                 dates = [obs['date'] for obs in observations]
#                 values = [float(obs['value']) if obs['value'] != '.' else np.nan for obs in observations]
# 
#                 series_df = pd.DataFrame({description: values}, index=pd.DatetimeIndex(dates))
#                 data[description] = series_df
#                 logger.info(f"Успешно загружены данные FRED для {series_id} ({description})")
#             else:
#                 logger.error(f"Ошибка загрузки данных FRED для {series_id}: {result.get('error_message', 'Неизвестная ошибка')}")
# 
#         except Exception as e:
#             logger.error(f"Ошибка загрузки данных FRED для {series_id}: {str(e)}")
# 
#         # Respect API rate limits
#         time.sleep(1)
# 
#     if not data:
#         return pd.DataFrame()
# 
#     # Combine all series into one dataframe
#     combined_df = pd.concat(data.values(), axis=1)
# 
#     # Resample to business days and forward fill missing values
#     business_days = pd.date_range(start=combined_df.index.min(), end=combined_df.index.max(), freq='B')
#     combined_df = combined_df.reindex(business_days)
#     combined_df = combined_df.fillna(method='ffill')
# 
#     return combined_df
# 
# 
# def get_standard_macro_indicators():
#     """
#     Returns a dictionary of standard macroeconomic indicators from FRED.
# 
#     Returns:
#         dict: Dictionary mapping FRED series IDs to their descriptions
#     """
#     return {
#         "FEDFUNDS": "Ставка ФРС",                  # Federal Funds Effective Rate
#         "CPIAUCSL": "Индекс потребительских цен",  # Consumer Price Index for All Urban Consumers
#         "UNRATE": "Уровень безработицы",           # Unemployment Rate
#         "GDP": "ВВП",                              # Gross Domestic Product
#         "INDPRO": "Промышленное производство",     # Industrial Production Index
#         "HOUST": "Жилищное строительство",         # Housing Starts
#         "RETAILSMNSA": "Розничные продажи",        # Retail Sales
#         "M2SL": "Денежная масса M2",               # M2 Money Stock
#         "T10Y2Y": "Кривая доходности",             # 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity
#         "DTWEXBGS": "Индекс доллара"               # Trade Weighted U.S. Dollar Index: Broad
#     }
# 
# 
# def load_market_indices(start_date=None, end_date=None):
#     """
#     Load major market indices for comparison and market analysis.
# 
#     Args:
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with market indices
#     """
#     indices = [
#         "^GSPC",  # S&P 500
#         "^DJI",   # Dow Jones Industrial Average
#         "^IXIC",  # NASDAQ Composite
#         "^RUT",   # Russell 2000
#         "^VIX",   # CBOE Volatility Index
#         "^FTSE",  # FTSE 100
#         "^N225",  # Nikkei 225
#         "^HSI"    # Hang Seng Index
#     ]
# 
#     index_names = {
#         "^GSPC": "S&P_500",
#         "^DJI": "Dow_Jones",
#         "^IXIC": "NASDAQ",
#         "^RUT": "Russell_2000",
#         "^VIX": "VIX",
#         "^FTSE": "FTSE_100",
#         "^N225": "Nikkei_225",
#         "^HSI": "Hang_Seng"
#     }
# 
#     data = load_stock_data_from_yahoo(indices, start_date, end_date)
# 
#     # Rename columns to more descriptive names
#     if not data.empty:
#         data = data.rename(columns=index_names)
# 
#     return data
# 
# 
# def get_sector_performance(start_date=None, end_date=None):
#     """
#     Load sector ETFs to analyze sector performance.
# 
#     Args:
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with sector ETF prices
#     """
#     sector_etfs = {
#         "XLF": "Финансы",
#         "XLK": "Технологии",
#         "XLV": "Здравоохранение",
#         "XLE": "Энергетика",
#         "XLU": "Коммунальные услуги",
#         "XLI": "Промышленность",
#         "XLY": "Потребительские товары",
#         "XLP": "Товары первой необходимости",
#         "XLB": "Материалы",
#         "XLRE": "Недвижимость",
#         "XLC": "Коммуникационные услуги"
#     }
# 
#     etf_list = list(sector_etfs.keys())
#     data = load_stock_data_from_yahoo(etf_list, start_date, end_date)
# 
#     # Rename columns to sector names
#     if not data.empty:
#         data = data.rename(columns=sector_etfs)
# 
#     return data
# 
# 
# def get_sector_for_ticker(ticker):
#     """
#     Gets the sector classification for a given ticker using Yahoo Finance.
# 
#     Args:
#         ticker (str): Stock ticker symbol
# 
#     Returns:
#         str: Sector name or None if not found
#     """
#     try:
#         stock = yf.Ticker(ticker)
#         info = stock.info
# 
#         if 'sector' in info:
#             return info['sector']
#         else:
#             logger.warning(f"Информация о секторе для {ticker} не найдена")
#             return None
#     except Exception as e:
#         logger.error(f"Ошибка при получении сектора для {ticker}: {str(e)}")
#         return None
# 
# 
# def classify_tickers_by_sector(tickers):
#     """
#     Classifies a list of tickers by their sectors.
# 
#     Args:
#         tickers (list): List of stock ticker symbols
# 
#     Returns:
#         dict: Dictionary mapping sectors to lists of tickers
#     """
#     sectors = {}
#     unknown_sector = []
# 
#     for ticker in tickers:
#         sector = get_sector_for_ticker(ticker)
# 
#         if sector:
#             if sector not in sectors:
#                 sectors[sector] = []
#             sectors[sector].append(ticker)
#         else:
#             unknown_sector.append(ticker)
# 
#         # Avoid hitting rate limits
#         time.sleep(1)
# 
#     if unknown_sector:
#         sectors['Неизвестно'] = unknown_sector
# 
#     return sectors
# 
# 
# # ===== FOREX AND COMMODITIES DATA =====
# 
# def load_forex_data(pairs, start_date=None, end_date=None):
#     """
#     Load forex exchange rate data using Yahoo Finance.
# 
#     Args:
#         pairs (list): List of forex pairs (e.g., 'EURUSD=X')
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with forex rates
#     """
#     # Format pairs for Yahoo Finance if needed
#     formatted_pairs = []
#     for pair in pairs:
#         if not pair.endswith('=X'):
#             formatted_pairs.append(f"{pair}=X")
#         else:
#             formatted_pairs.append(pair)
# 
#     return load_stock_data_from_yahoo(formatted_pairs, start_date, end_date)
# 
# 
# def load_commodity_data(start_date=None, end_date=None):
#     """
#     Load data for major commodities.
# 
#     Args:
#         start_date (str): Start date in format 'YYYY-MM-DD'
#         end_date (str): End date in format 'YYYY-MM-DD'
# 
#     Returns:
#         pandas.DataFrame: DataFrame with commodity prices
#     """
#     commodities = {
#         "GC=F": "Золото",
#         "SI=F": "Серебро",
#         "CL=F": "Нефть",
#         "NG=F": "Природный газ",
#         "ZW=F": "Пшеница",
#         "ZC=F": "Кукуруза",
#         "ZS=F": "Соя",
#         "HG=F": "Медь",
#         "CT=F": "Хлопок"
#     }
# 
#     data = load_stock_data_from_yahoo(list(commodities.keys()), start_date, end_date)
# 
#     # Rename columns to commodity names
#     if not data.empty:
#         data = data.rename(columns=commodities)
# 
#     return data
# 
# 
# # ===== NEWS API FUNCTIONS =====
# 
# def get_company_news(ticker, days_back=7):
#     """
#     Get news articles related to a company using News API.
# 
#     Args:
#         ticker (str): Stock ticker symbol
#         days_back (int): Number of days back to fetch news
# 
#     Returns:
#         list: List of news articles
#     """
#     global NEWS_API_KEY
# 
#     if not NEWS_API_KEY:
#         request_api_keys()
# 
#     if not NEWS_API_KEY or NEWS_API_KEY == "":
#         logger.error("News API ключ не предоставлен, пропуск запроса новостей")
#         return []
# 
#     try:
#         # Get company name from Yahoo Finance
#         stock = yf.Ticker(ticker)
#         info = stock.info
#         company_name = info.get('shortName', ticker)
# 
#         # Calculate date range
#         end_date = datetime.now()
#         start_date = end_date - timedelta(days=days_back)
# 
#         # Format dates for API
#         from_date = start_date.strftime('%Y-%m-%d')
#         to_date = end_date.strftime('%Y-%m-%d')
# 
#         # Query News API
#         url = f"https://newsapi.org/v2/everything?q={company_name} OR {ticker}&language=en&from={from_date}&to={to_date}&sortBy=relevancy&apiKey={NEWS_API_KEY}"
# 
#         response = requests.get(url)
#         data = response.json()
# 
#         if data.get('status') == 'ok':
#             articles = data.get('articles', [])
#             logger.info(f"Найдено {len(articles)} статей для {ticker}")
#             return articles
#         else:
#             logger.error(f"Ошибка News API: {data.get('message', 'Неизвестная ошибка')}")
#             return []
# 
#     except Exception as e:
#         logger.error(f"Ошибка при получении новостей для {ticker}: {str(e)}")
#         return []
# 
# 
# def get_market_news(days_back=3):
#     """
#     Get general market news using News API.
# 
#     Args:
#         days_back (int): Number of days back to fetch news
# 
#     Returns:
#         list: List of news articles
#     """
#     global NEWS_API_KEY
# 
#     if not NEWS_API_KEY:
#         request_api_keys()
# 
#     if not NEWS_API_KEY or NEWS_API_KEY == "":
#         logger.error("News API ключ не предоставлен, пропуск запроса новостей")
#         return []
# 
#     try:
#         # Calculate date range
#         end_date = datetime.now()
#         start_date = end_date - timedelta(days=days_back)
# 
#         # Format dates for API
#         from_date = start_date.strftime('%Y-%m-%d')
#         to_date = end_date.strftime('%Y-%m-%d')
# 
#         # Query News API
#         url = f"https://newsapi.org/v2/everything?q=stock market OR finance OR economy&language=en&from={from_date}&to={to_date}&sortBy=relevancy&apiKey={NEWS_API_KEY}"
# 
#         response = requests.get(url)
#         data = response.json()
# 
#         if data.get('status') == 'ok':
#             articles = data.get('articles', [])
#             logger.info(f"Найдено {len(articles)} статей о рынке")
#             return articles
#         else:
#             logger.error(f"Ошибка News API: {data.get('message', 'Неизвестная ошибка')}")
#             return []
# 
#     except Exception as e:
#         logger.error(f"Ошибка при получении новостей о рынке: {str(e)}")
#         return []
# 
# 
# # ===== DATA PREPROCESSING FUNCTIONS =====
# 
# def calculate_returns(data, periods=1):
#     """
#     Calculate returns over the specified number of periods.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data
#         periods (int): Number of periods for return calculation
# 
#     Returns:
#         pandas.DataFrame: DataFrame with returns
#     """
#     return data.pct_change(periods).dropna()
# 
# 
# def fill_missing_values(data, method='ffill'):
#     """
#     Fill missing values in the dataset.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data
#         method (str): Method for filling missing values ('ffill', 'bfill', 'linear', 'zero')
# 
#     Returns:
#         pandas.DataFrame: DataFrame with filled values
#     """
#     if method == 'ffill':
#         return data.fillna(method='ffill').fillna(method='bfill')
#     elif method == 'bfill':
#         return data.fillna(method='bfill').fillna(method='ffill')
#     elif method == 'linear':
#         return data.interpolate(method='linear')
#     elif method == 'zero':
#         return data.fillna(0)
#     else:
#         logger.warning(f"Неизвестный метод заполнения: {method}, используется forward fill")
#         return data.fillna(method='ffill').fillna(method='bfill')
# 
# 
# def align_datasets(stock_data, macro_data):
#     """
#     Align stock data and macroeconomic data to the same date range and frequency.
# 
#     Args:
#         stock_data (pandas.DataFrame): DataFrame with stock prices
#         macro_data (pandas.DataFrame): DataFrame with macroeconomic indicators
# 
#     Returns:
#         tuple: (aligned_stock_data, aligned_macro_data)
#     """
#     # Find common date range
#     start_date = max(stock_data.index.min(), macro_data.index.min())
#     end_date = min(stock_data.index.max(), macro_data.index.max())
# 
#     # Filter both datasets to common date range
#     stock_data_aligned = stock_data.loc[start_date:end_date]
#     macro_data_aligned = macro_data.loc[start_date:end_date]
# 
#     # Ensure both datasets have the same index
#     common_index = stock_data_aligned.index.intersection(macro_data_aligned.index)
# 
#     return stock_data_aligned.loc[common_index], macro_data_aligned.loc[common_index]
# 
# 
# def detect_outliers(data, method='zscore', threshold=3.0):
#     """
#     Detect outliers in financial data.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data or returns
#         method (str): Method for outlier detection ('zscore', 'iqr')
#         threshold (float): Threshold for outlier detection
# 
#     Returns:
#         pandas.DataFrame: Boolean DataFrame indicating outliers
#     """
#     if method == 'zscore':
#         # Z-score method
#         mean = data.mean()
#         std = data.std()
#         z_scores = (data - mean) / std
#         return abs(z_scores) > threshold
# 
#     elif method == 'iqr':
#         # Interquartile Range method
#         q1 = data.quantile(0.25)
#         q3 = data.quantile(0.75)
#         iqr = q3 - q1
#         lower_bound = q1 - threshold * iqr
#         upper_bound = q3 + threshold * iqr
# 
#         outliers = pd.DataFrame(False, index=data.index, columns=data.columns)
#         for col in data.columns:
#             outliers[col] = (data[col] < lower_bound[col]) | (data[col] > upper_bound[col])
# 
#         return outliers
# 
#     else:
#         logger.warning(f"Неизвестный метод обнаружения выбросов: {method}, используется Z-score")
#         mean = data.mean()
#         std = data.std()
#         z_scores = (data - mean) / std
#         return abs(z_scores) > threshold
# 
# 
# def handle_outliers(data, outliers, method='winsorize'):
#     """
#     Handle outliers in financial data.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data or returns
#         outliers (pandas.DataFrame): Boolean DataFrame indicating outliers
#         method (str): Method for handling outliers ('winsorize', 'remove', 'replace_mean', 'replace_median')
# 
#     Returns:
#         pandas.DataFrame: DataFrame with handled outliers
#     """
#     result = data.copy()
# 
#     if method == 'winsorize':
#         # Replace outliers with the nearest non-outlier value
#         for col in data.columns:
#             col_outliers = outliers[col]
#             if col_outliers.any():
#                 q1 = data[col].quantile(0.25)
#                 q3 = data[col].quantile(0.75)
#                 iqr = q3 - q1
#                 lower_bound = q1 - 1.5 * iqr
#                 upper_bound = q3 + 1.5 * iqr
# 
#                 result.loc[col_outliers & (data[col] < lower_bound), col] = lower_bound
#                 result.loc[col_outliers & (data[col] > upper_bound), col] = upper_bound
# 
#     elif method == 'remove':
#         # Remove rows with outliers
#         mask = outliers.any(axis=1)
#         result = result.loc[~mask]
# 
#     elif method == 'replace_mean':
#         # Replace outliers with mean
#         for col in data.columns:
#             col_outliers = outliers[col]
#             if col_outliers.any():
#                 col_mean = data.loc[~col_outliers, col].mean()
#                 result.loc[col_outliers, col] = col_mean
# 
#     elif method == 'replace_median':
#         # Replace outliers with median
#         for col in data.columns:
#             col_outliers = outliers[col]
#             if col_outliers.any():
#                 col_median = data.loc[~col_outliers, col].median()
#                 result.loc[col_outliers, col] = col_median
# 
#     else:
#         logger.warning(f"Неизвестный метод обработки выбросов: {method}, выбросы не обработаны")
# 
#     return result
# 
# 
# def normalize_data(data, method='zscore'):
#     """
#     Normalize financial data.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data or returns
#         method (str): Method for normalization ('zscore', 'minmax', 'robust')
# 
#     Returns:
#         pandas.DataFrame: Normalized DataFrame
#     """
#     if method == 'zscore':
#         # Z-score normalization
#         mean = data.mean()
#         std = data.std()
#         return (data - mean) / std
# 
#     elif method == 'minmax':
#         # Min-max normalization to [0, 1]
#         min_vals = data.min()
#         max_vals = data.max()
#         return (data - min_vals) / (max_vals - min_vals)
# 
#     elif method == 'robust':
#         # Robust normalization using median and IQR
#         median = data.median()
#         q1 = data.quantile(0.25)
#         q3 = data.quantile(0.75)
#         iqr = q3 - q1
#         return (data - median) / iqr
# 
#     else:
#         logger.warning(f"Неизвестный метод нормализации: {method}, используется Z-score")
#         mean = data.mean()
#         std = data.std()
#         return (data - mean) / std
# 
# 
# # ===== MAIN FUNCTION FOR TESTING =====
# 
# def test_data_loading():
#     """Test function to verify data loading capabilities."""
#     # Initialize API keys
#     request_api_keys()
# 
#     # Test ticker data loading
#     print("\n=== Тест загрузки данных акций ===")
#     tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'FB']
# 
#     print("Загрузка данных из Yahoo Finance...")
#     yahoo_data = load_stock_data_from_yahoo(tickers)
# 
#     if not yahoo_data.empty:
#         print(f"Размер данных из Yahoo: {yahoo_data.shape}")
#         print("Первые 5 строк данных из Yahoo:")
#         print(yahoo_data.head())
#     else:
#         print("Не удалось загрузить данные из Yahoo Finance")
# 
#     # Test macroeconomic data
#     print("\n=== Тест загрузки макроэкономических данных ===")
#     macro_indicators = get_standard_macro_indicators()
#     fred_data = load_fred_data(macro_indicators)
# 
#     if not fred_data.empty:
#         print(f"Размер данных из FRED: {fred_data.shape}")
#         print("Первые 5 строк данных из FRED:")
#         print(fred_data.head())
#     else:
#         print("Не удалось загрузить макроэкономические данные")
# 
#     # Test market indices
#     print("\n=== Тест загрузки рыночных индексов ===")
#     indices_data = load_market_indices()
# 
#     if not indices_data.empty:
#         print(f"Размер данных индексов: {indices_data.shape}")
#         print("Первые 5 строк данных индексов:")
#         print(indices_data.head())
#     else:
#         print("Не удалось загрузить данные рыночных индексов")
# 
#     # Test sector performance
#     print("\n=== Тест загрузки данных по секторам ===")
#     sector_data = get_sector_performance()
# 
#     if not sector_data.empty:
#         print(f"Размер данных по секторам: {sector_data.shape}")
#         print("Первые 5 строк данных по секторам:")
#         print(sector_data.head())
#     else:
#         print("Не удалось загрузить данные по секторам")
# 
#     # Test news API
#     print("\n=== Тест загрузки новостей ===")
#     if NEWS_API_KEY and NEWS_API_KEY != "":
#         company_news = get_company_news('AAPL', days_back=3)
#         if company_news:
#             print(f"Найдено {len(company_news)} новостей о компании Apple")
#             print("Первая новость:")
#             print(f"Заголовок: {company_news[0]['title']}")
#             print(f"Дата: {company_news[0]['publishedAt']}")
#             print(f"Источник: {company_news[0]['source']['name']}")
#         else:
#             print("Не удалось загрузить новости о компании")
#     else:
#         print("Ключ News API не предоставлен, пропуск теста новостей")
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_data_loading()

"""## module_date"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_date.py
# """
# Module for handling date selection and time period management for financial analysis.
# Provides user-friendly functions for selecting date ranges and time periods with
# both interactive and programmatic interfaces.
# """
# 
# import pandas as pd
# from datetime import datetime, timedelta
# import calendar
# import logging
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# def get_start_date(interactive=True, period=None):
#     """
#     Get a start date for financial analysis.
# 
#     Args:
#         interactive (bool): Whether to prompt for user input
#         period (str): Predefined period if not interactive ('1y', '3y', '5y', '10y', etc.)
# 
#     Returns:
#         datetime: Start date as a datetime object
#     """
#     if not interactive:
#         if period:
#             return calculate_date_from_period(period)
#         else:
#             # Default to 5 years ago
#             return datetime.now() - timedelta(days=5*365)
# 
#     print("\nВыберите начальную дату:")
#     print("1: Сегодня")
#     print("2: Год назад")
#     print("3: 3 года назад")
#     print("4: 5 лет назад")
#     print("5: 10 лет назад")
#     print("6: Ввести конкретную дату (ГГГГ-ММ-ДД)")
#     print("7: Ввести произвольный период (например, 2.5y для 2.5 лет)")
# 
#     choice = input("\nВведите ваш выбор (1-7): ")
# 
#     try:
#         if choice == '1':
#             return datetime.now()
#         elif choice == '2':
#             return datetime.now() - timedelta(days=365)
#         elif choice == '3':
#             return datetime.now() - timedelta(days=3*365)
#         elif choice == '4':
#             return datetime.now() - timedelta(days=5*365)
#         elif choice == '5':
#             return datetime.now() - timedelta(days=10*365)
#         elif choice == '6':
#             date_str = input("Введите дату (ГГГГ-ММ-ДД): ")
#             return parse_date_string(date_str)
#         elif choice == '7':
#             period_str = input("Введите период (например, 2.5y, 6m, 30d): ")
#             return calculate_date_from_period(period_str)
#         else:
#             logger.warning("Неверный выбор, используется значение по умолчанию (5 лет назад)")
#             return datetime.now() - timedelta(days=5*365)
#     except ValueError as e:
#         logger.error(f"Ошибка при обработке даты: {str(e)}")
#         logger.info("Используется значение по умолчанию (5 лет назад)")
#         return datetime.now() - timedelta(days=5*365)
# 
# 
# def get_end_date(interactive=True, end_date_str=None):
#     """
#     Get an end date for financial analysis.
# 
#     Args:
#         interactive (bool): Whether to prompt for user input
#         end_date_str (str): Specific end date if not interactive (YYYY-MM-DD)
# 
#     Returns:
#         datetime: End date as a datetime object
#     """
#     if not interactive:
#         if end_date_str:
#             return parse_date_string(end_date_str)
#         else:
#             # Default to today
#             return datetime.now()
# 
#     print("\nВыберите конечную дату:")
#     print("1: Сегодня")
#     print("2: Конец прошлого месяца")
#     print("3: Конец прошлого квартала")
#     print("4: Конец прошлого года")
#     print("5: Ввести конкретную дату (ГГГГ-ММ-ДД)")
# 
#     choice = input("\nВведите ваш выбор (1-5): ")
# 
#     try:
#         if choice == '1':
#             return datetime.now()
#         elif choice == '2':
#             return get_end_of_last_month()
#         elif choice == '3':
#             return get_end_of_last_quarter()
#         elif choice == '4':
#             return get_end_of_last_year()
#         elif choice == '5':
#             date_str = input("Введите дату (ГГГГ-ММ-ДД): ")
#             return parse_date_string(date_str)
#         else:
#             logger.warning("Неверный выбор, используется значение по умолчанию (сегодня)")
#             return datetime.now()
#     except ValueError as e:
#         logger.error(f"Ошибка при обработке даты: {str(e)}")
#         logger.info("Используется значение по умолчанию (сегодня)")
#         return datetime.now()
# 
# 
# def parse_date_string(date_str):
#     """
#     Parse a date string in various formats.
# 
#     Args:
#         date_str (str): Date string to parse
# 
#     Returns:
#         datetime: Parsed date
# 
#     Raises:
#         ValueError: If the date string cannot be parsed
#     """
#     formats = [
#         '%Y-%m-%d',
#         '%Y/%m/%d',
#         '%d-%m-%Y',
#         '%d/%m/%Y',
#         '%m-%d-%Y',
#         '%m/%d/%Y',
#         '%Y.%m.%d',
#         '%d.%m.%Y',
#         '%m.%d.%Y'
#     ]
# 
#     for fmt in formats:
#         try:
#             return datetime.strptime(date_str, fmt)
#         except ValueError:
#             continue
# 
#     raise ValueError(f"Не удалось распознать формат даты: {date_str}")
# 
# 
# def calculate_date_from_period(period_str):
#     """
#     Calculate a date by subtracting a period from the current date.
# 
#     Args:
#         period_str (str): Period string (e.g., '1y', '6m', '30d', '2.5y')
# 
#     Returns:
#         datetime: Calculated date
# 
#     Raises:
#         ValueError: If the period string is invalid
#     """
#     period_str = period_str.strip().lower()
# 
#     # Extract the numeric value and unit
#     if period_str[-1] == 'y':
#         unit = 'years'
#         value = period_str[:-1]
#     elif period_str[-1] == 'm':
#         unit = 'months'
#         value = period_str[:-1]
#     elif period_str[-1] == 'd':
#         unit = 'days'
#         value = period_str[:-1]
#     elif period_str[-1] == 'w':
#         unit = 'weeks'
#         value = period_str[:-1]
#     else:
#         raise ValueError(f"Неизвестная единица измерения в периоде: {period_str}")
# 
#     try:
#         value = float(value)
#     except ValueError:
#         raise ValueError(f"Неверное числовое значение в периоде: {period_str}")
# 
#     now = datetime.now()
# 
#     if unit == 'years':
#         days = int(value * 365)
#         return now - timedelta(days=days)
#     elif unit == 'months':
#         # Approximate months as 30.44 days
#         days = int(value * 30.44)
#         return now - timedelta(days=days)
#     elif unit == 'weeks':
#         days = int(value * 7)
#         return now - timedelta(days=days)
#     elif unit == 'days':
#         return now - timedelta(days=int(value))
# 
# 
# def get_end_of_last_month():
#     """
#     Get the date for the end of the previous month.
# 
#     Returns:
#         datetime: End of last month
#     """
#     today = datetime.now()
#     first_of_month = datetime(today.year, today.month, 1)
#     last_day_of_prev_month = first_of_month - timedelta(days=1)
#     return last_day_of_prev_month
# 
# 
# def get_end_of_last_quarter():
#     """
#     Get the date for the end of the previous quarter.
# 
#     Returns:
#         datetime: End of last quarter
#     """
#     today = datetime.now()
#     current_quarter = (today.month - 1) // 3 + 1
#     if current_quarter == 1:
#         # End of Q4 previous year
#         last_quarter_end_month = 12
#         year = today.year - 1
#     else:
#         # End of previous quarter this year
#         last_quarter_end_month = (current_quarter - 1) * 3
#         year = today.year
# 
#     # Get the last day of the last month of the quarter
#     last_day = calendar.monthrange(year, last_quarter_end_month)[1]
#     return datetime(year, last_quarter_end_month, last_day)
# 
# 
# def get_end_of_last_year():
#     """
#     Get the date for the end of the previous year.
# 
#     Returns:
#         datetime: End of last year
#     """
#     today = datetime.now()
#     return datetime(today.year - 1, 12, 31)
# 
# 
# def get_trading_days(start_date, end_date=None):
#     """
#     Get a DatetimeIndex of trading days between start_date and end_date.
# 
#     Args:
#         start_date (datetime): Start date
#         end_date (datetime, optional): End date. Defaults to today.
# 
#     Returns:
#         pandas.DatetimeIndex: Index of trading days
#     """
#     if end_date is None:
#         end_date = datetime.now()
# 
#     # Convert to pandas datetime
#     start = pd.Timestamp(start_date)
#     end = pd.Timestamp(end_date)
# 
#     # Generate business days (trading days)
#     trading_days = pd.date_range(start=start, end=end, freq='B')
# 
#     return trading_days
# 
# 
# def get_date_range_description(start_date, end_date=None):
#     """
#     Get a human-readable description of a date range.
# 
#     Args:
#         start_date (datetime): Start date
#         end_date (datetime, optional): End date. Defaults to today.
# 
#     Returns:
#         str: Description of the date range
#     """
#     if end_date is None:
#         end_date = datetime.now()
# 
#     delta = end_date - start_date
#     days = delta.days
# 
#     if days < 30:
#         return f"{days} дней"
#     elif days < 365:
#         months = round(days / 30.44)
#         return f"{months} месяцев"
#     else:
#         years = round(days / 365, 1)
#         return f"{years} лет"
# 
# 
# def format_date(date, format_str='%Y-%m-%d'):
#     """
#     Format a date as a string.
# 
#     Args:
#         date (datetime): Date to format
#         format_str (str, optional): Format string. Defaults to '%Y-%m-%d'.
# 
#     Returns:
#         str: Formatted date string
#     """
#     if isinstance(date, datetime):
#         return date.strftime(format_str)
#     else:
#         return date
# 
# 
# def date_range_to_str(start_date, end_date=None, format_str='%Y-%m-%d'):
#     """
#     Convert a date range to formatted strings.
# 
#     Args:
#         start_date (datetime): Start date
#         end_date (datetime, optional): End date. Defaults to today.
#         format_str (str, optional): Format string. Defaults to '%Y-%m-%d'.
# 
#     Returns:
#         tuple: (start_date_str, end_date_str)
#     """
#     if end_date is None:
#         end_date = datetime.now()
# 
#     start_str = format_date(start_date, format_str)
#     end_str = format_date(end_date, format_str)
# 
#     return start_str, end_str
# 
# 
# def get_quarter_start_end(year, quarter):
#     """
#     Get the start and end dates for a specific quarter.
# 
#     Args:
#         year (int): Year
#         quarter (int): Quarter (1-4)
# 
#     Returns:
#         tuple: (start_date, end_date)
#     """
#     if quarter < 1 or quarter > 4:
#         raise ValueError("Quarter must be between 1 and 4")
# 
#     start_month = (quarter - 1) * 3 + 1
#     start_date = datetime(year, start_month, 1)
# 
#     if quarter == 4:
#         end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
#     else:
#         end_month = start_month + 3
#         end_date = datetime(year, end_month, 1) - timedelta(days=1)
# 
#     return start_date, end_date
# 
# 
# def get_month_start_end(year, month):
#     """
#     Get the start and end dates for a specific month.
# 
#     Args:
#         year (int): Year
#         month (int): Month (1-12)
# 
#     Returns:
#         tuple: (start_date, end_date)
#     """
#     if month < 1 or month > 12:
#         raise ValueError("Month must be between 1 and 12")
# 
#     start_date = datetime(year, month, 1)
# 
#     if month == 12:
#         end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
#     else:
#         end_date = datetime(year, month + 1, 1) - timedelta(days=1)
# 
#     return start_date, end_date
# 
# 
# def get_year_start_end(year):
#     """
#     Get the start and end dates for a specific year.
# 
#     Args:
#         year (int): Year
# 
#     Returns:
#         tuple: (start_date, end_date)
#     """
#     start_date = datetime(year, 1, 1)
#     end_date = datetime(year, 12, 31)
# 
#     return start_date, end_date
# 
# 
# def get_recent_periods():
#     """
#     Get a dictionary of recent time periods for analysis.
# 
#     Returns:
#         dict: Dictionary of time periods with start and end dates
#     """
#     today = datetime.now()
# 
#     periods = {
#         '1_month': {
#             'start': today - timedelta(days=30),
#             'end': today,
#             'description': '1 месяц'
#         },
#         '3_months': {
#             'start': today - timedelta(days=90),
#             'end': today,
#             'description': '3 месяца'
#         },
#         '6_months': {
#             'start': today - timedelta(days=180),
#             'end': today,
#             'description': '6 месяцев'
#         },
#         '1_year': {
#             'start': today - timedelta(days=365),
#             'end': today,
#             'description': '1 год'
#         },
#         '3_years': {
#             'start': today - timedelta(days=3*365),
#             'end': today,
#             'description': '3 года'
#         },
#         '5_years': {
#             'start': today - timedelta(days=5*365),
#             'end': today,
#             'description': '5 лет'
#         },
#         'ytd': {
#             'start': datetime(today.year, 1, 1),
#             'end': today,
#             'description': 'С начала года'
#         }
#     }
# 
#     return periods
# 
# 
# def is_market_open():
#     """
#     Check if the market is currently open (US market hours).
# 
#     Returns:
#         bool: True if market is open, False otherwise
#     """
#     now = datetime.now()
# 
#     # Check if it's a weekday (0 = Monday, 6 = Sunday)
#     if now.weekday() >= 5:
#         return False
# 
#     # US market hours: 9:30 AM - 4:00 PM Eastern Time
#     # Need to convert to Eastern Time if in different timezone
#     # This is a simplified version assuming machine is in US Eastern Time
#     market_open = datetime(now.year, now.month, now.day, 9, 30)
#     market_close = datetime(now.year, now.month, now.day, 16, 0)
# 
#     return market_open <= now <= market_close
# 
# 
# def next_market_open():
#     """
#     Get the next market open date/time (US market).
# 
#     Returns:
#         datetime: Next market open date/time
#     """
#     now = datetime.now()
# 
#     # Start with today
#     next_open_date = now.date()
# 
#     # If it's after market close or weekend, move to next business day
#     if now.weekday() >= 5 or (now.weekday() < 5 and now.hour >= 16):
#         days_to_add = 1
#         if now.weekday() == 4 and now.hour >= 16:  # Friday after close
#             days_to_add = 3
#         elif now.weekday() == 5:  # Saturday
#             days_to_add = 2
#         elif now.weekday() == 6:  # Sunday
#             days_to_add = 1
# 
#         next_open_date = (now + timedelta(days=days_to_add)).date()
# 
#     # Return the next open time (9:30 AM)
#     return datetime.combine(next_open_date, datetime.min.time()) + timedelta(hours=9, minutes=30)
# 
# 
# # Test function to demonstrate module functionality
# def test_date_module():
#     """Test functionality of the date module."""
#     print("\nТестирование модуля обработки дат:")
# 
#     # Test period calculation
#     period = "2.5y"
#     date = calculate_date_from_period(period)
#     print(f"Дата {period} назад: {date.strftime('%Y-%m-%d')}")
# 
#     # Test end of period functions
#     print(f"Конец прошлого месяца: {get_end_of_last_month().strftime('%Y-%m-%d')}")
#     print(f"Конец прошлого квартала: {get_end_of_last_quarter().strftime('%Y-%m-%d')}")
#     print(f"Конец прошлого года: {get_end_of_last_year().strftime('%Y-%m-%d')}")
# 
#     # Test trading days
#     start = datetime.now() - timedelta(days=30)
#     trading_days = get_trading_days(start)
#     print(f"Количество торговых дней за последние 30 дней: {len(trading_days)}")
# 
#     # Test date range description
#     print(f"Описание периода: {get_date_range_description(start)}")
# 
#     # Test market open/close functions
#     print(f"Рынок сейчас открыт: {is_market_open()}")
#     print(f"Следующее открытие рынка: {next_market_open().strftime('%Y-%m-%d %H:%M')}")
# 
#     # Test quarter functions
#     q_start, q_end = get_quarter_start_end(2023, 1)
#     print(f"1 квартал 2023: {q_start.strftime('%Y-%m-%d')} - {q_end.strftime('%Y-%m-%d')}")
# 
#     # Test recent periods
#     periods = get_recent_periods()
#     print("\nДоступные периоды для анализа:")
#     for name, period in periods.items():
#         print(f"{period['description']}: {period['start'].strftime('%Y-%m-%d')} - {period['end'].strftime('%Y-%m-%d')}")
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_date_module()

"""## module_investment_allocation.py"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_investment_allocation.py
# """
# Module for investment allocation and portfolio management.
# Provides functions for allocating investment amounts based on portfolio weights,
# calculating rebalancing needs, and generating investment plans.
# """
# 
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import logging
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# def print_investment_allocation(weights, investment_amount, tickers, threshold=0.005):
#     """
#     Print how much to invest in each asset based on portfolio weights.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         investment_amount (float): Total investment amount
#         tickers (list): List of ticker symbols
#         threshold (float): Minimum weight threshold to include in output
#     """
#     # Ensure weights is numpy array
#     weights = np.array(weights)
# 
#     # Calculate allocation amounts
#     allocations = weights * investment_amount
# 
#     # Create formatted output
#     print("\nИнвестиционное распределение:")
#     print("=" * 50)
#     print(f"{'Актив':<10} {'Вес':<10} {'Сумма, USD':<15}")
#     print("-" * 50)
# 
#     # Sort by allocation amount in descending order
#     sorted_indices = allocations.argsort()[::-1]
# 
#     total_displayed = 0
#     for i in sorted_indices:
#         if weights[i] >= threshold:
#             ticker = tickers[i] if i < len(tickers) else f"Asset {i+1}"
#             print(f"{ticker:<10} {weights[i]:>9.2%} {allocations[i]:>15,.2f}")
#             total_displayed += allocations[i]
# 
#     # Show 'Other' category for small allocations
#     other_amount = investment_amount - total_displayed
#     if other_amount > 0:
#         print(f"{'Другие':<10} {other_amount/investment_amount:>9.2%} {other_amount:>15,.2f}")
# 
#     print("-" * 50)
#     print(f"{'Всего':<10} {1.0:>9.2%} {investment_amount:>15,.2f}")
#     print("=" * 50)
# 
# 
# def calculate_allocation_table(weights, investment_amount, tickers, include_count=True):
#     """
#     Calculate allocation table with optional share counts.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         investment_amount (float): Total investment amount
#         tickers (list): List of ticker symbols
#         include_count (bool): Whether to include share counts
# 
#     Returns:
#         pandas.DataFrame: Table with allocation details
#     """
#     # Ensure weights is numpy array
#     weights = np.array(weights)
# 
#     # Calculate allocation amounts
#     allocations = weights * investment_amount
# 
#     # Create DataFrame
#     data = {
#         'Тикер': tickers,
#         'Вес': weights,
#         'Сумма': allocations
#     }
# 
#     df = pd.DataFrame(data)
# 
#     # If share counts requested, try to fetch current prices
#     if include_count:
#         try:
#             # Import here to avoid circular imports
#             from module_data import load_stock_data_from_yahoo
# 
#             # Get latest prices
#             latest_prices = load_stock_data_from_yahoo(tickers)
# 
#             if not latest_prices.empty:
#                 # Get the most recent price for each ticker
#                 current_prices = latest_prices.iloc[-1]
# 
#                 # Calculate share counts
#                 share_counts = {}
#                 for ticker, allocation in zip(tickers, allocations):
#                     if ticker in current_prices and current_prices[ticker] > 0:
#                         share_counts[ticker] = allocation / current_prices[ticker]
#                     else:
#                         share_counts[ticker] = np.nan
# 
#                 df['Цена'] = [current_prices.get(ticker, np.nan) for ticker in tickers]
#                 df['Количество'] = [share_counts.get(ticker, np.nan) for ticker in tickers]
# 
#                 # Round share counts to integers or appropriate decimals
#                 df['Количество_округлено'] = np.floor(df['Количество'])
#                 df['Остаток'] = (df['Количество'] - df['Количество_округлено']) * df['Цена']
# 
#         except Exception as e:
#             logger.error(f"Ошибка при расчете количества акций: {str(e)}")
# 
#     # Format columns
#     df['Вес'] = df['Вес'].map('{:.2%}'.format)
#     df['Сумма'] = df['Сумма'].map('${:,.2f}'.format)
# 
#     if 'Цена' in df.columns:
#         df['Цена'] = df['Цена'].map('${:,.2f}'.format)
# 
#     if 'Количество' in df.columns:
#         df['Количество'] = df['Количество'].map('{:.4f}'.format)
#         df['Количество_округлено'] = df['Количество_округлено'].map('{:.0f}'.format)
#         df['Остаток'] = df['Остаток'].map('${:,.2f}'.format)
# 
#     return df
# 
# 
# def format_allocation_for_export(weights, investment_amount, tickers, filename=None):
#     """
#     Format allocation data for export to CSV or Excel.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         investment_amount (float): Total investment amount
#         tickers (list): List of ticker symbols
#         filename (str, optional): Output filename. If provided, saves to file.
# 
#     Returns:
#         pandas.DataFrame: Formatted allocation data
#     """
#     # Get allocation table
#     df = calculate_allocation_table(weights, investment_amount, tickers)
# 
#     # Save to file if requested
#     if filename:
#         if filename.endswith('.csv'):
#             df.to_csv(filename, index=False)
#             print(f"Данные инвестиционного распределения сохранены в {filename}")
#         elif filename.endswith('.xlsx'):
#             df.to_excel(filename, index=False)
#             print(f"Данные инвестиционного распределения сохранены в {filename}")
#         else:
#             # Default to CSV
#             df.to_csv(f"{filename}.csv", index=False)
#             print(f"Данные инвестиционного распределения сохранены в {filename}.csv")
# 
#     return df
# 
# 
# def plot_investment_allocation(weights, tickers, title="Распределение инвестиций", threshold=0.02):
#     """
#     Plot investment allocation as a pie chart.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         tickers (list): List of ticker symbols
#         title (str): Chart title
#         threshold (float): Minimum weight threshold to display separately
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Ensure weights is numpy array
#     weights = np.array(weights)
# 
#     # Combine small allocations into 'Other'
#     small_indices = np.where(weights < threshold)[0]
#     large_indices = np.where(weights >= threshold)[0]
# 
#     # Create plot data
#     plot_weights = []
#     plot_labels = []
# 
#     # Add main allocations
#     for i in large_indices:
#         plot_weights.append(weights[i])
#         plot_labels.append(tickers[i] if i < len(tickers) else f"Asset {i+1}")
# 
#     # Add 'Other' if there are small allocations
#     if len(small_indices) > 0:
#         other_weight = sum(weights[i] for i in small_indices)
#         if other_weight > 0:
#             plot_weights.append(other_weight)
#             plot_labels.append("Другие")
# 
#     # Create figure and axis
#     fig, ax = plt.subplots(figsize=(10, 7))
# 
#     # Create pie chart
#     wedges, texts, autotexts = ax.pie(
#         plot_weights,
#         labels=plot_labels,
#         autopct='%1.1f%%',
#         startangle=90,
#         wedgeprops={'edgecolor': 'w', 'linewidth': 1}
#     )
# 
#     # Styling
#     for text in texts + autotexts:
#         text.set_fontsize(9)
# 
#     for autotext in autotexts:
#         autotext.set_color('white')
#         autotext.set_fontweight('bold')
# 
#     # Title
#     ax.set_title(title, fontsize=14, pad=20)
# 
#     # Add legend
#     ax.legend(wedges, plot_labels, title="Активы", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))
# 
#     # Equal aspect ratio ensures the pie chart is circular
#     ax.set_aspect('equal')
# 
#     plt.tight_layout()
# 
#     return fig
# 
# 
# def calculate_rebalancing_needs(current_weights, target_weights, current_value, tickers):
#     """
#     Calculate rebalancing needs to match target portfolio weights.
# 
#     Args:
#         current_weights (numpy.array): Current portfolio weights
#         target_weights (numpy.array): Target portfolio weights
#         current_value (float): Current portfolio value
#         tickers (list): List of ticker symbols
# 
#     Returns:
#         pandas.DataFrame: Rebalancing actions
#     """
#     # Ensure weights are numpy arrays
#     current_weights = np.array(current_weights)
#     target_weights = np.array(target_weights)
# 
#     # Calculate current and target values
#     current_values = current_weights * current_value
#     target_values = target_weights * current_value
# 
#     # Calculate differences
#     value_diffs = target_values - current_values
# 
#     # Create DataFrame
#     data = {
#         'Тикер': tickers,
#         'Текущий вес': current_weights,
#         'Целевой вес': target_weights,
#         'Разница весов': target_weights - current_weights,
#         'Текущая стоимость': current_values,
#         'Целевая стоимость': target_values,
#         'Изменение': value_diffs,
#         'Действие': ['Купить' if d > 0 else 'Продать' if d < 0 else 'Держать' for d in value_diffs]
#     }
# 
#     df = pd.DataFrame(data)
# 
#     # Format columns
#     df['Текущий вес'] = df['Текущий вес'].map('{:.2%}'.format)
#     df['Целевой вес'] = df['Целевой вес'].map('{:.2%}'.format)
#     df['Разница весов'] = df['Разница весов'].map('{:+.2%}'.format)
#     df['Текущая стоимость'] = df['Текущая стоимость'].map('${:,.2f}'.format)
#     df['Целевая стоимость'] = df['Целевая стоимость'].map('${:,.2f}'.format)
#     df['Изменение'] = df['Изменение'].map('${:+,.2f}'.format)
# 
#     # Sort by absolute value of change
#     df['abs_change'] = abs(value_diffs)
#     df = df.sort_values('abs_change', ascending=False)
#     df = df.drop('abs_change', axis=1)
# 
#     return df
# 
# 
# def recommend_investment_plan(weights, investment_amount, tickers, frequency='monthly', duration=12):
#     """
#     Recommend an investment plan over time for dollar-cost averaging.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         investment_amount (float): Total investment amount
#         tickers (list): List of ticker symbols
#         frequency (str): Investment frequency ('weekly', 'monthly', 'quarterly')
#         duration (int): Number of periods
# 
#     Returns:
#         pandas.DataFrame: Investment plan over time
#     """
#     # Ensure weights is numpy array
#     weights = np.array(weights)
# 
#     # Determine period amount
#     period_amount = investment_amount / duration
# 
#     # Create period labels
#     if frequency == 'weekly':
#         period_labels = [f"Неделя {i+1}" for i in range(duration)]
#     elif frequency == 'monthly':
#         period_labels = [f"Месяц {i+1}" for i in range(duration)]
#     elif frequency == 'quarterly':
#         period_labels = [f"Квартал {i+1}" for i in range(duration)]
#     else:
#         period_labels = [f"Период {i+1}" for i in range(duration)]
# 
#     # Calculate period allocations
#     period_allocations = {}
#     for i, ticker in enumerate(tickers):
#         period_allocations[ticker] = [weights[i] * period_amount] * duration
# 
#     # Create DataFrame
#     df = pd.DataFrame(period_allocations, index=period_labels)
# 
#     # Add totals
#     df['Итого'] = df.sum(axis=1)
# 
#     # Format
#     for col in df.columns:
#         df[col] = df[col].map('${:,.2f}'.format)
# 
#     return df
# 
# 
# def calculate_asset_drift(start_weights, returns, periods=252):
#     """
#     Calculate how portfolio weights would drift over time due to differing asset returns.
# 
#     Args:
#         start_weights (numpy.array): Starting portfolio weights
#         returns (pandas.DataFrame): Asset returns over time
#         periods (int): Number of periods to simulate
# 
#     Returns:
#         pandas.DataFrame: Portfolio weights over time
#     """
#     # Ensure weights is numpy array
#     weights = np.array(start_weights).copy()
# 
#     # Select returns to use
#     if len(returns) < periods:
#         periods = len(returns)
# 
#     # Use the most recent periods
#     recent_returns = returns.iloc[-periods:]
# 
#     # Initialize weights DataFrame
#     weights_over_time = pd.DataFrame(index=range(periods + 1), columns=returns.columns)
#     weights_over_time.iloc[0] = weights
# 
#     # Simulate weight drift over time
#     for i in range(periods):
#         # Calculate growth of each asset
#         period_returns = 1 + recent_returns.iloc[i % len(recent_returns)]
# 
#         # Update asset values
#         weights = weights * period_returns
# 
#         # Normalize weights
#         weights = weights / weights.sum()
# 
#         # Store weights
#         weights_over_time.iloc[i + 1] = weights
# 
#     return weights_over_time
# 
# 
# def visualize_asset_drift(weights_over_time, title="Дрейф весов активов со временем"):
#     """
#     Visualize how portfolio weights would drift over time.
# 
#     Args:
#         weights_over_time (pandas.DataFrame): Portfolio weights over time
#         title (str): Plot title
# 
#     Returns:
#         matplotlib.figure.Figure: Generated figure
#     """
#     # Create figure and axis
#     fig, ax = plt.subplots(figsize=(12, 8))
# 
#     # Plot each asset
#     for col in weights_over_time.columns:
#         ax.plot(weights_over_time.index, weights_over_time[col], label=col)
# 
#     # Add markers for initial and final weights
#     ax.scatter(0, weights_over_time.iloc[0], s=50, c='black', zorder=5)
#     ax.scatter(
#         weights_over_time.index[-1],
#         weights_over_time.iloc[-1],
#         s=50, c='black',
#         zorder=5
#     )
# 
#     # Styling
#     ax.set_xlabel('Период', fontsize=12)
#     ax.set_ylabel('Вес в портфеле', fontsize=12)
#     ax.set_title(title, fontsize=14)
#     ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
#     ax.grid(True, alpha=0.3)
# 
#     # Y-axis as percentage
#     ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
# 
#     plt.tight_layout()
# 
#     return fig
# 
# 
# def generate_tax_lots(weights, investment_amount, tickers, dates=None, price_data=None):
#     """
#     Generate simulated tax lots for portfolio analysis.
# 
#     Args:
#         weights (numpy.array): Portfolio weights
#         investment_amount (float): Total investment amount
#         tickers (list): List of ticker symbols
#         dates (list, optional): Purchase dates for tax lots
#         price_data (pandas.DataFrame, optional): Historical price data
# 
#     Returns:
#         pandas.DataFrame: Tax lot information
#     """
#     # Ensure weights is numpy array
#     weights = np.array(weights)
# 
#     # Default to a single tax lot if no dates provided
#     if dates is None:
#         dates = [datetime.now()]
# 
#     # Create tax lots
#     tax_lots = []
# 
#     for i, ticker in enumerate(tickers):
#         ticker_amount = weights[i] * investment_amount
# 
#         # Divide across dates
#         for j, date in enumerate(dates):
#             lot_amount = ticker_amount / len(dates)
# 
#             # Get price if available
#             price = None
#             if price_data is not None and ticker in price_data.columns:
#                 # Find closest date
#                 closest_date = price_data.index[price_data.index <= date][-1]
#                 price = price_data.loc[closest_date, ticker]
# 
#             shares = None
#             if price:
#                 shares = lot_amount / price
# 
#             tax_lots.append({
#                 'Тикер': ticker,
#                 'Дата': date,
#                 'Сумма': lot_amount,
#                 'Цена': price,
#                 'Количество': shares,
#                 'Лот': j + 1
#             })
# 
#     # Create DataFrame
#     df = pd.DataFrame(tax_lots)
# 
#     # Format columns
#     df['Сумма'] = df['Сумма'].map('${:,.2f}'.format)
#     if 'Цена' in df.columns and df['Цена'].notna().any():
#         df['Цена'] = df['Цена'].map('${:,.2f}'.format)
#     if 'Количество' in df.columns and df['Количество'].notna().any():
#         df['Количество'] = df['Количество'].map('{:.4f}'.format)
# 
#     return df
# 
# 
# def print_results(name, returns, risk, sharpe_ratio=None):
#     """
#     Print portfolio results in a formatted manner.
# 
#     Args:
#         name (str): Portfolio name or description
#         returns (float): Expected returns
#         risk (float): Expected risk (volatility)
#         sharpe_ratio (float, optional): Sharpe ratio
#     """
#     print(f"\n{name}:")
#     print("-" * 40)
#     print(f"  Ожидаемая доходность: {returns:.2%}")
#     print(f"  Ожидаемый риск (волатильность): {risk:.2%}")
#     if sharpe_ratio is not None:
#         print(f"  Коэффициент Шарпа: {sharpe_ratio:.2f}")
#     print("-" * 40)
# 
# 
# # Test function
# def test_investment_allocation():
#     """Test function for the investment allocation module."""
#     print("\n=== Тестирование модуля инвестиционного распределения ===")
# 
#     # Sample data
#     tickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'FB', 'TSLA', 'NVDA', 'JPM', 'V', 'PG']
#     weights = np.array([0.15, 0.15, 0.12, 0.12, 0.10, 0.08, 0.08, 0.07, 0.07, 0.06])
#     investment_amount = 10000
# 
#     # Test print allocation
#     print("\nТест функции печати распределения:")
#     print_investment_allocation(weights, investment_amount, tickers)
# 
#     # Test allocation table
#     print("\nТест таблицы распределения:")
#     table = calculate_allocation_table(weights, investment_amount, tickers, include_count=False)
#     print(table)
# 
#     # Test rebalancing
#     print("\nТест расчета ребалансировки:")
#     current_weights = weights * np.random.normal(1, 0.1, size=len(weights))
#     current_weights = current_weights / current_weights.sum()
#     rebalancing = calculate_rebalancing_needs(current_weights, weights, investment_amount, tickers)
#     print(rebalancing)
# 
#     # Test investment plan
#     print("\nТест плана инвестирования:")
#     plan = recommend_investment_plan(weights, investment_amount, tickers, frequency='monthly', duration=6)
#     print(plan)
# 
#     print("\n=== Тестирование завершено ===")
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_investment_allocation()

"""## module_risk_return.py


"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_risk_return.py
# """
# Module for calculating risk and return metrics for financial analysis.
# Provides comprehensive functions for calculating various risk metrics,
# return calculations, and performance measures for investment portfolios.
# """
# 
# import numpy as np
# import pandas as pd
# from scipy import stats
# import logging
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# 
# def calculate_returns(data, period_months=12, freq='daily', annualized=True):
#     """
#     Calculate returns and risk metrics based on price data.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data
#         period_months (int): Investment period in months
#         freq (str): Return frequency ('daily', 'weekly', 'monthly')
#         annualized (bool): Whether to annualize returns and risk
# 
#     Returns:
#         tuple: (mean_returns, cov_matrix, std_deviation)
#     """
#     # Validate input data
#     if data.empty:
#         logger.error("Empty data provided for return calculation")
#         return None, None, None
# 
#     # Determine the period to use for returns calculation
#     if freq == 'daily':
#         returns = data.pct_change().dropna()
#         ann_factor = 252  # Trading days in a year
#     elif freq == 'weekly':
#         returns = data.resample('W').last().pct_change().dropna()
#         ann_factor = 52   # Weeks in a year
#     elif freq == 'monthly':
#         returns = data.resample('M').last().pct_change().dropna()
#         ann_factor = 12   # Months in a year
#     else:
#         logger.warning(f"Unknown frequency: {freq}, using daily")
#         returns = data.pct_change().dropna()
#         ann_factor = 252
# 
#     # Adjust calculation based on investment period
#     if period_months <= 12:
#         # Short-term: focus on recent data
#         if len(returns) > 252:  # If we have more than a year of data
#             # Use more weight on recent data for short-term investment
#             recent_returns = returns.iloc[-252:]
#             mean_returns = recent_returns.mean()
#             cov_matrix = recent_returns.cov()
#             std_deviation = recent_returns.std()
#         else:
#             mean_returns = returns.mean()
#             cov_matrix = returns.cov()
#             std_deviation = returns.std()
#     else:
#         # Long-term: use all available data
#         mean_returns = returns.mean()
#         cov_matrix = returns.cov()
#         std_deviation = returns.std()
# 
#     # Annualize if requested
#     if annualized:
#         mean_returns = mean_returns * ann_factor
#         cov_matrix = cov_matrix * ann_factor
#         std_deviation = std_deviation * np.sqrt(ann_factor)
# 
#     return mean_returns, cov_matrix, std_deviation
# 
# 
# def calculate_volatility(returns):
#     """
#     Calculate the volatility (standard deviation of returns).
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
# 
#     Returns:
#         float/pandas.Series: Volatility
#     """
#     return returns.std()
# 
# 
# def calculate_annualized_volatility(returns, periods_per_year=252):
#     """
#     Calculate annualized volatility.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Annualized volatility
#     """
#     return returns.std() * np.sqrt(periods_per_year)
# 
# 
# def calculate_var(returns, confidence_level=0.95):
#     """
#     Calculate Value at Risk (VaR) at a specified confidence level.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         confidence_level (float): Confidence level (e.g., 0.95 for 95%)
# 
#     Returns:
#         float/pandas.Series: Value at Risk
#     """
#     if isinstance(returns, pd.DataFrame):
#         return returns.quantile(1 - confidence_level)
#     else:
#         return np.percentile(returns, (1 - confidence_level) * 100)
# 
# 
# def calculate_cvar(returns, confidence_level=0.95):
#     """
#     Calculate Conditional Value at Risk (CVaR) at a specified confidence level.
#     Also known as Expected Shortfall.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         confidence_level (float): Confidence level (e.g., 0.95 for 95%)
# 
#     Returns:
#         float/pandas.Series: Conditional Value at Risk
#     """
#     var = calculate_var(returns, confidence_level)
# 
#     if isinstance(returns, pd.DataFrame):
#         # For a DataFrame, calculate CVaR for each column
#         cvar_values = {}
#         for column in returns.columns:
#             column_returns = returns[column]
#             column_var = var[column]
#             cvar_values[column] = column_returns[column_returns <= column_var].mean()
#         return pd.Series(cvar_values)
#     else:
#         # For a Series
#         return returns[returns <= var].mean()
# 
# 
# def calculate_sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):
#     """
#     Calculate the Sharpe ratio.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         risk_free_rate (float): Annualized risk-free rate
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Sharpe ratio
#     """
#     # Convert annual risk-free rate to per-period rate
#     rf_per_period = ((1 + risk_free_rate) ** (1 / periods_per_year)) - 1
# 
#     excess_returns = returns - rf_per_period
#     ann_sharpe = (excess_returns.mean() / returns.std()) * np.sqrt(periods_per_year)
# 
#     return ann_sharpe
# 
# 
# def calculate_sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):
#     """
#     Calculate the Sortino ratio (uses downside deviation instead of standard deviation).
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         risk_free_rate (float): Annualized risk-free rate
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Sortino ratio
#     """
#     # Convert annual risk-free rate to per-period rate
#     rf_per_period = ((1 + risk_free_rate) ** (1 / periods_per_year)) - 1
# 
#     excess_returns = returns - rf_per_period
# 
#     # Calculate downside deviation (standard deviation of negative returns only)
#     negative_returns = returns.copy()
#     if isinstance(returns, pd.DataFrame):
#         for column in negative_returns.columns:
#             negative_returns.loc[negative_returns[column] > 0, column] = 0
#     else:
#         negative_returns[negative_returns > 0] = 0
# 
#     downside_deviation = np.sqrt((negative_returns ** 2).mean()) * np.sqrt(periods_per_year)
# 
#     # Avoid division by zero
#     downside_deviation = np.where(downside_deviation == 0, 0.000001, downside_deviation)
# 
#     # Calculate Sortino ratio
#     sortino = (excess_returns.mean() * periods_per_year) / downside_deviation
# 
#     return sortino
# 
# 
# def calculate_maximum_drawdown(price_data):
#     """
#     Calculate the maximum drawdown from price data.
# 
#     Args:
#         price_data (pandas.DataFrame/Series): Price data
# 
#     Returns:
#         float/pandas.Series: Maximum drawdown
#     """
#     # Calculate the cumulative maximum
#     roll_max = price_data.cummax()
# 
#     # Calculate the drawdown
#     drawdown = (price_data / roll_max) - 1
# 
#     # Get the maximum drawdown
#     max_drawdown = drawdown.min()
# 
#     return max_drawdown
# 
# 
# def calculate_calmar_ratio(returns, price_data, periods_per_year=252):
#     """
#     Calculate the Calmar ratio (annualized return divided by maximum drawdown).
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         price_data (pandas.DataFrame/Series): Price data
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Calmar ratio
#     """
#     # Calculate annualized return
#     ann_return = returns.mean() * periods_per_year
# 
#     # Calculate maximum drawdown
#     max_drawdown = calculate_maximum_drawdown(price_data)
# 
#     # Avoid division by zero
#     max_drawdown = np.where(max_drawdown == 0, 0.000001, max_drawdown)
# 
#     # Calculate Calmar ratio
#     calmar = ann_return / abs(max_drawdown)
# 
#     return calmar
# 
# 
# def calculate_information_ratio(returns, benchmark_returns, periods_per_year=252):
#     """
#     Calculate the Information ratio (excess return over benchmark divided by tracking error).
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         benchmark_returns (pandas.Series): Benchmark returns
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Information ratio
#     """
#     # Calculate excess returns
#     excess_returns = returns.sub(benchmark_returns, axis=0)
# 
#     # Calculate tracking error (standard deviation of excess returns)
#     tracking_error = excess_returns.std() * np.sqrt(periods_per_year)
# 
#     # Calculate Information ratio
#     info_ratio = (excess_returns.mean() * periods_per_year) / tracking_error
# 
#     return info_ratio
# 
# 
# def calculate_beta(returns, market_returns):
#     """
#     Calculate the beta (market risk) of returns.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         market_returns (pandas.Series): Market returns
# 
#     Returns:
#         float/pandas.Series: Beta
#     """
#     if isinstance(returns, pd.DataFrame):
#         # Calculate beta for each column in the DataFrame
#         betas = {}
#         for column in returns.columns:
#             covariance = returns[column].cov(market_returns)
#             market_variance = market_returns.var()
#             betas[column] = covariance / market_variance
#         return pd.Series(betas)
#     else:
#         # Calculate beta for a Series
#         covariance = returns.cov(market_returns)
#         market_variance = market_returns.var()
#         return covariance / market_variance
# 
# 
# def calculate_alpha(returns, market_returns, risk_free_rate=0.02, periods_per_year=252):
#     """
#     Calculate Jensen's alpha.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         market_returns (pandas.Series): Market returns
#         risk_free_rate (float): Annualized risk-free rate
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Alpha
#     """
#     # Convert annual risk-free rate to per-period rate
#     rf_per_period = ((1 + risk_free_rate) ** (1 / periods_per_year)) - 1
# 
#     # Calculate beta
#     beta = calculate_beta(returns, market_returns)
# 
#     # Calculate expected return according to CAPM
#     expected_return = rf_per_period + beta * (market_returns.mean() - rf_per_period)
# 
#     # Calculate alpha (excess return above CAPM)
#     alpha = returns.mean() - expected_return
# 
#     # Annualize alpha
#     alpha = alpha * periods_per_year
# 
#     return alpha
# 
# 
# def calculate_treynor_ratio(returns, market_returns, risk_free_rate=0.02, periods_per_year=252):
#     """
#     Calculate the Treynor ratio (excess return divided by beta).
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         market_returns (pandas.Series): Market returns
#         risk_free_rate (float): Annualized risk-free rate
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Treynor ratio
#     """
#     # Convert annual risk-free rate to per-period rate
#     rf_per_period = ((1 + risk_free_rate) ** (1 / periods_per_year)) - 1
# 
#     # Calculate excess return
#     excess_return = returns.mean() - rf_per_period
# 
#     # Calculate beta
#     beta = calculate_beta(returns, market_returns)
# 
#     # Avoid division by zero
#     beta = np.where(beta == 0, 0.000001, beta)
# 
#     # Calculate Treynor ratio
#     treynor = (excess_return * periods_per_year) / beta
# 
#     return treynor
# 
# 
# def calculate_skewness(returns):
#     """
#     Calculate the skewness of returns.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
# 
#     Returns:
#         float/pandas.Series: Skewness
#     """
#     return returns.skew()
# 
# 
# def calculate_kurtosis(returns):
#     """
#     Calculate the kurtosis of returns.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
# 
#     Returns:
#         float/pandas.Series: Kurtosis
#     """
#     return returns.kurtosis()
# 
# 
# def calculate_jarque_bera(returns):
#     """
#     Calculate the Jarque-Bera test statistic and p-value for normality.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
# 
#     Returns:
#         tuple: (jb_statistic, p_value) or (Series of jb_statistics, Series of p_values)
#     """
#     if isinstance(returns, pd.DataFrame):
#         # Calculate JB for each column in the DataFrame
#         jb_stats = {}
#         p_values = {}
#         for column in returns.columns:
#             jb_stat, p_value = stats.jarque_bera(returns[column].dropna())
#             jb_stats[column] = jb_stat
#             p_values[column] = p_value
#         return pd.Series(jb_stats), pd.Series(p_values)
#     else:
#         # Calculate JB for a Series
#         return stats.jarque_bera(returns.dropna())
# 
# 
# def calculate_omega_ratio(returns, threshold=0.0, periods_per_year=252):
#     """
#     Calculate the Omega ratio.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         threshold (float): Minimum acceptable return
#         periods_per_year (int): Number of periods in a year
# 
#     Returns:
#         float/pandas.Series: Omega ratio
#     """
#     threshold_per_period = ((1 + threshold) ** (1 / periods_per_year)) - 1
# 
#     # Calculate excess returns over threshold
#     excess = returns - threshold_per_period
# 
#     # Calculate positive and negative excess returns
#     positive_excess = excess.copy()
#     negative_excess = excess.copy()
# 
#     if isinstance(returns, pd.DataFrame):
#         for column in positive_excess.columns:
#             positive_excess.loc[positive_excess[column] < 0, column] = 0
#             negative_excess.loc[negative_excess[column] > 0, column] = 0
#     else:
#         positive_excess[positive_excess < 0] = 0
#         negative_excess[negative_excess > 0] = 0
# 
#     # Calculate mean of positive and negative excess returns
#     positive_mean = positive_excess.mean()
#     negative_mean = abs(negative_excess.mean())
# 
#     # Avoid division by zero
#     negative_mean = np.where(negative_mean == 0, 0.000001, negative_mean)
# 
#     # Calculate Omega ratio
#     omega = positive_mean / negative_mean
# 
#     return omega
# 
# 
# def get_risk_return_metrics(returns, price_data=None, market_returns=None, risk_free_rate=0.02, threshold=0.0):
#     """
#     Calculate comprehensive risk and return metrics for a portfolio or individual assets.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         price_data (pandas.DataFrame/Series, optional): Price data for drawdown calculations
#         market_returns (pandas.Series, optional): Market returns for beta and alpha calculations
#         risk_free_rate (float): Annualized risk-free rate
#         threshold (float): Minimum acceptable return for Omega ratio
# 
#     Returns:
#         pandas.DataFrame: DataFrame with all calculated metrics
#     """
#     metrics = {}
# 
#     # Basic statistics
#     metrics['Mean Return (Ann.)'] = returns.mean() * 252
#     metrics['Volatility (Ann.)'] = calculate_annualized_volatility(returns)
#     metrics['Sharpe Ratio'] = calculate_sharpe_ratio(returns, risk_free_rate)
#     metrics['Sortino Ratio'] = calculate_sortino_ratio(returns, risk_free_rate)
#     metrics['VaR (95%)'] = calculate_var(returns)
#     metrics['CVaR (95%)'] = calculate_cvar(returns)
#     metrics['Skewness'] = calculate_skewness(returns)
#     metrics['Kurtosis'] = calculate_kurtosis(returns)
#     metrics['Omega Ratio'] = calculate_omega_ratio(returns, threshold)
# 
#     # If price data is provided, calculate drawdown metrics
#     if price_data is not None:
#         metrics['Maximum Drawdown'] = calculate_maximum_drawdown(price_data)
#         metrics['Calmar Ratio'] = calculate_calmar_ratio(returns, price_data)
# 
#     # If market returns are provided, calculate market-related metrics
#     if market_returns is not None:
#         metrics['Beta'] = calculate_beta(returns, market_returns)
#         metrics['Alpha (Ann.)'] = calculate_alpha(returns, market_returns, risk_free_rate)
#         metrics['Information Ratio'] = calculate_information_ratio(returns, market_returns)
#         metrics['Treynor Ratio'] = calculate_treynor_ratio(returns, market_returns, risk_free_rate)
# 
#     # Convert to DataFrame
#     return pd.DataFrame(metrics)
# 
# 
# def calculate_returns_summary(data, periods=[1, 5, 20, 60, 252]):
#     """
#     Calculate returns over various time periods.
# 
#     Args:
#         data (pandas.DataFrame): DataFrame with price data
#         periods (list): List of periods to calculate returns for
# 
#     Returns:
#         pandas.DataFrame: DataFrame with returns for each period
#     """
#     returns_dict = {}
# 
#     for period in periods:
#         period_returns = data.pct_change(period).iloc[-1]
#         if period == 1:
#             returns_dict['1 Day'] = period_returns
#         elif period == 5:
#             returns_dict['1 Week'] = period_returns
#         elif period == 20:
#             returns_dict['1 Month'] = period_returns
#         elif period == 60:
#             returns_dict['3 Months'] = period_returns
#         elif period == 252:
#             returns_dict['1 Year'] = period_returns
# 
#     # Add YTD return
#     today = pd.Timestamp.now()
#     start_of_year = pd.Timestamp(today.year, 1, 1)
#     if start_of_year in data.index or start_of_year < data.index[0]:
#         if start_of_year in data.index:
#             start_idx = data.index.get_loc(start_of_year)
#         else:
#             start_idx = 0
#         ytd_return = (data.iloc[-1] / data.iloc[start_idx]) - 1
#         returns_dict['YTD'] = ytd_return
# 
#     return pd.DataFrame(returns_dict)
# 
# 
# def calculate_rolling_metrics(returns, window=252, risk_free_rate=0.02):
#     """
#     Calculate rolling metrics over a specified window.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         window (int): Rolling window size
#         risk_free_rate (float): Annualized risk-free rate
# 
#     Returns:
#         tuple: (rolling_sharpe, rolling_volatility, rolling_returns)
#     """
#     # Calculate daily risk-free rate
#     daily_rf = ((1 + risk_free_rate) ** (1 / 252)) - 1
# 
#     # Calculate rolling metrics
#     rolling_returns = returns.rolling(window=window).mean() * 252
#     rolling_volatility = returns.rolling(window=window).std() * np.sqrt(252)
# 
#     # Calculate rolling Sharpe ratio
#     excess_returns = returns - daily_rf
#     rolling_sharpe = (excess_returns.rolling(window=window).mean() /
#                       returns.rolling(window=window).std()) * np.sqrt(252)
# 
#     return rolling_sharpe, rolling_volatility, rolling_returns
# 
# 
# def calculate_stress_test(returns, scenarios=None):
#     """
#     Perform stress testing by simulating portfolio performance under different scenarios.
# 
#     Args:
#         returns (pandas.DataFrame): Returns data
#         scenarios (dict, optional): Dictionary of scenarios to test
# 
#     Returns:
#         pandas.DataFrame: DataFrame with stress test results
#     """
#     if scenarios is None:
#         # Default scenarios
#         scenarios = {
#             'Market Crash': {'SPY': -0.30, 'AGG': 0.05},
#             'Interest Rate Hike': {'SPY': -0.10, 'AGG': -0.15},
#             'Economic Boom': {'SPY': 0.20, 'AGG': -0.05},
#             'Recession': {'SPY': -0.25, 'AGG': 0.10},
#             'Stagflation': {'SPY': -0.15, 'AGG': -0.10}
#         }
# 
#     results = {}
# 
#     # Calculate portfolio beta to S&P 500 (if 'SPY' exists)
#     if 'SPY' in returns.columns:
#         spy_returns = returns['SPY']
#         portfolio_returns = returns.drop('SPY', axis=1)
#         betas = calculate_beta(portfolio_returns, spy_returns)
#     else:
#         betas = pd.Series(0.5, index=returns.columns)  # Default beta if no SPY data
# 
#     # Calculate portfolio beta to bonds (if 'AGG' exists)
#     if 'AGG' in returns.columns:
#         agg_returns = returns['AGG']
#         portfolio_returns = returns.drop('AGG', axis=1)
#         bond_betas = calculate_beta(portfolio_returns, agg_returns)
#     else:
#         bond_betas = pd.Series(0.2, index=returns.columns)  # Default bond beta if no AGG data
# 
#     # Calculate stress test results for each scenario
#     for scenario_name, scenario_shocks in scenarios.items():
#         scenario_returns = {}
# 
#         for ticker in returns.columns:
#             if ticker not in ['SPY', 'AGG']:
#                 # Calculate expected return under this scenario based on betas
#                 equity_impact = scenario_shocks.get('SPY', 0) * betas.get(ticker, 0.5)
#                 bond_impact = scenario_shocks.get('AGG', 0) * bond_betas.get(ticker, 0.2)
# 
#                 # Add some idiosyncratic component (random)
#                 idiosyncratic = np.random.normal(0, 0.05)
# 
#                 # Total impact is the sum of equity impact, bond impact, and idiosyncratic component
#                 total_impact = equity_impact + bond_impact + idiosyncratic
# 
#                 scenario_returns[ticker] = total_impact
#             else:
#                 # Direct impact for market indices
#                 scenario_returns[ticker] = scenario_shocks.get(ticker, 0)
# 
#         results[scenario_name] = pd.Series(scenario_returns)
# 
#     return pd.DataFrame(results)
# 
# 
# def calculate_monte_carlo_var(returns, num_simulations=10000, horizon=20, confidence_level=0.95):
#     """
#     Calculate Value at Risk using Monte Carlo simulation.
# 
#     Args:
#         returns (pandas.DataFrame/Series): Returns data
#         num_simulations (int): Number of Monte Carlo simulations
#         horizon (int): Forecast horizon in days
#         confidence_level (float): VaR confidence level
# 
#     Returns:
#         float/pandas.Series: Monte Carlo VaR
#     """
#     # Calculate mean and covariance of returns
#     mu = returns.mean().values
#     cov = returns.cov().values
# 
#     # Generate random returns
#     np.random.seed(42)  # For reproducibility
#     random_returns = np.random.multivariate_normal(mu, cov, (num_simulations, horizon))
# 
#     # Calculate cumulative returns for each simulation
#     cumulative_returns = np.cumprod(1 + random_returns, axis=1) - 1
# 
#     # Get final cumulative returns for each simulation
#     final_returns = cumulative_returns[:, -1]
# 
#     # Calculate VaR
#     if len(mu) == 1:
#         var = np.percentile(final_returns, (1 - confidence_level) * 100)
#         return float(var)
#     else:
#         vars = {}
#         for i, col in enumerate(returns.columns):
#             vars[col] = np.percentile(final_returns[:, i], (1 - confidence_level) * 100)
#         return pd.Series(vars)
# 
# 
# # Test function
# def test_risk_return_module():
#     """Test the risk-return module with sample data."""
#     # Create sample price data
#     dates = pd.date_range(start='2020-01-01', end='2022-12-31', freq='B')
#     np.random.seed(42)
# 
#     # Generate price data for 3 assets
#     price_data = pd.DataFrame({
#         'Asset1': 100 * (1 + np.random.normal(0.0001, 0.01, len(dates))).cumprod(),
#         'Asset2': 100 * (1 + np.random.normal(0.0002, 0.015, len(dates))).cumprod(),
#         'Asset3': 100 * (1 + np.random.normal(0.0001, 0.02, len(dates))).cumprod(),
#         'SPY': 100 * (1 + np.random.normal(0.0002, 0.012, len(dates))).cumprod()
#     }, index=dates)
# 
#     # Calculate returns
#     returns = price_data.pct_change().dropna()
# 
#     # Calculate and print various metrics
#     print("\nBasic Return and Risk Metrics:")
#     mean_returns, cov_matrix, std_deviation = calculate_returns(price_data, period_months=12)
#     for asset, ret, risk in zip(price_data.columns, mean_returns, std_deviation):
#         print(f"{asset}: Return = {ret:.2%}, Risk = {risk:.2%}")
# 
#     print("\nPortfolio Metrics:")
#     sharpe = calculate_sharpe_ratio(returns)
#     sortino = calculate_sortino_ratio(returns)
#     var_95 = calculate_var(returns)
#     cvar_95 = calculate_cvar(returns)
# 
#     print(f"Sharpe Ratio: {sharpe['Asset1']:.4f}, {sharpe['Asset2']:.4f}, {sharpe['Asset3']:.4f}")
#     print(f"Sortino Ratio: {sortino['Asset1']:.4f}, {sortino['Asset2']:.4f}, {sortino['Asset3']:.4f}")
#     print(f"VaR (95%): {var_95['Asset1']:.4%}, {var_95['Asset2']:.4%}, {var_95['Asset3']:.4%}")
#     print(f"CVaR (95%): {cvar_95['Asset1']:.4%}, {cvar_95['Asset2']:.4%}, {cvar_95['Asset3']:.4%}")
# 
#     # Test comprehensive metrics
#     print("\nComprehensive Metrics for Asset1:")
#     metrics = get_risk_return_metrics(
#         returns['Asset1'],
#         price_data['Asset1'],
#         returns['SPY'],
#         risk_free_rate=0.02
#     )
#     print(metrics)
# 
#     # Test Monte Carlo VaR
#     print("\nMonte Carlo VaR (95%, 20-day horizon):")
#     mc_var = calculate_monte_carlo_var(returns, num_simulations=1000, horizon=20)
#     print(mc_var)
# 
# 
# if __name__ == "__main__":
#     # Run tests when module is executed directly
#     test_risk_return_module()

"""## Claude API AI News"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile module_news_sentiment.py
# """
# Module for news sentiment analysis using Anthropic API.
# Provides functions to fetch and analyze news articles related to stocks
# and markets to extract sentiment and potential impact on investments.
# """
# 
# import requests
# import json
# import pandas as pd
# import numpy as np
# from datetime import datetime, timedelta
# import time
# import os
# import logging
# import anthropic
# from typing import List, Dict, Any, Optional, Tuple
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)
# 
# # Global variables
# ANTHROPIC_API_KEY = None
# NEWS_API_KEY = None
# 
# 
# def request_api_keys():
#     """
#     Request API keys from the user and save them for future use.
# 
#     Returns:
#         dict: Dictionary containing API keys
#     """
#     global ANTHROPIC_API_KEY, NEWS_API_KEY
# 
#     # Check if keys are already loaded
#     if ANTHROPIC_API_KEY and NEWS_API_KEY:
#         return {
#             'anthropic': ANTHROPIC_API_KEY,
#             'news_api': NEWS_API_KEY
#         }
# 
#     # Check if keys are saved in a file
#     if os.path.exists('sentiment_api_keys.json'):
#         try:
#             with open('sentiment_api_keys.json', 'r') as f:
#                 keys = json.load(f)
#                 ANTHROPIC_API_KEY = keys.get('anthropic')
#                 NEWS_API_KEY = keys.get('news_api')
# 
#                 print("API ключи загружены из файла.")
#                 return keys
#         except Exception as e:
#             logger.error(f"Ошибка загрузки API ключей из файла: {str(e)}")
# 
#     # Request new keys from user
#     print("\n=== Настройка API ключей для анализа новостей ===")
# 
#     # Anthropic API (required)
#     print("\n1. Anthropic API (https://www.anthropic.com/api)")
#     ANTHROPIC_API_KEY = input("Введите ваш Anthropic API ключ (обязательно): ")
#     while not ANTHROPIC_API_KEY:
#         ANTHROPIC_API_KEY = input("Anthropic API ключ обязателен. Пожалуйста, введите действительный ключ: ")
# 
#     # News API (required)
#     print("\n2. News API (https://newsapi.org/)")
#     NEWS_API_KEY = input("Введите ваш News API ключ (обязательно): ")
#     while not NEWS_API_KEY:
#         NEWS_API_KEY = input("News API ключ обязателен. Пожалуйста, введите действительный ключ: ")
# 
#     # Save keys to file
#     keys = {
#         'anthropic': ANTHROPIC_API_KEY,
#         'news_api': NEWS_API_KEY
#     }
# 
#     try:
#         with open('sentiment_api_keys.json', 'w') as f:
#             json.dump(keys, f)
#         print("\nAPI ключи успешно сохранены для будущего использования.")
#     except Exception as e:
#         logger.error(f"Ошибка сохранения API ключей в файл: {str(e)}")
# 
#     return keys
# 
# 
# # ===== NEWS FETCHING FUNCTIONS =====
# 
# def get_company_news(ticker, company_name=None, days_back=7, language='en'):
#     """
#     Get news articles related to a company using News API.
# 
#     Args:
#         ticker (str): Stock ticker symbol
#         company_name (str, optional): Company name for search query
#         days_back (int): Number of days back to fetch news
#         language (str): Language for articles ('en', 'ru', etc.)
# 
#     Returns:
#         list: List of news articles
#     """
#     global NEWS_API_KEY
# 
#     if not NEWS_API_KEY:
#         request_api_keys()
# 
#     try:
#         # If company name not provided, try to get it using Yahoo Finance
#         if not company_name:
#             try:
#                 import yfinance as yf
#                 stock = yf.Ticker(ticker)
#                 info = stock.info
#                 company_name = info.get('shortName', ticker)
#             except Exception as e:
#                 logger.warning(f"Не удалось получить название компании для {ticker}: {str(e)}")
#                 company_name = ticker
# 
#         # Calculate date range
#         end_date = datetime.now()
#         start_date = end_date - timedelta(days=days_back)
# 
#         # Format dates for API
#         from_date = start_date.strftime('%Y-%m-%d')
#         to_date = end_date.strftime('%Y-%m-%d')
# 
#         # Query News API
#         query = f'({company_name} OR {ticker})'
#         url = f"https://newsapi.org/v2/everything?q={query}&language={language}&from={from_date}&to={to_date}&sortBy=relevancy&apiKey={NEWS_API_KEY}"
# 
#         response = requests.get(url)
#         data = response.json()
# 
#         if data.get('status') == 'ok':
#             articles = data.get('articles', [])
#             logger.info(f"Найдено {len(articles)} статей для {ticker} ({company_name})")
#             return articles
#         else:
#             logger.error(f"Ошибка News API: {data.get('message', 'Неизвестная ошибка')}")
#             return []
# 
#     except Exception as e:
#         logger.error(f"Ошибка при получении новостей для {ticker}: {str(e)}")
#         return []
# 
# 
# def get_market_news(days_back=3, language='en', category='business'):
#     """
#     Get general market news using News API.
# 
#     Args:
#         days_back (int): Number of days back to fetch news
#         language (str): Language for articles ('en', 'ru', etc.)
#         category (str): News category ('business', 'technology', etc.)
# 
#     Returns:
#         list: List of news articles
#     """
#     global NEWS_API_KEY
# 
#     if not NEWS_API_KEY:
#         request_api_keys()
# 
#     try:
#         # Calculate date range
#         end_date = datetime.now()
#         start_date = end_date - timedelta(days=days_back)
# 
#         # Format dates for API
#         from_date = start_date.strftime('%Y-%m-%d')
#         to_date = end_date.strftime('%Y-%m-%d')
# 
#         # Query News API - top headlines for business category
#         url = f"https://newsapi.org/v2/top-headlines?category={category}&language={language}&from={from_date}&to={to_date}&apiKey={NEWS_API_KEY}"
# 
#         response = requests.get(url)
#         data = response.json()
# 
#         headlines = []
#         if data.get('status') == 'ok':
#             headlines = data.get('articles', [])
#             logger.info(f"Найдено {len(headlines)} главных новостей в категории {category}")
# 
#         # Additional search query for general market news
#         query = 'stock market OR finance OR economy OR investment'
#         url_everything = f"https://newsapi.org/v2/everything?q={query}&language={language}&from={from_date}&to={to_date}&sortBy=relevancy&apiKey={NEWS_API_KEY}"
# 
#         response_everything = requests.get(url_everything)
#         data_everything = response_everything.json()
# 
#         if data_everything.get('status') == 'ok':
#             articles = data_everything.get('articles', [])
#             logger.info(f"Найдено {len(articles)} статей о рынке")
# 
#             # Combine with headlines, removing duplicates
#             headline_urls = {article['url'] for article in headlines}
#             for article in articles:
#                 if article['url'] not in headline_urls:
#                     headlines.append(article)
# 
#             logger.info(f"Всего найдено {len(headlines)} уникальных статей о рынке")
#             return headlines
#         else:
#             logger.error(f"Ошибка News API: {data_everything.get('message', 'Неизвестная ошибка')}")
#             return headlines
# 
#     except Exception as e:
#         logger.error(f"Ошибка при получении новостей о рынке: {str(e)}")
#         return []
# 
# 
# def get_sector_news(sector, days_back=5, language='en'):
#     """
#     Get news articles related to a specific market sector.
# 
#     Args:
#         sector (str): Market sector name (e.g., 'Technology', 'Healthcare')
#         days_back (int): Number of days back to fetch news
#         language (str): Language for articles ('en', 'ru', etc.)
# 
#     Returns:
#         list: List of news articles
#     """
#     global NEWS_API_KEY
# 
#     if not NEWS_API_KEY:
#         request_api_keys()
# 
#     try:
#         # Calculate date range
#         end_date = datetime.now()
#         start_date = end_date - timedelta(days=days_back)
# 
#         # Format dates for API
#         from_date = start_date.strftime('%Y-%m-%d')
#         to_date = end_date.strftime('%Y-%m-%d')
# 
#         # Create sector-specific query terms
#         sector_terms = {
#             'Technology': 'technology sector OR tech stocks OR tech industry OR technology companies',
#             'Healthcare': 'healthcare sector OR pharmaceutical stocks OR medical stocks OR healthcare industry',
#             'Energy': 'energy sector OR oil stocks OR gas stocks OR renewable energy stocks',
#             'Financial': 'financial sector OR bank stocks OR financial industry OR financial services',
#             'Consumer': 'consumer sector OR retail stocks OR consumer goods OR consumer discretionary',
#             'Industrial': 'industrial sector OR manufacturing stocks OR industrial companies',
#             'Materials': 'materials sector OR commodity stocks OR basic materials',
#             'Real Estate': 'real estate sector OR REIT stocks OR property market',
#             'Telecom': 'telecom sector OR telecommunications stocks OR communications industry',
#             'Utilities': 'utility sector OR utility stocks OR utilities industry'
#         }
# 
#         query = sector_terms.get(sector, f"{sector} sector OR {sector} stocks OR {sector} industry")
# 
#         # Query News API
#         url = f"https://newsapi.org/v2/everything?q={query}&language={language}&from={from_date}&to={to_date}&sortBy=relevancy&apiKey={NEWS_API_KEY}"
# 
#         response = requests.get(url)
#         data = response.json()
# 
#         if data.get('status') == 'ok':
#             articles = data.get('articles', [])
#             logger.info(f"Найдено {len(articles)} статей для сектора {sector}")
#             return articles
#         else:
#             logger.error(f"Ошибка News API: {data.get('message', 'Неизвестная ошибка')}")
#             return []
# 
#     except Exception as e:
#         logger.error(f"Ошибка при получении новостей для сектора {sector}: {str(e)}")
#         return []
# 
# 
# # ===== ANTHROPIC SENTIMENT ANALYSIS =====
# 
# def initialize_anthropic_client():
#     """
#     Initialize the Anthropic client.
# 
#     Returns:
#         anthropic.Anthropic: Anthropic client instance
#     """
#     global ANTHROPIC_API_KEY
# 
#     if not ANTHROPIC_API_KEY:
#         request_api_keys()
# 
#     return anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
# 
# 
# def analyze_article_sentiment(article, ticker=None, company_name=None):
#     """
#     Analyze sentiment of a news article using Anthropic API.
# 
#     Args:
#         article (dict): News article data
#         ticker (str, optional): Stock ticker symbol
#         company_name (str, optional): Company name
# 
#     Returns:
#         dict: Sentiment analysis results
#     """
#     client = initialize_anthropic_client()
# 
#     # Extract article info
#     title = article.get('title', '')
#     description = article.get('description', '')
#     content = article.get('content', '')
#     published_at = article.get('publishedAt', '')
#     source = article.get('source', {}).get('name', '')
#     url = article.get('url', '')
# 
#     # Format date if available
#     date_str = ''
#     if published_at:
#         try:
#             date = datetime.strptime(published_at, "%Y-%m-%dT%H:%M:%SZ")
#             date_str = date.strftime("%d %B %Y")
#         except:
#             date_str = published_at
# 
#     # Create prompt for Anthropic
#     context = f"Статья: {title}\n\nИсточник: {source}\nДата: {date_str}\n\nОписание: {description}\n\nСодержание: {content[:1000]}...\n\nURL: {url}"
# 
#     company_context = f" о компании {company_name} ({ticker})" if company_name and ticker else ""
# 
#     prompt = f"""Проведи анализ новостной статьи{company_context}. Оцени общий сентимент и потенциальное влияние на рыночную стоимость компании или общий рынок.
# 
# Структурируй ответ:
# 1. Сентимент: оцени от -5 (крайне негативный) до +5 (крайне позитивный), где 0 - нейтральный
# 2. Краткое резюме важных фактов (до 3 пунктов)
# 3. Потенциальное влияние на рыночную стоимость: "позитивное", "негативное" или "нейтральное"
# 4. Уверенность в оценке: "высокая", "средняя" или "низкая"
# 5. Возможное влияние на временной горизонт: "краткосрочное", "среднесрочное" или "долгосрочное"
# 
# Выводы должны быть в формате JSON для дальнейшей обработки.
# 
# {context}
# """
# 
#     try:
#         response = client.messages.create(
#             model="claude-3-haiku-20240307",
#             max_tokens=1000,
#             temperature=0,
#             system="Ты финансовый аналитик, специализирующийся на анализе новостных статей и их влиянии на рынки и отдельные акции. Отвечай только в формате JSON.",
#             messages=[
#                 {"role": "user", "content": prompt}
#             ]
#         )
# 
#         response_text = response.content[0].text
# 
#         # Extract JSON from response
#         try:
#             # Try to find JSON in the response
#             json_start = response_text.find('{')
#             json_end = response_text.rfind('}')
# 
#             if json_start >= 0 and json_end >= 0:
#                 json_str = response_text[json_start:json_end+1]
#                 analysis = json.loads(json_str)
# 
#                 # Add metadata
#                 analysis['article_title'] = title
#                 analysis['article_source'] = source
#                 analysis['article_date'] = date_str
#                 analysis['article_url'] = url
# 
#                 return analysis
#             else:
#                 # Try to parse the whole response as JSON
#                 analysis = json.loads(response_text)
# 
#                 # Add metadata
#                 analysis['article_title'] = title
#                 analysis['article_source'] = source
#                 analysis['article_date'] = date_str
#                 analysis['article_url'] = url
# 
#                 return analysis
# 
#         except json.JSONDecodeError as e:
#             logger.error(f"Ошибка при извлечении JSON из ответа: {str(e)}")
# 
#             # Try to extract information manually
#             lines = response_text.split('\n')
#             sentiment_line = next((line for line in lines if 'сентимент' in line.lower() or 'sentiment' in line.lower()), None)
#             impact_line = next((line for line in lines if 'влияние' in line.lower() or 'impact' in line.lower()), None)
# 
#             sentiment_value = 0
#             if sentiment_line:
#                 try:
#                     sentiment_value = int(''.join(filter(lambda x: x.isdigit() or x == '-', sentiment_line)))
#                 except:
#                     pass
# 
#             impact = 'нейтральное'
#             if impact_line:
#                 if 'позитивн' in impact_line.lower() or 'positive' in impact_line.lower():
#                     impact = 'позитивное'
#                 elif 'негативн' in impact_line.lower() or 'negative' in impact_line.lower():
#                     impact = 'негативное'
# 
#             return {
#                 'sentiment': sentiment_value,
#                 'summary': ["Не удалось извлечь резюме из ответа"],
#                 'market_impact': impact,
#                 'confidence': "низкая",
#                 'time_horizon': "неопределенно",
#                 'article_title': title,
#                 'article_source': source,
#                 'article_date': date_str,
#                 'article_url': url,
#                 'error': "Ошибка при извлечении JSON из ответа"
#             }
# 
#     except Exception as e:
#         logger.error(f"Ошибка при анализе статьи: {str(e)}")
#         return {
#             'sentiment': 0,
#             'summary': ["Ошибка при анализе статьи"],
#             'market_impact': "нейтральное",
#             'confidence': "низкая",
#             'time_horizon': "неопределенно",
#             'article_title': title,
#             'article_source': source,
#             'article_date': date_str,
#             'article_url': url,
#             'error': str(e)
#         }
# 
# 
# def analyze_multiple_articles(articles, ticker=None, company_name=None, max_articles=5):
#     """
#     Analyze sentiment of multiple news articles.
# 
#     Args:
#         articles (list): List of news articles
#         ticker (str, optional): Stock ticker symbol
#         company_name (str, optional): Company name
#         max_articles (int): Maximum number of articles to analyze
# 
#     Returns:
#         list: List of sentiment analysis results
#     """
#     if not articles:
#         return []
# 
#     results = []
# 
#     # Sort articles by relevance (if we have a relevance score) or date
#     if 'relevance' in articles[0]:
#         sorted_articles = sorted(articles, key=lambda x: x.get('relevance', 0), reverse=True)
#     else:
#         sorted_articles = sorted(articles, key=lambda x: x.get('publishedAt', ''), reverse=True)
# 
#     # Limit number of articles
#     articles_to_analyze = sorted_articles[:max_articles]
# 
#     for i, article in enumerate(articles_to_analyze):
#         logger.info(f"Анализ статьи {i+1} из {len(articles_to_analyze)}: {article.get('title', 'Без заголовка')}")
# 
#         # Analyze article
#         result = analyze_article_sentiment(article, ticker, company_name)
#         results.append(result)
# 
#         # Respect API rate limits
#         if i < len(articles_to_analyze) - 1:
#             time.sleep(2)
# 
#     return results
# 
# 
# def aggregate_sentiment_analysis(analysis_results):
#     """
#     Aggregate sentiment analysis results from multiple articles.
# 
#     Args:
#         analysis_results (list): List of sentiment analysis results
# 
#     Returns:
#         dict: Aggregated sentiment analysis
#     """
#     if not analysis_results:
#         return {
#             'average_sentiment': 0,
#             'sentiment_count': {
#                 'positive': 0,
#                 'neutral': 0,
#                 'negative': 0
#             },
#             'market_impact': 'нейтральное',
#             'confidence': 'низкая',
#             'key_points': [],
#             'articles_analyzed': 0
#         }
# 
#     # Extract sentiments
#     sentiments = [result.get('sentiment', 0) for result in analysis_results]
# 
#     # Count sentiment distribution
#     sentiment_count = {
#         'positive': len([s for s in sentiments if s > 0]),
#         'neutral': len([s for s in sentiments if s == 0]),
#         'negative': len([s for s in sentiments if s < 0])
#     }
# 
#     # Calculate average sentiment
#     average_sentiment = sum(sentiments) / len(sentiments)
# 
#     # Determine overall market impact
#     impacts = [result.get('market_impact', 'нейтральное').lower() for result in analysis_results]
#     impact_count = {
#         'позитивное': impacts.count('позитивное'),
#         'нейтральное': impacts.count('нейтральное'),
#         'негативное': impacts.count('негативное')
#     }
# 
#     market_impact = max(impact_count.items(), key=lambda x: x[1])[0]
# 
#     # Determine confidence
#     confidences = [result.get('confidence', 'средняя').lower() for result in analysis_results]
#     confidence_values = {'высокая': 3, 'средняя': 2, 'низкая': 1}
#     confidence_score = sum(confidence_values.get(conf, 1) for conf in confidences) / len(confidences)
# 
#     if confidence_score >= 2.5:
#         confidence = 'высокая'
#     elif confidence_score >= 1.5:
#         confidence = 'средняя'
#     else:
#         confidence = 'низкая'
# 
#     # Extract key points
#     all_points = []
#     for result in analysis_results:
#         summary = result.get('summary', [])
#         if isinstance(summary, list):
#             all_points.extend(summary)
#         elif isinstance(summary, str):
#             all_points.append(summary)
# 
#     # Remove duplicates and empty points
#     key_points = []
#     seen_points = set()
#     for point in all_points:
#         if point and point.strip() and point.strip().lower() not in seen_points:
#             key_points.append(point.strip())
#             seen_points.add(point.strip().lower())
# 
#     # Limit to top 5 points
#     key_points = key_points[:5]
# 
#     return {
#         'average_sentiment': round(average_sentiment, 2),
#         'sentiment_count': sentiment_count,
#         'market_impact': market_impact,
#         'confidence': confidence,
#         'key_points': key_points,
#         'articles_analyzed': len(analysis_results),
#         'articles': [
#             {
#                 'title': result.get('article_title', ''),
#                 'source': result.get('article_source', ''),
#                 'date': result.get('article_date', ''),
#                 'url': result.get('article_url', ''),
#                 'sentiment': result.get('sentiment', 0)
#             }
#             for result in analysis_results
#         ]
#     }
# 
# 
# def analyze_company_sentiment(ticker, company_name=None, days_back=7, max_articles=5):
#     """
#     Analyze sentiment for a specific company from recent news.
# 
#     Args:
#         ticker (str): Stock ticker symbol
#         company_name (str, optional): Company name
#         days_back (int): Number of days back to fetch news
#         max_articles (int): Maximum number of articles to analyze
# 
#     Returns:
#         dict: Aggregated sentiment analysis
#     """
#     # Get company name if not provided
#     if not company_name:
#         try:
#             import yfinance as yf
#             stock = yf.Ticker(ticker)
#             info = stock.info
#             company_name = info.get('shortName', ticker)
#         except Exception as e:
#             logger.warning(f"Не удалось получить название компании для {ticker}: {str(e)}")
#             company_name = ticker
# 
#     logger.info(f"Анализ сентимента для {company_name} ({ticker})")
# 
#     # Fetch news articles
#     articles = get_company_news(ticker, company_name, days_back)
# 
#     if not articles:
#         logger.warning(f"Не найдено статей для {company_name} ({ticker})")
#         return {
#             'ticker': ticker,
#             'company_name': company_name,
#             'average_sentiment': 0,
#             'sentiment_count': {'positive': 0, 'neutral': 0, 'negative': 0},
#             'market_impact': 'нейтральное',
#             'confidence': 'низкая',
#             'key_points': ["Не найдено статей для анализа"],
#             'articles_analyzed': 0,
#             'articles': []
#         }
# 
#     # Analyze articles
#     analysis_results = analyze_multiple_articles(articles, ticker, company_name, max_articles)
# 
#     # Aggregate results
#     aggregated = aggregate_sentiment_analysis(analysis_results)
# 
#     # Add ticker and company information
#     aggregated['ticker'] = ticker
#     aggregated['company_name'] = company_name
# 
#     return aggregated
# 
# 
# def analyze_market_sentiment(days_back=3, max_articles=8):
#     """
#     Analyze overall market sentiment from recent news.
# 
#     Args:
#         days_back (int): Number of days back to fetch news
#         max_articles (int): Maximum number of articles to analyze
# 
#     Returns:
#         dict: Aggregated market sentiment analysis
#     """
#     logger.info("Анализ общего рыночного сентимента")
# 
#     # Fetch market news
#     articles = get_market_news(days_back)
# 
#     if not articles:
#         logger.warning("Не найдено статей о рынке")
#         return {
#             'average_sentiment': 0,
#             'sentiment_count': {'positive': 0, 'neutral': 0, 'negative': 0},
#             'market_impact': 'нейтральное',
#             'confidence': 'низкая',
#             'key_points': ["Не найдено статей для анализа"],
#             'articles_analyzed': 0,
#             'articles': []
#         }
# 
#     # Analyze articles
#     analysis_results = analyze_multiple_articles(articles, max_articles=max_articles)
# 
#     # Aggregate results
#     aggregated = aggregate_sentiment_analysis(analysis_results)
#     aggregated['market_type'] = 'general'
# 
#     return aggregated
# 
# 
# def analyze_sector_sentiment(sector, days_back=5, max_articles=5):
#     """
#     Analyze sentiment for a specific market sector from recent news.
# 
#     Args:
#         sector (str): Market sector name
#         days_back (int): Number of days back to fetch news
#         max_articles (int): Maximum number of articles to analyze
# 
#     Returns:
#         dict: Aggregated sector sentiment analysis
#     """
#     logger.info(f"Анализ сентимента для сектора {sector}")
# 
#     # Fetch sector news
#     articles = get_sector_news(sector, days_back)
# 
#     if not articles:
#         logger.warning(f"Не найдено статей для сектора {sector}")
#         return {
#             'sector': sector,
#             'average_sentiment': 0,
#             'sentiment_count': {'positive': 0, 'neutral': 0, 'negative': 0},
#             'market_impact': 'нейтральное',
#             'confidence': 'низкая',
#             'key_points': [f"Не найдено статей для сектора {sector}"],
#             'articles_analyzed': 0,
#             'articles': []
#         }
# 
#     # Analyze articles
#     analysis_results = analyze_multiple_articles(articles, max_articles=max_articles)
# 
#     # Aggregate results
#     aggregated = aggregate_sentiment_analysis(analysis_results)
#     aggregated['sector'] = sector
# 
#     return aggregated
# 
# 
# def generate_market_forecast(company_sentiments=None, sector_sentiments=None, market_sentiment=None):
#     """
#     Generate a comprehensive market forecast based on sentiment analysis.
# 
#     Args:
#         company_sentiments (list): List of company sentiment analyses
#         sector_sentiments (list): List of sector sentiment analyses
#         market_sentiment (dict): Overall market sentiment analysis
# 
#     Returns:
#         dict: Market forecast
#     """
#     client = initialize_anthropic_client()
# 
#     # Prepare data for prompt
#     company_data = ""
#     if company_sentiments:
#         company_data = "Sentiment by Company:\n"
#         for company in company_sentiments:
#             company_data += f"{company.get('company_name', '')} ({company.get('ticker', '')}): "
#             company_data += f"Sentiment: {company.get('average_sentiment', 0)}, "
#             company_data += f"Impact: {company.get('market_impact', 'нейтральное')}\n"
#             if company.get('key_points'):
#                 company_data += "Key points:\n"
#                 for point in company.get('key_points', [])[:3]:
#                     company_data += f"- {point}\n"
#                 company_data += "\n"
# 
#     sector_data = ""
#     if sector_sentiments:
#         sector_data = "Sentiment by Sector:\n"
#         for sector in sector_sentiments:
#             sector_data += f"{sector.get('sector', '')}: "
#             sector_data += f"Sentiment: {sector.get('average_sentiment', 0)}, "
#             sector_data += f"Impact: {sector.get('market_impact', 'нейтральное')}\n"
#             if sector.get('key_points'):
#                 sector_data += "Key points:\n"
#                 for point in sector.get('key_points', [])[:2]:
#                     sector_data += f"- {point}\n"
#                 sector_data += "\n"
# 
#     market_data = ""
#     if market_sentiment:
#         market_data = "Overall Market Sentiment:\n"
#         market_data += f"Sentiment: {market_sentiment.get('average_sentiment', 0)}, "
#         market_data += f"Impact: {market_sentiment.get('market_impact', 'нейтральное')}\n"
#         if market_sentiment.get('key_points'):
#             market_data += "Key points:\n"
#             for point in market_sentiment.get('key_points', []):
#                 market_data += f"- {point}\n"
#             market_data += "\n"
# 
#     # Create prompt
#     current_date = datetime.now().strftime("%d %B %Y")
#     prompt = f"""Проведи анализ рыночного сентимента на основе новостей и подготовь прогноз рынка на ближайшее время.
# 
# Дата анализа: {current_date}
# 
# {market_data}
# {sector_data}
# {company_data}
# 
# Подготовь полный прогноз со следующей структурой:
# 1. Общее настроение рынка и ключевые факторы
# 2. Секторальный прогноз - какие сектора выглядят наиболее и наименее перспективными
# 3. Прогноз по отдельным компаниям, если есть данные
# 4. Рекомендации для инвесторов (краткосрочные и среднесрочные)
# 5. Потенциальные риски
# 
# Сформируй ответ в формате JSON с соответствующими полями.
# """
# 
#     try:
#         response = client.messages.create(
#             model="claude-3-haiku-20240307",
#             max_tokens=2000,
#             temperature=0.2,
#             system="Ты опытный финансовый аналитик, специализирующийся на рыночных прогнозах и рекомендациях для инвесторов. Твои прогнозы основаны на анализе новостного сентимента и исторических данных.",
#             messages=[
#                 {"role": "user", "content": prompt}
#             ]
#         )
# 
#         response_text = response.content[0].text
# 
#         # Extract JSON from response
#         try:
#             # Try to find JSON in the response
#             json_start = response_text.find('{')
#             json_end = response_text.rfind('}')
# 
#             if json_start >= 0 and json_end >= 0:
#                 json_str = response_text[json_start:json_end+1]
#                 forecast = json.loads(json_str)
# 
#                 # Add metadata
#                 forecast['analysis_date'] = current_date
#                 return forecast
#             else:
#                 # Try to parse the whole response as JSON
#                 forecast = json.loads(response_text)
# 
#                 # Add metadata
#                 forecast['analysis_date'] = current_date
#                 return forecast
# 
#         except json.JSONDecodeError as e:
#             logger.error(f"Ошибка при извлечении JSON из ответа: {str(e)}")
# 
#             # Structured format if JSON parsing fails
#             sections = response_text.split('\n\n')
#             forecast = {
#                 'market_sentiment': "Не удалось извлечь данные",
#                 'sector_outlook': "Не удалось извлечь данные",
#                 'company_outlook': "Не удалось извлечь данные",
#                 'recommendations': "Не удалось извлечь данные",
#                 'risks': "Не удалось извлечь данные",
#                 'analysis_date': current_date,
#                 'raw_response': response_text,
#                 'error': "Ошибка при извлечении JSON из ответа"
#             }
# 
#             # Try to extract sections from text
#             for section in sections:
#                 if section.lower().startswith('1.') or 'настроение рынка' in section.lower():
#                     forecast['market_sentiment'] = section
#                 elif section.lower().startswith('2.') or 'сектор' in section.lower():
#                     forecast['sector_outlook'] = section
#                 elif section.lower().startswith('3.') or 'компан' in section.lower():
#                     forecast['company_outlook'] = section
#                 elif section.lower().startswith('4.') or 'рекоменд' in section.lower():
#                     forecast['recommendations'] = section
#                 elif section.lower().startswith('5.') or 'риск' in section.lower():
#                     forecast['risks'] = section
# 
#             return forecast
# 
#     except Exception as e:
#         logger.error(f"Ошибка при генерации прогноза рынка: {str(e)}")
#         return {
#             'market_sentiment': "Ошибка при генерации прогноза",
#             'sector_outlook': "Ошибка при генерации прогноза",
#             'company_outlook': "Ошибка при генерации прогноза",
#             'recommendations': "Ошибка при генерации прогноза",
#             'risks': "Ошибка при генерации прогноза",
#             'analysis_date': current_date,
#             'error': str(e)
#         }